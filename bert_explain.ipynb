{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Sequence Classification\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained model for natural language processing tasks, including sequence classification. It leverages the Transformer architecture and self-attention mechanism to capture contextual information from input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Representation\n",
    "\n",
    "![BERT Input Representation](https://miro.medium.com/v2/resize:fit:800/format:webp/1*tLYLk2ZGu2XYmc0n4Hn8Mg.png)\n",
    "\n",
    "BERT takes a sequence of tokens as input, typically represented as:\n",
    "- `[CLS]` token: A special token added at the beginning of the sequence, used for classification tasks.\n",
    "- Token embeddings: Each token in the input sequence is converted into a dense vector representation.\n",
    "- Segment embeddings: Used to differentiate between different segments (e.g., sentences) in the input sequence.\n",
    "- Position embeddings: Captures the positional information of each token in the sequence.\n",
    "\n",
    "These embeddings are summed element-wise to obtain the final input representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Encoder\n",
    "\n",
    "![BERT Encoder Architecture](https://miro.medium.com/v2/resize:fit:560/format:webp/1*VtfbRAAiQhb0IUi7fSKTaQ.png)\n",
    "\n",
    "The BERT encoder consists of multiple layers of Transformer blocks. Each Transformer block contains:\n",
    "- Multi-head self-attention: Allows the model to attend to different positions of the input sequence, capturing relationships between tokens.\n",
    "- Feed-forward neural network: Applies non-linear transformations to the output of the self-attention layer.\n",
    "\n",
    "The encoder processes the input sequence and generates contextualized representations for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention\n",
    "\n",
    "The multi-head self-attention mechanism allows the model to attend to different positions of the input sequence, capturing relationships between tokens. It consists of multiple attention heads that operate in parallel.\n",
    "\n",
    "Each attention head computes attention scores between all pairs of tokens in the sequence, using query (Q), key (K), and value (V) matrices:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "```\n",
    "\n",
    "Where `d_k` is the dimension of the key vectors.\n",
    "\n",
    "The outputs of all attention heads are concatenated and passed through a linear transformation to obtain the final self-attention output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Neural Network\n",
    "\n",
    "After the self-attention sub-layer, a position-wise feed-forward neural network is applied to each position separately and identically. It consists of two linear transformations with a ReLU activation in between:\n",
    "\n",
    "```\n",
    "FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "```\n",
    "\n",
    "Where `W_1`, `b_1`, `W_2`, and `b_2` are learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization and Residual Connections\n",
    "\n",
    "Each sub-layer in the Transformer block is followed by a layer normalization and a residual connection. The residual connection helps in propagating the information from lower layers to higher layers and facilitates gradient flow during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "The output of the BERT encoder is a sequence of contextualized token representations. These representations can be used for various downstream tasks, such as sequence classification, token classification, question answering, and more.\n",
    "\n",
    "For sequence classification tasks, the `[CLS]` token representation from the final layer of the BERT encoder is typically used as the aggregate representation of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification\n",
    "\n",
    "For sequence classification, the `[CLS]` token representation from the final layer of the BERT encoder is used. This representation captures the overall contextual information of the input sequence.\n",
    "\n",
    "The `[CLS]` token representation is passed through a linear layer followed by a softmax activation to obtain the class probabilities:\n",
    "\n",
    "```\n",
    "class_probabilities = softmax(linear(cls_representation))\n",
    "```\n",
    "\n",
    "The model is trained using a cross-entropy loss function to optimize the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "BERT is pre-trained on large-scale unsupervised data using masked language modeling and next sentence prediction tasks. For sequence classification, the pre-trained BERT model is fine-tuned on a labeled dataset specific to the classification task. After pre-training, BERT can be fine-tuned on specific downstream tasks by adding task-specific layers on top of the pre-trained encoder and training the model on labeled data.\n",
    "\n",
    "During fine-tuning, the pre-trained BERT encoder weights are updated, and the additional classification layer is trained from scratch. This allows the model to adapt to the specific classification task while leveraging the pre-trained knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "At inference time, the input sequence is passed through the fine-tuned BERT model, and the class probabilities are obtained from the `[CLS]` token representation. The class with the highest probability is predicted as the output.\n",
    "\n",
    "BERT sequence classification has achieved state-of-the-art performance on various benchmarks and has become a go-to approach for many natural language processing tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE BERT ARCHITECTURE\n",
    "\n",
    "![BERT ARCHITECTURE](assets/bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings\n",
    "\n",
    "The input to the BERT model consists of three types of embeddings:\n",
    "1. Word embeddings: Each token in the input sequence is converted into a dense vector representation.\n",
    "2. Position embeddings: Captures the positional information of each token in the sequence.\n",
    "3. Token type embeddings: Used to differentiate between different segments (e.g., sentences) in the input sequence.\n",
    "\n",
    "These embeddings are summed element-wise to obtain the final input representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 x Bert Encoder\n",
    "\n",
    "The BERT encoder consists of 12 identical layers (in the base version). Each layer contains two main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Attention\n",
    "\n",
    "The Bert Attention module is based on the self-attention mechanism. It consists of three matrices: Query, Key, and Value. The attention mechanism computes the relationship between each token and all other tokens in the sequence.\n",
    "\n",
    "The output of the Bert Attention module goes through a dropout layer to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Output\n",
    "\n",
    "The Bert Output module takes the output from the Bert Attention module and passes it through the following components:\n",
    "1. Linear layer: Applies a linear transformation to the attention output.\n",
    "2. Layer Normalization: Normalizes the activations across the features.\n",
    "3. Dropout: Applies dropout regularization.\n",
    "\n",
    "The output of the Bert Output module is then added to the input of the Bert Attention module through a residual connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Self Output\n",
    "\n",
    "After the 12 Bert Encoder layers, the output goes through the Bert Self Output module, which consists of:\n",
    "1. Linear layer: Applies a linear transformation to the output of the last Bert Encoder layer.\n",
    "2. GELU activation: Applies the Gaussian Error Linear Unit (GELU) activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Pooler\n",
    "\n",
    "The Bert Pooler module is used to obtain a fixed-size representation of the input sequence. It consists of:\n",
    "1. Linear layer: Applies a linear transformation to the output of the first token (usually the `[CLS]` token) from the last Bert Encoder layer.\n",
    "2. Tanh activation: Applies the hyperbolic tangent activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "Finally, the output of the Bert Pooler module is passed through a dropout layer and then fed into a linear classifier. The classifier produces the final output probabilities for the task at hand (e.g., sentiment classification, named entity recognition, etc.).\n",
    "\n",
    "This diagram provides a detailed overview of the BERT architecture, showcasing the flow of data through the various components. The Bert Encoder layers form the core of the model, capturing the contextual information through self-attention mechanisms. The Bert Pooler and Classifier modules are task-specific and can be adapted based on the downstream task requirements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
