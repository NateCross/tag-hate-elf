{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss and Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the behavior of cross entropy loss and adam optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT = ['the quick brown fox jumped over the lazy dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calamancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nate/miniconda3/lib/python3.9/site-packages/spacy/util.py:918: UserWarning: [W094] Model 'tl_calamancy_md' (0.1.0) specifies an under-constrained spaCy version requirement: >=3.5.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.7.4,<3.8.0\n",
      "  warnings.warn(warn_msg)\n",
      "/home/nate/miniconda3/lib/python3.9/site-packages/spacy/util.py:918: UserWarning: [W094] Model 'tl_calamancy_lg' (0.1.0) specifies an under-constrained spaCy version requirement: >=3.5.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.7.4,<3.8.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.tl.Tagalog at 0x7fb39aeffdf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import calamancy\n",
    "\n",
    "Calamancy = calamancy.load(\"tl_calamancy_md-0.1.0\")\n",
    "\n",
    "Calamancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34760/1945608868.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch import Tensor\n",
    "from collections import Counter\n",
    "\n",
    "def get_calamancy_tokens(data):\n",
    "  # Allows it to work with both dataframes and\n",
    "  # simple lists of strings\n",
    "  if isinstance(data, pd.Series):\n",
    "    data = data.values\n",
    "\n",
    "  samples = []\n",
    "\n",
    "  progress_bar = tqdm.tqdm(total=len(data))\n",
    "\n",
    "  for sample in Calamancy.pipe(data):\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    tokens = []\n",
    "    for token in sample:\n",
    "      tokens.append(token)\n",
    "\n",
    "    samples.append(tokens)\n",
    "\n",
    "  progress_bar.close()\n",
    "\n",
    "  return samples\n",
    "\n",
    "def get_token_vectors(tokens):\n",
    "  vectors = []\n",
    "\n",
    "  progress_bar = tqdm.tqdm(total=len(tokens))\n",
    "\n",
    "  for sample in tokens:\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    token_vectors = []\n",
    "    # Check in case empty due to processing\n",
    "    if not sample:\n",
    "      token_vectors.append(np.zeros((200)))\n",
    "    else:\n",
    "      for token in sample:\n",
    "        if token.has_vector:\n",
    "          token_vectors.append(token.vector)\n",
    "    token_vectors = Tensor(np.array(token_vectors))\n",
    "\n",
    "    vectors.append(token_vectors)\n",
    "\n",
    "  progress_bar.close()\n",
    "\n",
    "  return vectors\n",
    "\n",
    "def data_remove_stopwords(data):\n",
    "  stopwords_list = open(\n",
    "    './src/stopwords-tl.txt',\n",
    "    'r',\n",
    "  ).read().split('\\n')\n",
    "  stopwords_dict = Counter(stopwords_list)\n",
    "  return [\n",
    "    ' '.join([\n",
    "      word for word in sample.split()\n",
    "      if word not in stopwords_dict\n",
    "    ])\n",
    "    for sample \n",
    "    in data\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "input_tokenized = get_calamancy_tokens(INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[the, quick, brown, fox, jumped, over, the, lazy, dog]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 88.41it/s]\n"
     ]
    }
   ],
   "source": [
    "input_vectorized = get_token_vectors(input_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0448,  0.2607,  0.1448,  ...,  0.4243, -0.1153, -0.0620],\n",
       "         [ 0.5258, -0.3996,  0.0114,  ...,  0.4171, -0.3223,  0.1630],\n",
       "         [-0.0630,  0.6724,  1.0455,  ...,  0.2032, -0.4797,  0.5314],\n",
       "         ...,\n",
       "         [-0.0448,  0.2607,  0.1448,  ...,  0.4243, -0.1153, -0.0620],\n",
       "         [-0.1454,  0.5539,  0.2981,  ...,  1.6336, -0.2340, -0.1128],\n",
       "         [ 0.1416, -0.5068,  0.7797,  ...,  0.2225, -0.7088, -0.3081]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 200])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vectorized[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim \n",
    "\n",
    "INPUT_SIZE = 200\n",
    "NUM_OF_HIDDEN_NODES = 50\n",
    "OUTPUT_SIZE = 2\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "OPTIMIZER = optim.Adam\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LstmModel(\n",
       "  (lstm): LSTM(200, 50, batch_first=True)\n",
       "  (linear): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LstmModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.lstm = nn.LSTM(\n",
    "      INPUT_SIZE,\n",
    "      NUM_OF_HIDDEN_NODES,\n",
    "      batch_first=True,\n",
    "    )\n",
    "    self.linear = nn.Linear(NUM_OF_HIDDEN_NODES, OUTPUT_SIZE)\n",
    "\n",
    "    self.lstm_output = None\n",
    "    self.lstm_hidden_state = None\n",
    "    self.lstm_cell_state = None\n",
    "\n",
    "  def forward(self, input):\n",
    "    self.lstm_output, (self.lstm_hidden_state, self.lstm_cell_state) = self.lstm(input)\n",
    "\n",
    "    linear_output = self.linear(self.lstm_output[:, -1])\n",
    "\n",
    "    return linear_output\n",
    "\n",
    "Lstm = LstmModel()\n",
    "\n",
    "Lstm.to(DEVICE)\n",
    "\n",
    "Lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.stack(input_vectorized).to(DEVICE)\n",
    "\n",
    "output = Lstm(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1412, 0.2750]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import LongTensor\n",
    "\n",
    "output_loss = loss_function(\n",
    "  output, \n",
    "  LongTensor([1]).to(DEVICE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that actual value is 1\n",
    "\n",
    "Hence, the actual probability distribution is [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_probabilities = torch.softmax(\n",
    "  output,\n",
    "  dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4666, 0.5334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6285088"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_probabilities = [0, 1]\n",
    "\n",
    "predicted_probabilities = output_probabilities[0][0] * actual_probabilities[0] + output_probabilities[0][1] * actual_probabilities[1]\n",
    "\n",
    "-np.log(predicted_probabilities.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLE_INPUTS = [\n",
    "  'the quick brown fox jumped',\n",
    "  'over the lazy dog near',\n",
    "  'the bank of the river',\n",
    "]\n",
    "\n",
    "TARGET_INPUTS = [1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.22it/s]\n"
     ]
    }
   ],
   "source": [
    "multiple_input_tokens = get_calamancy_tokens(MULTIPLE_INPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[the, quick, brown, fox, jumped],\n",
       " [over, the, lazy, dog, near],\n",
       " [the, bank, of, the, river]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2446.61it/s]\n"
     ]
    }
   ],
   "source": [
    "multiple_input_vectors = get_token_vectors(multiple_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.4816e-02,  2.6072e-01,  1.4476e-01,  1.3985e-01,  8.2186e-01,\n",
       "          -1.8453e-01,  3.9610e-01, -3.3248e-01, -1.8321e-03, -3.2702e-01,\n",
       "          -1.4955e-01, -7.9797e-01,  3.2212e-01,  5.0457e-01, -1.4466e-02,\n",
       "          -1.5588e-01, -5.8039e-01, -8.2334e-01, -1.2749e-01,  1.2681e-02,\n",
       "           1.7029e-01,  2.0639e-01,  1.8592e-01,  9.1905e-02,  7.9670e-01,\n",
       "           3.7350e-03, -1.1508e-01, -9.2960e-02, -5.0474e-02, -5.2370e-01,\n",
       "           2.4938e-01,  5.0046e-02, -2.2510e-01,  8.2782e-01, -8.8028e-02,\n",
       "          -3.1285e-01,  2.1367e-01, -1.2164e-01,  6.2164e-02,  3.9969e-02,\n",
       "          -1.9242e-01,  5.9761e-01,  8.7907e-01,  2.6625e-01, -6.2292e-01,\n",
       "           3.5723e-01, -5.6910e-01, -2.2975e-02, -4.4734e-01, -2.3245e-01,\n",
       "          -1.6014e-02,  2.4307e-02, -2.9628e-01,  3.4902e-03,  2.8123e-01,\n",
       "          -3.6569e-01, -5.6186e-02, -8.3109e-02,  3.7591e-01, -3.0810e-01,\n",
       "          -3.9157e-01, -5.8018e-01,  4.4514e-02, -3.4728e-01, -1.4072e-02,\n",
       "           9.6888e-02,  4.2398e-02, -2.0702e-01,  1.1550e-03,  4.9811e-01,\n",
       "          -3.3000e-01, -2.3525e-01, -8.9399e-02,  2.0654e-01, -1.4491e-01,\n",
       "          -4.1638e-02,  2.2768e-01,  1.7223e-01, -1.9337e-01,  5.4533e-01,\n",
       "          -1.8066e-01,  4.5702e-02, -1.8615e-01, -5.7819e-01,  3.1169e-02,\n",
       "           1.2292e-02, -3.2395e-01,  5.6787e-01, -3.4982e-01, -1.6895e-01,\n",
       "          -4.9290e-01,  1.7113e-01,  1.2016e-01,  6.5679e-01,  5.1149e-01,\n",
       "           1.3962e-01, -1.4009e-02,  8.0457e-01, -1.6864e-03,  4.5210e-01,\n",
       "           3.1804e-01, -6.4181e-02, -3.4219e-01, -1.9089e-01, -5.3410e-01,\n",
       "          -5.3507e-01, -5.3707e-02, -5.6995e-02,  4.4185e-02,  2.6180e-01,\n",
       "          -5.3361e-01, -5.3296e-01, -3.7533e-01,  5.2777e-01, -4.5300e-02,\n",
       "          -1.5200e-01, -1.8493e-01, -1.5568e-01,  1.2953e-01,  2.1748e-01,\n",
       "          -4.5083e-01,  1.0669e-01, -2.5463e-01,  2.3678e-01,  5.8170e-03,\n",
       "           1.8802e-01, -9.0083e-02, -9.5030e-01, -3.3442e-01, -4.9307e-01,\n",
       "          -7.0966e-02, -5.5689e-01,  3.3150e-01,  1.6025e-01, -2.6116e-01,\n",
       "           6.1389e-02, -5.4729e-01, -1.4360e-01, -3.0535e-01, -3.5357e-01,\n",
       "          -6.2616e-01, -2.2439e-02, -2.0740e-01,  2.1965e-01,  4.6478e-01,\n",
       "          -1.3669e-01, -1.0145e-01,  1.0842e-01,  5.0507e-03,  2.9468e-01,\n",
       "          -2.0390e-02,  7.9607e-02, -5.0416e-03, -9.2473e-02,  3.1267e-01,\n",
       "           5.8991e-01,  1.1121e+00,  3.1630e-02, -4.6566e-02, -3.7113e-01,\n",
       "          -8.5519e-02, -2.8330e-02, -2.7766e-02, -4.7535e-01, -6.3396e-01,\n",
       "          -7.5772e-02, -3.1621e-03,  3.7479e-02,  1.3982e-01, -5.8328e-02,\n",
       "           3.6327e-01, -4.1039e-02, -9.3303e-02,  6.7044e-02,  3.8285e-01,\n",
       "          -3.5371e-01,  5.5917e-01, -3.6336e-01,  7.0457e-03,  4.0882e-01,\n",
       "           3.3558e-01,  3.1595e-01,  2.6091e-01,  3.2326e-01,  7.1776e-02,\n",
       "          -2.9163e-01,  1.2613e-01,  1.7375e-01, -5.7487e-01,  3.6839e-01,\n",
       "           2.2042e-01,  3.1725e-01, -4.6196e-01, -5.9088e-02, -6.9970e-01,\n",
       "           2.6686e-02, -4.2077e-01,  4.2432e-01, -1.1535e-01, -6.1969e-02],\n",
       "         [ 5.2582e-01, -3.9957e-01,  1.1390e-02,  9.8938e-03,  1.7747e-01,\n",
       "           2.0426e-01,  4.7010e-01, -5.5379e-03,  3.7632e-01,  2.1025e-02,\n",
       "           7.5305e-02, -9.8197e-02,  2.3605e-01,  2.8973e-01, -4.6871e-01,\n",
       "          -1.1958e-01,  5.9096e-03, -7.4165e-01,  2.1289e-01,  4.0479e-02,\n",
       "          -2.0617e-01, -4.1237e-01,  3.6557e-01, -1.3649e-03,  2.2545e-01,\n",
       "           2.5235e-01, -2.0550e-01,  5.1982e-01,  4.9334e-01,  9.2994e-02,\n",
       "          -7.6709e-02,  1.6873e-01,  2.0939e-01,  7.4881e-02, -5.1258e-01,\n",
       "          -8.5820e-01,  1.6354e-01, -8.9909e-01,  4.0316e-01, -1.5315e-01,\n",
       "          -9.1007e-03,  2.1035e-01,  1.1436e+00,  3.5039e-01, -4.9243e-01,\n",
       "           1.1073e+00, -3.9505e-01, -7.3672e-01,  3.1965e-02, -1.1052e-01,\n",
       "           1.4942e-01, -4.3055e-01, -6.3311e-02,  8.1610e-02,  3.4618e-01,\n",
       "          -2.9015e-01, -5.7756e-02, -1.6263e-01,  2.6232e-01, -7.0096e-03,\n",
       "          -4.9251e-01, -8.4032e-01,  1.0436e-02, -4.9564e-01, -2.2579e-01,\n",
       "           4.5158e-03, -2.1058e-01,  6.1756e-02,  2.4745e-01, -1.0154e-01,\n",
       "          -5.3641e-01, -5.4325e-01,  4.7301e-01,  1.0782e-01,  1.3411e-01,\n",
       "          -5.9710e-01,  3.2889e-01,  1.5896e-01, -8.1743e-02,  4.3795e-01,\n",
       "          -4.4972e-01,  1.9432e-01, -3.6185e-01,  5.1602e-03, -1.1319e-01,\n",
       "          -1.5882e-02,  6.6906e-01, -1.2913e-01,  4.0810e-01, -6.6620e-03,\n",
       "          -2.5002e-01, -1.3401e-01, -1.3108e-01,  4.2851e-01,  2.0056e-01,\n",
       "           1.8707e-01, -5.3876e-01,  2.7650e-02, -1.8840e-01,  3.1943e-01,\n",
       "           5.1957e-01, -2.3585e-01, -1.6077e-01, -4.5108e-01, -3.2736e-01,\n",
       "           2.4534e-01, -4.3506e-01, -1.7187e-01,  2.4753e-02,  1.3861e-01,\n",
       "           5.3755e-02, -1.5925e-01, -6.8991e-02, -1.9627e-02, -2.5720e-02,\n",
       "           2.0635e-01, -6.6691e-01,  3.7582e-01,  3.3616e-01, -5.4611e-01,\n",
       "           2.7783e-01, -2.5366e-01,  8.2039e-01,  2.0999e-01, -2.1332e-02,\n",
       "           5.5277e-01, -4.7521e-01, -6.1330e-01,  2.9165e-01, -3.0554e-01,\n",
       "          -1.9863e-01, -4.1631e-01,  6.0528e-01,  1.8256e-01, -2.5560e-02,\n",
       "           5.4986e-01, -4.8742e-01,  3.6153e-01,  9.7532e-03, -8.2782e-02,\n",
       "          -7.1096e-01, -4.7132e-01, -2.7811e-01,  2.4559e-01,  6.0404e-01,\n",
       "          -9.0283e-02,  5.5116e-02,  1.0785e-01, -6.1579e-01,  2.8370e-01,\n",
       "          -2.4776e-01,  1.6538e-01, -2.9603e-01,  2.8756e-01, -6.0230e-02,\n",
       "           3.6150e-01,  2.6907e-01,  1.5677e-01,  1.4881e-01,  1.2983e-01,\n",
       "          -7.3904e-02, -4.4660e-01, -3.3024e-01,  8.4394e-02, -1.7000e-01,\n",
       "          -3.9127e-01, -5.1365e-01,  1.9484e-01,  2.4687e-01, -1.8259e-01,\n",
       "           3.9010e-02,  3.1322e-01,  9.3358e-01, -2.0409e-01,  1.0829e+00,\n",
       "           1.8914e-01,  9.8692e-02, -3.9530e-01, -7.5977e-01,  2.9325e-01,\n",
       "          -3.3745e-01,  1.1537e-01,  1.2746e-01,  4.9039e-01, -5.2925e-02,\n",
       "           3.4013e-01,  7.9464e-02, -2.0501e-01, -1.1362e+00,  3.0718e-01,\n",
       "           2.0904e-01, -4.6873e-01,  1.9942e-01, -1.8950e-01,  8.5889e-02,\n",
       "           2.4270e-01, -2.3279e-01,  4.1708e-01, -3.2227e-01,  1.6304e-01],\n",
       "         [-6.2968e-02,  6.7242e-01,  1.0455e+00,  3.6472e-01,  4.1441e-01,\n",
       "          -4.3495e-02,  3.1925e-01, -8.9794e-02,  4.3778e-01,  1.3582e-01,\n",
       "          -2.0251e-01, -5.7321e-02, -1.8505e-01, -2.3192e-01, -1.3180e-01,\n",
       "           1.2560e-01, -1.2763e-01, -2.0208e-01,  6.4287e-03,  2.6766e-01,\n",
       "           4.8296e-01,  4.5433e-01, -9.4795e-02, -8.3432e-02,  1.3545e-01,\n",
       "           8.1204e-02, -6.4749e-02, -5.1711e-01, -5.1881e-01, -3.6176e-01,\n",
       "          -6.6832e-01,  1.1883e-01, -1.1837e-01, -5.1527e-01,  1.8740e-01,\n",
       "          -1.7782e-01,  1.6195e-01, -1.0626e-01, -7.1311e-01, -2.6944e-01,\n",
       "          -5.9343e-02,  4.4534e-01,  7.9498e-01, -1.7180e-02, -4.4074e-01,\n",
       "           5.0367e-01, -2.1153e-02,  1.2234e-01,  1.4064e-01,  3.4787e-01,\n",
       "          -2.1203e-01, -8.7750e-02, -5.7605e-01,  1.0794e-01,  9.8911e-02,\n",
       "          -1.4260e-01, -4.4556e-01,  5.5467e-01,  3.6178e-01,  3.7078e-01,\n",
       "          -8.2019e-03, -2.8629e-01,  9.2128e-02,  1.2331e-01,  2.3766e-01,\n",
       "           4.3419e-01, -2.8217e-01,  4.4114e-02,  7.7627e-02, -1.3490e-01,\n",
       "          -4.9788e-01,  2.3944e-01,  4.8443e-02, -3.1436e-01,  8.2149e-02,\n",
       "           2.6548e-01,  3.9599e-01, -8.9486e-01, -1.9219e-01,  8.2651e-02,\n",
       "          -3.0722e-01,  4.1871e-01, -6.6705e-01, -4.3573e-01,  1.7064e-01,\n",
       "           8.9452e-02,  3.2756e-02, -1.8004e-01,  5.5068e-02, -7.7420e-01,\n",
       "          -3.7546e-01, -2.4502e-01, -7.8392e-03, -3.3128e-01, -2.3278e-01,\n",
       "          -5.4355e-01,  1.0617e-01,  2.0729e-01, -1.4228e-01, -1.1683e-01,\n",
       "           1.5202e-01,  2.2899e-01, -1.4420e-01, -1.6212e-01, -3.9624e-01,\n",
       "          -1.0645e-01,  2.4998e-01,  8.5707e-01, -4.5600e-01, -5.2756e-02,\n",
       "           2.1509e-01, -2.8672e-01, -1.9751e-02,  2.8852e-02, -3.9469e-01,\n",
       "          -5.5048e-01, -5.3897e-01,  4.6681e-02,  1.0439e-01, -3.0208e-01,\n",
       "           1.6174e-01, -1.0714e-01, -2.6083e-01,  2.2511e-01, -1.6846e-01,\n",
       "           2.9357e-01,  2.5711e-01, -2.2997e-01,  3.0387e-01, -5.5053e-01,\n",
       "           3.7336e-01, -4.3008e-02,  3.1381e-01,  3.4852e-01, -2.4234e-01,\n",
       "           2.7139e-01, -1.0534e-01,  1.9515e-02,  3.3996e-01,  1.0211e-01,\n",
       "          -3.2328e-01,  1.4245e-01, -1.2768e-01,  7.5923e-01, -1.6191e-01,\n",
       "          -5.6238e-01, -8.6730e-01,  2.1947e-01, -2.0236e-01, -4.0445e-01,\n",
       "          -1.1988e-03,  1.6201e-01, -1.3094e-01, -6.2289e-01, -5.4628e-05,\n",
       "           3.6729e-01, -4.4023e-01, -5.4574e-04,  5.0367e-01,  6.4463e-01,\n",
       "           1.1006e-01, -9.8956e-01, -2.4277e-01,  1.8927e-01,  4.2673e-01,\n",
       "          -2.9745e-02,  1.4799e-01,  2.8096e-01,  2.8654e-01, -9.2266e-02,\n",
       "           6.4654e-01,  5.1758e-01,  3.5818e-02, -6.7137e-01,  6.8926e-02,\n",
       "          -2.6086e-01, -1.0211e-01,  2.3365e-01,  3.8064e-02, -1.0616e-01,\n",
       "          -2.1372e-01, -2.0122e-01,  3.0760e-01,  4.6263e-01, -5.9128e-02,\n",
       "          -4.6866e-01,  1.4062e-01, -8.0100e-02,  1.4165e-01, -3.3927e-01,\n",
       "           6.4791e-01, -3.8325e-02, -1.0251e-01,  1.4774e-02, -3.2966e-01,\n",
       "          -8.0827e-01, -1.6598e-01,  2.0323e-01, -4.7967e-01,  5.3143e-01],\n",
       "         [ 5.3453e-01,  8.9322e-01,  5.4035e-01, -3.8142e-01,  5.6625e-01,\n",
       "           5.6520e-01,  7.8617e-01, -3.3560e-01,  2.4861e-01,  3.9402e-01,\n",
       "           2.3372e-01, -1.0249e+00, -5.2546e-01, -1.3582e-01, -1.3783e+00,\n",
       "          -4.3251e-01,  6.6572e-01, -9.9388e-01, -9.3003e-01,  2.6695e-01,\n",
       "           5.4414e-01, -3.6710e-02, -3.3585e-01, -5.5716e-01,  5.2425e-01,\n",
       "           2.2787e-01, -4.5318e-03,  1.6051e-01, -4.3154e-01,  1.3725e+00,\n",
       "           1.6735e-01,  3.7182e-01,  1.0513e+00, -1.2945e-01, -8.9678e-01,\n",
       "           3.6963e-01,  7.7352e-01, -1.4455e+00, -3.5308e-01, -3.3889e-02,\n",
       "          -9.7435e-01, -3.1993e-01,  1.7209e-01,  1.1243e+00, -1.1892e+00,\n",
       "           6.8171e-01,  3.8381e-01, -8.8983e-01,  4.4023e-01, -1.9213e-02,\n",
       "          -6.6555e-01, -1.0384e+00,  1.5955e-01,  5.3144e-04, -7.3056e-02,\n",
       "          -4.7849e-01, -8.5359e-01,  4.6828e-02,  6.6151e-01,  4.4777e-01,\n",
       "          -4.1704e-01, -8.3729e-01, -2.5145e-01, -6.5960e-01, -3.3396e-01,\n",
       "           1.2831e+00,  1.4391e-01, -3.0199e-01, -8.1084e-01,  1.3911e-01,\n",
       "          -1.0649e+00, -2.0366e-01, -3.0544e-01,  3.6532e-01,  3.3138e-01,\n",
       "          -1.0624e-01,  2.1494e-01, -4.2405e-01,  2.6690e-01,  9.7092e-01,\n",
       "          -8.8282e-01, -6.1802e-01,  6.0888e-01, -2.9226e-01,  2.7972e-01,\n",
       "           5.5565e-01,  3.8582e-03,  7.3056e-02, -8.0739e-01, -1.0916e+00,\n",
       "          -2.3233e-01, -4.8161e-01,  5.9149e-02,  3.5635e-01, -1.8832e+00,\n",
       "           2.1809e-01,  4.3210e-01, -1.1417e-01,  1.3493e-01, -6.5069e-01,\n",
       "           6.2443e-01,  1.4634e-01,  8.8526e-02,  2.0698e-01,  7.2677e-02,\n",
       "          -3.3530e-01,  1.3533e+00,  1.5302e-01, -1.2944e-01,  5.5431e-01,\n",
       "           5.9981e-02, -6.7858e-02, -8.7081e-02, -9.7391e-01, -2.9278e-01,\n",
       "          -1.7713e-01,  7.3422e-01,  1.8044e-01, -1.0316e-01,  6.3261e-01,\n",
       "           2.9656e-01,  7.3183e-01, -9.1582e-01,  5.2432e-01,  9.4620e-02,\n",
       "          -5.9796e-02,  7.4538e-01,  1.3032e-01,  3.0220e-01, -1.1962e-01,\n",
       "           1.6633e+00,  8.0571e-03,  1.3139e+00,  1.5560e-01,  6.7555e-01,\n",
       "           7.7513e-01,  8.3048e-01,  1.0965e-01, -2.5111e-01, -4.0532e-02,\n",
       "          -1.1496e+00,  6.9076e-01, -1.5477e-01,  1.1205e+00,  2.7069e-01,\n",
       "          -4.3419e-01, -3.6857e-01, -6.7692e-01, -8.9022e-01,  8.3964e-01,\n",
       "           5.8913e-01,  3.5117e-01,  2.2308e-01,  8.4952e-01,  2.1158e-01,\n",
       "          -7.5775e-01, -2.2421e-02, -1.0313e+00,  1.6339e+00, -1.7067e-01,\n",
       "           1.2633e-01, -1.2614e+00,  3.6366e-01,  2.5118e-01,  4.8618e-01,\n",
       "          -9.8725e-01, -4.5705e-01,  3.3377e-01, -4.2046e-02, -4.2415e-01,\n",
       "           4.7096e-01,  6.4284e-01,  2.5389e-01, -1.0142e-01,  6.7314e-02,\n",
       "          -2.2180e+00,  2.6569e-01,  4.6163e-02, -1.0615e+00,  1.0376e+00,\n",
       "          -5.2256e-01,  3.9886e-01, -2.0516e-01,  2.0748e-01,  1.4123e-01,\n",
       "          -1.8153e-01, -1.0119e+00, -2.5736e-01, -1.0638e-01, -9.5944e-02,\n",
       "           7.4550e-01, -7.7631e-01,  2.0808e-01, -3.7264e-01,  6.8881e-01,\n",
       "          -1.3418e+00, -2.9791e-01,  7.4957e-01, -2.0193e-01,  6.4317e-01],\n",
       "         [ 4.0237e-01, -4.3079e-01,  4.0140e-01,  3.8917e-01,  3.9254e-01,\n",
       "           7.3992e-01, -3.6405e-01,  3.0677e-02, -7.2171e-01, -1.8627e-01,\n",
       "           1.6950e-01, -3.0568e-01, -6.6851e-02,  3.0010e-01, -3.0154e-01,\n",
       "          -8.2374e-01, -5.6618e-01, -7.0135e-01,  3.2981e-01, -1.8343e-01,\n",
       "           5.1941e-01,  7.4040e-01,  3.7198e-01, -9.0341e-02,  3.2229e-01,\n",
       "          -1.1438e-01, -3.4804e-02, -2.3584e-01,  1.2480e-01, -3.1851e-01,\n",
       "           1.5030e-01,  1.6525e-01, -9.1870e-02, -1.5099e-01,  2.8054e-01,\n",
       "          -3.1211e-01,  5.6248e-01, -8.9609e-01, -5.8133e-01, -5.2176e-02,\n",
       "           1.0643e-01,  3.9270e-01,  6.5161e-01,  4.5113e-01, -5.5697e-01,\n",
       "           5.1014e-01, -2.2502e-01,  2.1462e-01, -1.5204e-01, -1.6236e-01,\n",
       "          -2.5608e-01, -3.0328e-01,  2.4568e-02,  3.0776e-01,  1.4191e-01,\n",
       "          -2.1584e-01,  1.6103e-01,  1.4722e-01,  2.7303e-01, -3.1852e-02,\n",
       "          -4.3707e-01, -3.0806e-01,  1.0603e+00,  9.3847e-02, -3.7324e-01,\n",
       "           1.4434e-01, -2.9833e-01,  1.9924e-01,  2.0985e-01,  3.5541e-01,\n",
       "          -4.9584e-01, -2.4642e-01,  1.3900e-01, -1.1140e-01,  3.1301e-01,\n",
       "           3.8376e-01,  5.3094e-01,  2.8201e-01, -2.6519e-01,  1.0257e-01,\n",
       "           2.6175e-01, -1.6348e-01, -1.9408e-01, -5.1454e-01,  2.9924e-01,\n",
       "          -6.5998e-01,  8.6718e-03,  1.9480e-01, -6.4623e-01, -1.7241e-01,\n",
       "           1.4216e-01, -2.9613e-01, -1.8087e-01, -2.3885e-01,  1.0546e-01,\n",
       "           1.9908e-01, -1.7145e-02, -5.4881e-01,  4.0679e-01, -4.6920e-01,\n",
       "           4.9049e-01, -9.2103e-02, -2.5435e-02, -6.5964e-01, -4.7339e-01,\n",
       "           8.5093e-01,  3.0596e-01,  4.4412e-01, -4.7295e-01,  1.1708e+00,\n",
       "           4.8305e-01, -2.4402e-01,  1.5420e-01,  3.5689e-01,  1.1582e-01,\n",
       "          -3.1562e-01, -1.6549e-01,  7.7367e-01,  3.9228e-01,  1.1579e-01,\n",
       "           4.0971e-01,  5.4513e-01,  7.2031e-01,  3.7067e-01, -6.0253e-01,\n",
       "           6.3257e-01,  1.9416e-01, -7.2429e-01,  2.0785e-01, -5.3375e-01,\n",
       "           7.0769e-02,  3.5887e-01,  6.5136e-01, -4.2391e-01, -3.9593e-01,\n",
       "          -1.4935e-01, -8.3433e-01,  8.2376e-01, -1.2467e-01, -2.8000e-01,\n",
       "          -5.4580e-01,  1.6281e-01,  1.5052e-01,  1.1934e-01,  8.8216e-01,\n",
       "          -3.0178e-03,  3.3257e-01,  3.4231e-01,  2.2400e-01, -3.7833e-02,\n",
       "          -2.4915e-01,  3.9799e-01,  4.8354e-01,  4.3415e-01,  4.2723e-01,\n",
       "           1.1100e-01,  2.8731e-01,  1.8105e-01, -1.7005e-01,  4.3117e-02,\n",
       "           6.3371e-01, -6.6275e-01, -5.6791e-01,  6.8223e-01,  5.5571e-01,\n",
       "          -3.5512e-01,  4.2885e-01, -1.7566e-01,  6.3898e-01, -6.9407e-01,\n",
       "           4.4170e-02,  2.2975e-01, -5.4211e-01, -1.4389e-01, -1.5036e-01,\n",
       "          -3.8541e-01,  6.8533e-02, -8.3389e-01, -3.4367e-01, -7.6084e-01,\n",
       "           5.1460e-01,  2.4641e-01,  2.7039e-01,  4.0971e-02,  4.0235e-01,\n",
       "          -4.6860e-01, -6.8394e-03,  2.0025e-01,  2.5531e-01,  3.0677e-01,\n",
       "           5.5962e-02, -1.6154e-02, -6.2793e-01, -4.8660e-02,  4.6340e-01,\n",
       "          -3.4588e-01, -5.2617e-01,  5.4931e-01, -4.1621e-01, -1.8028e-01]]),\n",
       " tensor([[-8.9689e-02,  3.4316e-01,  2.6373e-01,  1.2438e-01,  7.1749e-01,\n",
       "           2.2371e-01,  2.7924e-01,  3.2778e-01,  1.8856e-01, -1.2572e-01,\n",
       "           8.6910e-02, -5.9665e-01, -1.7770e-01,  1.1413e-01, -1.4181e-01,\n",
       "          -1.8178e-01, -3.2238e-01, -5.3837e-01,  1.7155e-04,  1.8803e-01,\n",
       "           3.2020e-01, -3.5529e-01, -2.5224e-01,  1.9141e-01,  6.7685e-01,\n",
       "          -3.7449e-01, -5.2056e-01, -4.9048e-01, -2.3425e-01, -5.5392e-01,\n",
       "           2.0104e-01,  6.8938e-01,  1.0283e-01,  4.1450e-01,  9.8860e-02,\n",
       "          -2.3158e-01, -1.1151e-01, -3.2252e-02,  3.1593e-01, -2.0222e-01,\n",
       "          -2.6186e-01,  6.6811e-01,  8.4184e-01,  4.4114e-01, -9.1005e-01,\n",
       "           6.3740e-01,  1.3109e-01, -3.7000e-01, -6.9599e-01, -1.1701e-01,\n",
       "           8.2138e-02,  4.1532e-01, -2.0829e-01,  7.5296e-02,  3.1932e-01,\n",
       "           1.4721e-01,  1.5803e-01,  3.7935e-02,  1.8456e-01,  2.3756e-01,\n",
       "          -2.7831e-01, -5.3994e-01,  1.9349e-01, -7.1311e-02,  1.1485e-02,\n",
       "          -1.4057e-01, -1.8851e-01, -2.2892e-01,  1.0299e-02, -1.7852e-01,\n",
       "          -5.6609e-01, -1.2454e-01,  2.0107e-01, -6.3303e-02, -5.7659e-02,\n",
       "           4.0747e-01,  4.6605e-01,  2.0016e-02,  1.8296e-01,  1.8736e-01,\n",
       "           3.8332e-02, -2.0561e-03,  1.2988e-01, -6.1068e-01,  9.4046e-02,\n",
       "          -3.3597e-02, -1.3825e-01,  4.4027e-01, -3.2480e-01, -2.0222e-01,\n",
       "          -1.5927e-01, -2.6840e-01, -1.8518e-01,  2.5985e-01,  4.7509e-01,\n",
       "           2.9783e-01, -2.0591e-01,  7.0173e-01,  9.0242e-02, -7.7591e-02,\n",
       "           1.3952e-01,  6.4638e-02, -1.5523e-01, -7.2622e-01, -8.9134e-01,\n",
       "          -1.2291e-01,  5.5753e-02, -5.0978e-02, -1.4115e-02,  4.5564e-01,\n",
       "           1.3976e-01, -4.3956e-01,  2.2807e-01,  5.2432e-01, -2.0999e-02,\n",
       "          -3.3742e-01, -3.0892e-01,  2.7659e-01,  1.2481e-01,  6.4408e-02,\n",
       "          -2.0629e-02, -2.4609e-01,  1.3647e-01,  4.4294e-02,  5.6656e-02,\n",
       "           4.2760e-02,  1.4426e-01, -7.3484e-01,  6.0200e-01, -4.8247e-01,\n",
       "          -2.0407e-02, -1.3671e-01, -5.0679e-02, -3.3230e-02, -1.9161e-01,\n",
       "          -2.8508e-01, -2.1523e-01,  7.2780e-01, -2.4044e-01, -6.2502e-01,\n",
       "          -5.5045e-01,  2.6876e-01, -3.0711e-01,  4.0650e-01,  5.2903e-01,\n",
       "          -2.6782e-01, -4.0558e-01, -3.0511e-03,  3.9286e-01,  5.3367e-02,\n",
       "           3.2650e-02,  1.0442e-02, -3.4164e-01, -1.5811e-01,  3.2665e-01,\n",
       "           6.1399e-01,  3.9384e-01, -3.5572e-01,  7.3814e-02, -1.2838e-01,\n",
       "           1.7337e-01, -1.2974e-02,  2.4907e-02, -3.3338e-01, -2.6492e-01,\n",
       "          -1.7191e-01, -2.3980e-02,  4.1467e-01,  3.5364e-01,  8.3307e-02,\n",
       "          -5.0702e-02,  4.5850e-01,  9.2895e-02,  5.2893e-02,  2.1199e-01,\n",
       "          -1.0219e-01,  1.4996e-01, -3.8431e-01,  9.3946e-02, -2.9961e-01,\n",
       "           2.1955e-01,  2.0197e-01, -2.5227e-02,  2.5625e-01,  4.6795e-02,\n",
       "          -2.7049e-01, -1.9013e-01,  4.4952e-01, -3.9755e-01,  8.3703e-01,\n",
       "           2.4659e-01, -5.1385e-02, -4.3192e-01,  3.0225e-01, -6.7844e-02,\n",
       "          -1.1619e-01, -6.8668e-01,  1.8338e-01, -2.4968e-01,  6.2954e-04],\n",
       "         [-4.4816e-02,  2.6072e-01,  1.4476e-01,  1.3985e-01,  8.2186e-01,\n",
       "          -1.8453e-01,  3.9610e-01, -3.3248e-01, -1.8321e-03, -3.2702e-01,\n",
       "          -1.4955e-01, -7.9797e-01,  3.2212e-01,  5.0457e-01, -1.4466e-02,\n",
       "          -1.5588e-01, -5.8039e-01, -8.2334e-01, -1.2749e-01,  1.2681e-02,\n",
       "           1.7029e-01,  2.0639e-01,  1.8592e-01,  9.1905e-02,  7.9670e-01,\n",
       "           3.7350e-03, -1.1508e-01, -9.2960e-02, -5.0474e-02, -5.2370e-01,\n",
       "           2.4938e-01,  5.0046e-02, -2.2510e-01,  8.2782e-01, -8.8028e-02,\n",
       "          -3.1285e-01,  2.1367e-01, -1.2164e-01,  6.2164e-02,  3.9969e-02,\n",
       "          -1.9242e-01,  5.9761e-01,  8.7907e-01,  2.6625e-01, -6.2292e-01,\n",
       "           3.5723e-01, -5.6910e-01, -2.2975e-02, -4.4734e-01, -2.3245e-01,\n",
       "          -1.6014e-02,  2.4307e-02, -2.9628e-01,  3.4902e-03,  2.8123e-01,\n",
       "          -3.6569e-01, -5.6186e-02, -8.3109e-02,  3.7591e-01, -3.0810e-01,\n",
       "          -3.9157e-01, -5.8018e-01,  4.4514e-02, -3.4728e-01, -1.4072e-02,\n",
       "           9.6888e-02,  4.2398e-02, -2.0702e-01,  1.1550e-03,  4.9811e-01,\n",
       "          -3.3000e-01, -2.3525e-01, -8.9399e-02,  2.0654e-01, -1.4491e-01,\n",
       "          -4.1638e-02,  2.2768e-01,  1.7223e-01, -1.9337e-01,  5.4533e-01,\n",
       "          -1.8066e-01,  4.5702e-02, -1.8615e-01, -5.7819e-01,  3.1169e-02,\n",
       "           1.2292e-02, -3.2395e-01,  5.6787e-01, -3.4982e-01, -1.6895e-01,\n",
       "          -4.9290e-01,  1.7113e-01,  1.2016e-01,  6.5679e-01,  5.1149e-01,\n",
       "           1.3962e-01, -1.4009e-02,  8.0457e-01, -1.6864e-03,  4.5210e-01,\n",
       "           3.1804e-01, -6.4181e-02, -3.4219e-01, -1.9089e-01, -5.3410e-01,\n",
       "          -5.3507e-01, -5.3707e-02, -5.6995e-02,  4.4185e-02,  2.6180e-01,\n",
       "          -5.3361e-01, -5.3296e-01, -3.7533e-01,  5.2777e-01, -4.5300e-02,\n",
       "          -1.5200e-01, -1.8493e-01, -1.5568e-01,  1.2953e-01,  2.1748e-01,\n",
       "          -4.5083e-01,  1.0669e-01, -2.5463e-01,  2.3678e-01,  5.8170e-03,\n",
       "           1.8802e-01, -9.0083e-02, -9.5030e-01, -3.3442e-01, -4.9307e-01,\n",
       "          -7.0966e-02, -5.5689e-01,  3.3150e-01,  1.6025e-01, -2.6116e-01,\n",
       "           6.1389e-02, -5.4729e-01, -1.4360e-01, -3.0535e-01, -3.5357e-01,\n",
       "          -6.2616e-01, -2.2439e-02, -2.0740e-01,  2.1965e-01,  4.6478e-01,\n",
       "          -1.3669e-01, -1.0145e-01,  1.0842e-01,  5.0507e-03,  2.9468e-01,\n",
       "          -2.0390e-02,  7.9607e-02, -5.0416e-03, -9.2473e-02,  3.1267e-01,\n",
       "           5.8991e-01,  1.1121e+00,  3.1630e-02, -4.6566e-02, -3.7113e-01,\n",
       "          -8.5519e-02, -2.8330e-02, -2.7766e-02, -4.7535e-01, -6.3396e-01,\n",
       "          -7.5772e-02, -3.1621e-03,  3.7479e-02,  1.3982e-01, -5.8328e-02,\n",
       "           3.6327e-01, -4.1039e-02, -9.3303e-02,  6.7044e-02,  3.8285e-01,\n",
       "          -3.5371e-01,  5.5917e-01, -3.6336e-01,  7.0457e-03,  4.0882e-01,\n",
       "           3.3558e-01,  3.1595e-01,  2.6091e-01,  3.2326e-01,  7.1776e-02,\n",
       "          -2.9163e-01,  1.2613e-01,  1.7375e-01, -5.7487e-01,  3.6839e-01,\n",
       "           2.2042e-01,  3.1725e-01, -4.6196e-01, -5.9088e-02, -6.9970e-01,\n",
       "           2.6686e-02, -4.2077e-01,  4.2432e-01, -1.1535e-01, -6.1969e-02],\n",
       "         [-1.4536e-01,  5.5392e-01,  2.9812e-01,  4.4193e-01,  4.4817e-01,\n",
       "          -2.9871e-01,  5.8040e-01, -1.4770e+00, -2.0179e-01, -5.4853e-01,\n",
       "           2.7254e-01, -3.0692e-01,  9.1368e-01, -6.0429e-01,  4.9596e-02,\n",
       "          -4.9546e-01, -1.9805e-01, -9.9881e-01, -7.7062e-01, -5.6888e-01,\n",
       "           2.1491e-01, -4.1051e-01, -4.6947e-01, -1.1331e-01,  1.1265e+00,\n",
       "          -9.6643e-02, -4.7978e-01, -5.0860e-01,  1.6646e-01, -1.3358e-01,\n",
       "          -6.2170e-01, -1.9371e-02,  4.0330e-01, -2.4077e-01, -8.4937e-01,\n",
       "           4.9349e-01,  3.9260e-01, -4.6131e-01, -3.6409e-01, -3.4300e-01,\n",
       "           6.3853e-01, -1.1007e+00,  1.5062e+00,  6.1458e-01, -1.2928e+00,\n",
       "           8.1120e-02, -2.8643e-02, -4.1703e-01,  1.2918e-01,  9.3238e-02,\n",
       "           2.1694e-01,  8.3415e-01,  2.0509e-01, -1.2920e-01,  3.6541e-01,\n",
       "          -8.5944e-02, -3.1757e-01,  2.3406e-01,  1.1766e+00,  3.2831e-01,\n",
       "           2.7463e-01, -2.4430e-02, -4.0674e-01,  5.0283e-01,  6.8888e-01,\n",
       "           1.1240e-01, -3.8054e-01,  6.5934e-02, -6.2602e-01,  1.3339e-01,\n",
       "          -3.3167e-01,  4.3306e-02, -6.4323e-01, -6.8720e-01, -3.1595e-02,\n",
       "          -6.4985e-01,  7.4700e-01, -5.0015e-01, -6.2740e-01,  1.1236e+00,\n",
       "          -6.9697e-02, -8.1635e-01, -1.9673e-02, -6.5889e-02, -3.2569e-02,\n",
       "           4.2480e-01, -4.8756e-01,  2.1934e-01, -6.5235e-01, -6.6141e-01,\n",
       "          -7.8216e-02,  2.3134e-01, -9.7027e-02, -2.7521e-01,  1.4148e-01,\n",
       "           2.5657e-01,  2.2111e-01,  1.1657e+00,  4.1412e-01,  4.6133e-01,\n",
       "          -3.7757e-01, -8.0539e-01, -1.2181e-01, -5.1488e-02, -1.0198e-01,\n",
       "           7.3977e-01, -4.3945e-01,  1.8295e-01, -5.4279e-01,  6.7366e-01,\n",
       "           6.5178e-02, -1.1473e-01,  2.2861e-01,  7.9934e-02, -1.9251e-01,\n",
       "           1.1049e-01, -1.4996e-01, -5.4180e-01, -7.2600e-01,  2.2198e-02,\n",
       "           2.8628e-01, -3.8936e-01,  3.1211e-01, -6.5666e-01, -2.6273e-01,\n",
       "           8.6633e-02, -1.1114e+00, -6.0303e-01,  4.9688e-01, -1.8599e-01,\n",
       "          -2.2007e-01, -9.2576e-01,  1.1260e+00,  2.1478e-01,  6.2107e-01,\n",
       "           1.4413e+00,  1.0928e+00,  7.2858e-01,  5.7935e-03, -7.0988e-01,\n",
       "          -1.8991e-02,  2.6676e-01, -4.0584e-01, -1.5072e-01,  3.4060e-01,\n",
       "          -1.3636e+00,  2.9522e-01,  4.4813e-01, -1.1351e+00, -4.3622e-02,\n",
       "          -3.5761e-01,  1.6781e-02,  3.3663e-01,  5.7346e-02, -4.5389e-01,\n",
       "          -7.4884e-02, -2.5031e-01, -9.3544e-02,  3.6636e-01,  2.6193e-01,\n",
       "           1.2648e-01, -5.3385e-01, -3.8216e-01,  3.6900e-01, -4.5524e-01,\n",
       "           1.5273e-01, -3.1704e-01,  3.8389e-02, -6.4259e-01,  7.3908e-04,\n",
       "           1.3356e-01,  1.9521e-01,  5.3425e-01, -7.7229e-01,  6.3780e-01,\n",
       "           3.7297e-02, -8.9557e-01, -5.0720e-01, -3.8165e-01,  1.1355e+00,\n",
       "          -1.9222e-01,  5.3754e-02,  6.1322e-03,  4.2472e-01, -2.1601e-01,\n",
       "          -3.2917e-01,  4.9415e-01, -7.5613e-01,  1.5917e-01,  2.3985e-01,\n",
       "          -2.4819e-01, -2.2880e-01,  1.7150e-01, -9.0013e-02,  4.2296e-01,\n",
       "          -3.0832e-01, -1.6945e-01,  1.6336e+00, -2.3403e-01, -1.1281e-01],\n",
       "         [ 1.4156e-01, -5.0678e-01,  7.7966e-01,  8.0718e-01,  6.0686e-01,\n",
       "          -1.0141e-02,  5.4786e-01,  7.4213e-01,  2.8938e-01,  2.5785e-01,\n",
       "           4.7424e-01, -6.1639e-02,  7.4091e-02, -2.9605e-01,  1.8200e-01,\n",
       "          -8.2949e-03, -6.9129e-01, -7.7586e-01,  2.1147e-01, -5.7034e-01,\n",
       "           2.4722e-01,  2.8962e-01, -5.5969e-01, -7.9522e-03,  3.8855e-02,\n",
       "          -2.5700e-01, -6.2678e-02, -6.0297e-01,  3.3606e-01,  4.9242e-01,\n",
       "          -4.8362e-01,  2.2262e-01,  2.0062e-01,  3.4793e-01, -2.1385e-01,\n",
       "           2.0006e-01,  4.1648e-02, -1.1494e+00, -4.2293e-01,  6.3224e-01,\n",
       "           2.2060e-01,  1.3493e-01,  1.5375e-01,  3.9318e-01,  1.9202e-01,\n",
       "           5.1093e-01, -5.6307e-02, -5.0404e-01, -7.2558e-01,  3.0205e-01,\n",
       "          -1.4780e-01,  3.9894e-01,  2.6029e-01,  5.3768e-02,  6.9952e-01,\n",
       "          -6.7680e-01, -6.8947e-01,  1.0851e-01,  4.5234e-01, -2.7717e-01,\n",
       "           7.2645e-01,  3.9561e-02,  1.4238e-01, -2.7265e-01,  3.7847e-02,\n",
       "          -2.6816e-01,  5.7998e-01, -9.5002e-02,  3.4905e-01, -8.3479e-02,\n",
       "          -1.7814e-02,  1.7977e-01, -3.6334e-01,  9.9569e-02,  3.7420e-01,\n",
       "           3.3040e-01,  3.5661e-01, -2.5240e-01,  1.2895e-01,  9.0691e-02,\n",
       "          -5.3317e-01, -4.0063e-01,  4.9722e-01, -2.5452e-01,  2.8151e-01,\n",
       "          -8.7951e-02,  7.7936e-01,  3.5428e-03, -3.3081e-01,  3.2774e-01,\n",
       "          -6.2967e-01, -2.8500e-01,  7.0700e-03,  9.0628e-02, -2.2130e-01,\n",
       "          -2.5204e-01, -6.7136e-01,  5.7639e-01,  3.8927e-01,  2.1198e-02,\n",
       "           6.2152e-01,  8.5489e-02, -6.6080e-01,  6.1719e-01, -6.1704e-01,\n",
       "          -4.2415e-02,  6.7972e-01, -3.0315e-01,  1.3251e-01,  3.0303e-01,\n",
       "           8.2363e-01,  1.6104e-01,  7.9927e-02, -9.5795e-02, -8.5394e-02,\n",
       "          -6.4389e-01, -1.5715e-01, -1.4235e-01,  4.5985e-01, -6.8769e-01,\n",
       "          -5.5812e-01,  5.5923e-01,  2.9001e-01,  8.8750e-02,  6.0387e-01,\n",
       "           1.0140e-02,  1.0790e-01, -6.0872e-01,  3.2680e-01, -1.2027e-01,\n",
       "          -2.6654e-01, -5.5015e-01, -2.6577e-01, -7.7884e-02, -3.1011e-02,\n",
       "           4.6137e-01, -2.2166e-01, -1.1449e-01,  1.6601e-01,  4.9951e-01,\n",
       "           5.0290e-01,  5.5421e-01, -7.0185e-01,  6.0537e-01,  7.5473e-01,\n",
       "           4.2907e-01, -1.6841e-02, -3.5979e-01, -2.9707e-01,  4.6024e-01,\n",
       "          -3.0080e-02, -8.2760e-02,  6.2749e-01, -8.9281e-02,  2.2780e-01,\n",
       "          -5.5291e-01,  2.7622e-02,  5.9610e-01,  6.8398e-01,  9.3030e-01,\n",
       "          -1.7943e-03, -6.3688e-01, -4.9186e-01,  1.0878e-02,  1.4575e-01,\n",
       "          -6.4138e-01, -1.0492e-01,  5.6630e-01, -2.5475e-01,  1.6291e-01,\n",
       "           1.6652e-01,  5.1555e-01, -2.9265e-01,  1.6371e-01,  4.8898e-02,\n",
       "           5.6251e-02,  1.0005e+00,  1.0314e-02,  4.8191e-02,  5.4363e-02,\n",
       "           1.9312e-01, -4.1189e-01, -2.7724e-01,  3.7188e-01,  7.3635e-02,\n",
       "           2.0972e-01,  3.9814e-01, -1.0235e-01, -3.3091e-01,  2.5060e-01,\n",
       "           7.4936e-01,  1.8735e-01, -6.0726e-01,  2.4869e-01,  5.1823e-01,\n",
       "           7.6680e-02,  5.5211e-01,  2.2254e-01, -7.0877e-01, -3.0814e-01],\n",
       "         [ 8.7517e-02, -2.7345e-01, -4.0358e-01,  4.5261e-01,  5.9520e-01,\n",
       "          -2.9361e-01,  7.6543e-01, -1.1811e-01,  8.6190e-02, -1.7683e-01,\n",
       "          -3.5256e-02, -1.3046e+00, -4.4993e-01,  5.9332e-01,  6.7293e-01,\n",
       "          -1.1390e-01, -1.4913e-01, -3.5427e-01, -2.5689e-01,  1.1874e-01,\n",
       "           3.9823e-01,  1.1887e-01, -1.5072e-01,  5.1768e-01,  1.8243e-01,\n",
       "          -5.4341e-01, -5.5553e-01,  6.9274e-03,  1.5833e-02, -1.5042e-02,\n",
       "           3.2457e-01, -2.5263e-02, -6.9379e-01,  8.9511e-01, -8.1090e-01,\n",
       "          -2.8731e-01,  4.9048e-01, -1.1954e-01, -3.0801e-01,  3.4272e-01,\n",
       "           1.2604e-01,  5.1017e-01,  1.9480e-01,  8.4207e-01, -7.2043e-01,\n",
       "           7.5410e-01, -7.0714e-01,  6.2022e-02, -4.4928e-01, -3.0809e-01,\n",
       "           6.3112e-02,  3.7792e-01,  3.7696e-01, -1.4341e-01,  5.9667e-01,\n",
       "          -8.2156e-01,  4.6235e-01, -1.0687e-01,  9.2590e-02, -6.9684e-02,\n",
       "           3.4310e-03, -1.5380e+00, -1.8168e-01, -8.6865e-01, -3.7861e-01,\n",
       "          -4.8969e-02,  4.5914e-01,  1.1621e-02,  3.7716e-01, -1.8969e-01,\n",
       "          -7.2729e-01, -6.2742e-01,  3.2867e-01,  1.7686e-01, -7.4276e-01,\n",
       "          -1.0157e-01,  1.7580e-02,  1.1884e-01, -8.7564e-02,  2.5035e-01,\n",
       "          -6.6906e-01, -1.4886e-01, -3.2211e-01, -8.1222e-01, -4.7639e-01,\n",
       "          -4.7992e-01, -2.8818e-01,  5.1587e-01, -8.5724e-01, -1.9737e-01,\n",
       "           3.0092e-01,  1.7662e-01, -4.1648e-01,  5.7279e-01,  5.8098e-01,\n",
       "           1.1640e-01, -4.2991e-01,  1.6521e-01,  5.4644e-01,  3.9863e-01,\n",
       "           2.3163e-01,  2.5677e-01,  5.7594e-02, -2.5124e-01, -7.1971e-01,\n",
       "          -5.9191e-01, -4.5631e-02,  3.9170e-01,  6.7746e-01,  3.4129e-01,\n",
       "           1.1936e-01, -6.5064e-01, -2.4273e-01,  3.3908e-01,  3.6667e-01,\n",
       "          -3.4387e-01, -2.6435e-01,  5.2533e-02, -1.9481e-01, -5.1249e-01,\n",
       "           1.2265e-01,  6.7221e-01,  2.5098e-01,  1.0872e-01,  4.5318e-01,\n",
       "           7.1141e-01, -5.1394e-01, -2.0935e-01, -2.1140e-01, -3.1978e-01,\n",
       "           4.3257e-02, -8.4355e-02,  6.5692e-01,  1.0299e+00,  7.5366e-02,\n",
       "           3.4035e-01, -3.1163e-01, -4.5705e-04, -7.4522e-01,  6.7055e-03,\n",
       "           1.6613e-01,  4.0905e-02, -4.6555e-02,  4.6919e-01,  5.4006e-01,\n",
       "          -3.8579e-01, -6.0621e-01,  2.1555e-01, -2.8349e-01, -4.2690e-01,\n",
       "          -2.4389e-01, -4.7151e-02,  2.2807e-01,  6.5343e-01,  2.2175e-02,\n",
       "           1.4745e-01,  7.2050e-01, -2.7625e-02,  1.7752e-01, -2.0289e-01,\n",
       "           3.6847e-01,  2.6291e-01,  8.0636e-01, -1.5845e-01, -1.6706e-02,\n",
       "          -3.3238e-01, -1.4072e-01,  7.8498e-01,  8.4102e-01,  2.9727e-01,\n",
       "           6.7397e-01,  3.4847e-02,  2.5653e-01,  2.6768e-01,  5.4768e-01,\n",
       "          -3.7146e-01,  3.7702e-01, -5.8816e-01,  1.0338e-01,  1.5593e-01,\n",
       "           4.7690e-01,  4.1236e-01,  3.9635e-01,  4.8106e-01,  2.2893e-02,\n",
       "          -6.3951e-01, -6.6784e-01,  1.0034e-01, -3.5333e-01,  1.8592e-01,\n",
       "          -2.9962e-01,  3.9484e-01,  4.8660e-01, -6.7564e-02, -1.0215e-01,\n",
       "           2.2868e-02, -1.6731e-01,  8.7103e-01, -3.8299e-01, -4.3645e-01]]),\n",
       " tensor([[-4.4816e-02,  2.6072e-01,  1.4476e-01,  1.3985e-01,  8.2186e-01,\n",
       "          -1.8453e-01,  3.9610e-01, -3.3248e-01, -1.8321e-03, -3.2702e-01,\n",
       "          -1.4955e-01, -7.9797e-01,  3.2212e-01,  5.0457e-01, -1.4466e-02,\n",
       "          -1.5588e-01, -5.8039e-01, -8.2334e-01, -1.2749e-01,  1.2681e-02,\n",
       "           1.7029e-01,  2.0639e-01,  1.8592e-01,  9.1905e-02,  7.9670e-01,\n",
       "           3.7350e-03, -1.1508e-01, -9.2960e-02, -5.0474e-02, -5.2370e-01,\n",
       "           2.4938e-01,  5.0046e-02, -2.2510e-01,  8.2782e-01, -8.8028e-02,\n",
       "          -3.1285e-01,  2.1367e-01, -1.2164e-01,  6.2164e-02,  3.9969e-02,\n",
       "          -1.9242e-01,  5.9761e-01,  8.7907e-01,  2.6625e-01, -6.2292e-01,\n",
       "           3.5723e-01, -5.6910e-01, -2.2975e-02, -4.4734e-01, -2.3245e-01,\n",
       "          -1.6014e-02,  2.4307e-02, -2.9628e-01,  3.4902e-03,  2.8123e-01,\n",
       "          -3.6569e-01, -5.6186e-02, -8.3109e-02,  3.7591e-01, -3.0810e-01,\n",
       "          -3.9157e-01, -5.8018e-01,  4.4514e-02, -3.4728e-01, -1.4072e-02,\n",
       "           9.6888e-02,  4.2398e-02, -2.0702e-01,  1.1550e-03,  4.9811e-01,\n",
       "          -3.3000e-01, -2.3525e-01, -8.9399e-02,  2.0654e-01, -1.4491e-01,\n",
       "          -4.1638e-02,  2.2768e-01,  1.7223e-01, -1.9337e-01,  5.4533e-01,\n",
       "          -1.8066e-01,  4.5702e-02, -1.8615e-01, -5.7819e-01,  3.1169e-02,\n",
       "           1.2292e-02, -3.2395e-01,  5.6787e-01, -3.4982e-01, -1.6895e-01,\n",
       "          -4.9290e-01,  1.7113e-01,  1.2016e-01,  6.5679e-01,  5.1149e-01,\n",
       "           1.3962e-01, -1.4009e-02,  8.0457e-01, -1.6864e-03,  4.5210e-01,\n",
       "           3.1804e-01, -6.4181e-02, -3.4219e-01, -1.9089e-01, -5.3410e-01,\n",
       "          -5.3507e-01, -5.3707e-02, -5.6995e-02,  4.4185e-02,  2.6180e-01,\n",
       "          -5.3361e-01, -5.3296e-01, -3.7533e-01,  5.2777e-01, -4.5300e-02,\n",
       "          -1.5200e-01, -1.8493e-01, -1.5568e-01,  1.2953e-01,  2.1748e-01,\n",
       "          -4.5083e-01,  1.0669e-01, -2.5463e-01,  2.3678e-01,  5.8170e-03,\n",
       "           1.8802e-01, -9.0083e-02, -9.5030e-01, -3.3442e-01, -4.9307e-01,\n",
       "          -7.0966e-02, -5.5689e-01,  3.3150e-01,  1.6025e-01, -2.6116e-01,\n",
       "           6.1389e-02, -5.4729e-01, -1.4360e-01, -3.0535e-01, -3.5357e-01,\n",
       "          -6.2616e-01, -2.2439e-02, -2.0740e-01,  2.1965e-01,  4.6478e-01,\n",
       "          -1.3669e-01, -1.0145e-01,  1.0842e-01,  5.0507e-03,  2.9468e-01,\n",
       "          -2.0390e-02,  7.9607e-02, -5.0416e-03, -9.2473e-02,  3.1267e-01,\n",
       "           5.8991e-01,  1.1121e+00,  3.1630e-02, -4.6566e-02, -3.7113e-01,\n",
       "          -8.5519e-02, -2.8330e-02, -2.7766e-02, -4.7535e-01, -6.3396e-01,\n",
       "          -7.5772e-02, -3.1621e-03,  3.7479e-02,  1.3982e-01, -5.8328e-02,\n",
       "           3.6327e-01, -4.1039e-02, -9.3303e-02,  6.7044e-02,  3.8285e-01,\n",
       "          -3.5371e-01,  5.5917e-01, -3.6336e-01,  7.0457e-03,  4.0882e-01,\n",
       "           3.3558e-01,  3.1595e-01,  2.6091e-01,  3.2326e-01,  7.1776e-02,\n",
       "          -2.9163e-01,  1.2613e-01,  1.7375e-01, -5.7487e-01,  3.6839e-01,\n",
       "           2.2042e-01,  3.1725e-01, -4.6196e-01, -5.9088e-02, -6.9970e-01,\n",
       "           2.6686e-02, -4.2077e-01,  4.2432e-01, -1.1535e-01, -6.1969e-02],\n",
       "         [-1.4396e-01,  2.1481e-01, -3.5009e-01, -6.5346e-02,  5.2983e-01,\n",
       "           1.1885e-02, -2.6024e-01, -1.1156e-01, -1.1614e-01, -8.3254e-02,\n",
       "          -2.2453e-01,  2.3192e-01, -4.2215e-01, -1.8149e-01, -1.3944e-01,\n",
       "          -2.6138e-01, -1.2130e-01, -7.9088e-01, -3.4227e-01,  1.4570e-01,\n",
       "           3.3751e-02,  2.3919e-01, -8.2738e-02,  1.8252e-01,  2.6087e-01,\n",
       "          -1.6529e-01, -2.8137e-01,  3.6956e-01,  2.8971e-01,  7.7912e-01,\n",
       "          -3.7560e-01, -9.9901e-02,  1.2130e-01,  1.7315e-01, -3.3254e-01,\n",
       "          -4.8140e-01, -5.9787e-01,  3.2991e-01, -2.5274e-01, -2.3591e-01,\n",
       "           1.0740e-02,  8.7820e-02,  3.2987e-01, -4.4031e-02, -6.5200e-01,\n",
       "           1.1049e-01,  7.7680e-02, -4.5371e-01, -5.4696e-01, -6.2639e-01,\n",
       "           3.3679e-03, -4.7706e-01,  2.8693e-01, -1.2150e-01, -9.0868e-02,\n",
       "           1.1261e-01, -2.5957e-01, -2.0086e-01,  3.0610e-01, -3.2639e-01,\n",
       "          -5.7243e-01,  6.4499e-01,  1.2269e-01, -6.9326e-02,  4.5879e-02,\n",
       "           4.5423e-01, -3.4557e-01,  2.1610e-01,  2.7005e-01,  3.6942e-01,\n",
       "          -3.7049e-01, -3.3404e-01, -1.1758e-01,  2.6109e-01,  2.5053e-01,\n",
       "           3.7146e-01, -3.2846e-01, -4.4636e-01,  2.1562e-01,  5.2984e-01,\n",
       "          -6.7958e-02,  2.0044e-01, -7.1392e-01,  1.9113e-01,  3.3152e-01,\n",
       "          -1.4613e-01,  5.7090e-01,  2.5116e-02,  1.2803e-05, -6.3544e-01,\n",
       "           1.0075e-01, -1.1546e+00, -5.6727e-02, -7.8067e-02,  1.4473e-01,\n",
       "          -2.1801e-01, -1.4957e-02,  8.8176e-01,  1.0772e-01,  8.3376e-01,\n",
       "           3.0782e-01,  5.8873e-02,  3.8509e-02, -9.1644e-02, -4.5544e-01,\n",
       "          -1.8400e-01, -2.5928e-01,  2.8496e-01,  1.3513e-01,  2.5362e-01,\n",
       "          -7.2363e-01,  3.7540e-01,  5.1872e-01,  4.2864e-01, -2.7660e-01,\n",
       "           3.4767e-03, -3.1505e-01, -2.3124e-01, -7.6480e-02, -4.4539e-01,\n",
       "           3.4426e-01,  3.6610e-01, -8.0097e-01,  5.5373e-01,  4.6654e-01,\n",
       "           3.9718e-02,  3.1395e-01, -1.1974e+00,  4.4751e-01,  1.6503e-01,\n",
       "          -2.3665e-01, -6.4710e-02, -1.0447e-01,  4.5263e-02,  1.9797e-01,\n",
       "          -1.3300e-01, -3.1619e-01, -1.4163e-01, -2.5632e-01, -7.6195e-01,\n",
       "           5.9320e-01,  1.3063e-01,  3.8656e-01,  3.6273e-01,  2.8524e-01,\n",
       "          -1.1337e-01,  5.2575e-01,  5.7943e-01, -2.0706e-01, -4.8154e-01,\n",
       "          -6.1839e-02, -2.2911e-01,  4.5455e-01,  2.4437e-01,  1.5888e-01,\n",
       "           3.4238e-02,  1.1307e-01,  2.4155e-02,  4.2919e-01,  3.7045e-01,\n",
       "           2.2619e-01, -9.0192e-01,  1.9372e-02,  2.5740e-01, -1.8927e-01,\n",
       "          -2.3282e-01, -3.0733e-01,  5.0908e-01,  2.4381e-01,  2.8834e-01,\n",
       "          -1.9346e-01,  1.8007e-01,  3.9422e-02, -5.6632e-01,  2.3718e-01,\n",
       "          -2.1120e-01,  2.3435e-01, -1.4699e-01, -2.1394e-01, -3.3583e-01,\n",
       "           4.6670e-01,  5.8768e-01,  6.1518e-01,  1.2019e-01,  3.2928e-01,\n",
       "          -1.8714e-02,  3.1321e-01, -4.0959e-01, -4.3951e-02, -1.7004e-02,\n",
       "           4.6783e-02,  9.3191e-02, -8.4512e-01,  1.4903e-02,  6.8291e-01,\n",
       "          -2.0756e-01, -4.8853e-01,  1.6923e-01, -1.7999e-01,  1.4646e-01],\n",
       "         [-6.0200e-02,  6.7795e-01,  4.3946e-01,  2.3110e-01,  5.3686e-01,\n",
       "          -1.6125e-01,  1.4847e-01, -3.3724e-01,  1.1674e-02, -9.3484e-02,\n",
       "           2.9573e-02, -4.9728e-01,  3.8120e-01,  4.6101e-01, -9.8824e-02,\n",
       "          -2.9805e-01, -2.5457e-01, -5.9887e-01, -9.3961e-02, -5.4609e-02,\n",
       "           1.9000e-04,  3.7492e-02,  1.1499e-02,  3.9090e-01,  7.5765e-01,\n",
       "           1.8973e-01, -7.4093e-02, -1.2624e-01,  1.4864e-01, -4.4352e-01,\n",
       "           5.1175e-01,  3.9434e-01, -2.6017e-01,  8.1767e-01,  1.1335e-01,\n",
       "          -2.3792e-01, -2.5978e-01, -4.5919e-01,  1.5570e-01,  8.6123e-02,\n",
       "          -2.3690e-01,  5.1015e-01,  5.4774e-01,  1.9783e-01, -3.9902e-01,\n",
       "          -1.3025e-01, -3.9501e-01,  9.8975e-03, -2.6508e-01, -1.2424e-01,\n",
       "          -5.0638e-02,  8.9328e-02, -2.1367e-01,  8.3521e-02,  3.5002e-01,\n",
       "          -3.7924e-01, -5.2893e-02, -1.6210e-01,  4.5505e-01, -4.7546e-02,\n",
       "          -5.1950e-01, -3.1812e-01,  9.3436e-02, -5.2239e-01,  2.7634e-01,\n",
       "          -1.6042e-01, -2.4219e-01, -9.8085e-02,  8.5785e-02,  1.5771e-01,\n",
       "          -3.4957e-01, -2.8029e-01, -4.0376e-02,  2.4061e-01, -8.1955e-02,\n",
       "          -4.2529e-01,  3.2125e-01,  2.7218e-02,  1.1657e-01,  2.2890e-01,\n",
       "          -1.4386e-01, -1.2758e-01, -8.3897e-02, -2.0104e-01,  3.6062e-01,\n",
       "           3.1880e-02, -3.7200e-01,  2.1298e-01, -2.2907e-01, -1.0683e-01,\n",
       "          -1.9289e-01,  8.6656e-02, -1.6185e-01,  4.0439e-01,  3.0462e-01,\n",
       "           1.8924e-01,  1.2827e-01,  4.6709e-01,  5.7765e-01,  2.3836e-01,\n",
       "           1.3184e-01, -1.6096e-01, -1.6918e-01, -8.2903e-02, -3.3935e-01,\n",
       "          -7.1439e-01, -9.5788e-02, -2.5996e-01,  2.0538e-01,  3.0879e-01,\n",
       "          -4.1917e-01, -2.3519e-01, -1.0509e-01,  3.8413e-01, -1.1683e-01,\n",
       "          -3.0670e-01, -6.2121e-04, -3.6614e-01, -8.7534e-02, -2.3368e-01,\n",
       "          -5.6445e-01, -3.7052e-02, -2.0083e-01,  2.5548e-01,  2.6027e-01,\n",
       "           3.1072e-01,  9.5195e-02, -9.1055e-01, -3.0402e-01, -2.4066e-01,\n",
       "          -1.1391e-01, -5.6033e-01,  5.8719e-02,  8.2264e-02,  1.6512e-03,\n",
       "           3.1217e-01, -6.3473e-01, -1.2900e-01, -5.1286e-01, -4.3817e-01,\n",
       "          -1.6406e-01,  2.7804e-01, -2.8734e-01,  2.5258e-01,  2.3454e-01,\n",
       "          -2.7116e-01, -9.4259e-02,  3.2925e-02, -1.6711e-01,  5.4052e-01,\n",
       "           1.3911e-02, -2.4934e-01, -9.5650e-02, -1.1411e-01,  1.8863e-01,\n",
       "           2.4827e-01,  6.8400e-01, -3.2786e-01, -1.9717e-01, -1.0976e-01,\n",
       "          -1.1020e-01, -7.5480e-02, -9.4221e-02, -6.5897e-01, -2.4192e-01,\n",
       "          -5.7905e-02,  8.0696e-02,  4.1716e-01,  2.5168e-01, -1.5813e-01,\n",
       "           1.6550e-01, -1.7860e-01, -4.0642e-01, -1.1334e-01,  7.6290e-02,\n",
       "          -1.1712e-01,  4.4572e-01, -5.0249e-01, -1.4953e-01,  4.0058e-01,\n",
       "           3.8590e-01,  2.8534e-01,  3.2363e-01,  1.0158e-01, -5.4234e-02,\n",
       "          -4.1939e-01,  4.9763e-01,  2.7607e-01, -4.4584e-01,  3.2613e-01,\n",
       "           3.6119e-01,  7.7879e-02, -4.8412e-01, -2.2078e-01, -8.7630e-01,\n",
       "          -7.7526e-02, -7.8600e-02,  3.1040e-01, -7.7514e-02,  5.5448e-01],\n",
       "         [-4.4816e-02,  2.6072e-01,  1.4476e-01,  1.3985e-01,  8.2186e-01,\n",
       "          -1.8453e-01,  3.9610e-01, -3.3248e-01, -1.8321e-03, -3.2702e-01,\n",
       "          -1.4955e-01, -7.9797e-01,  3.2212e-01,  5.0457e-01, -1.4466e-02,\n",
       "          -1.5588e-01, -5.8039e-01, -8.2334e-01, -1.2749e-01,  1.2681e-02,\n",
       "           1.7029e-01,  2.0639e-01,  1.8592e-01,  9.1905e-02,  7.9670e-01,\n",
       "           3.7350e-03, -1.1508e-01, -9.2960e-02, -5.0474e-02, -5.2370e-01,\n",
       "           2.4938e-01,  5.0046e-02, -2.2510e-01,  8.2782e-01, -8.8028e-02,\n",
       "          -3.1285e-01,  2.1367e-01, -1.2164e-01,  6.2164e-02,  3.9969e-02,\n",
       "          -1.9242e-01,  5.9761e-01,  8.7907e-01,  2.6625e-01, -6.2292e-01,\n",
       "           3.5723e-01, -5.6910e-01, -2.2975e-02, -4.4734e-01, -2.3245e-01,\n",
       "          -1.6014e-02,  2.4307e-02, -2.9628e-01,  3.4902e-03,  2.8123e-01,\n",
       "          -3.6569e-01, -5.6186e-02, -8.3109e-02,  3.7591e-01, -3.0810e-01,\n",
       "          -3.9157e-01, -5.8018e-01,  4.4514e-02, -3.4728e-01, -1.4072e-02,\n",
       "           9.6888e-02,  4.2398e-02, -2.0702e-01,  1.1550e-03,  4.9811e-01,\n",
       "          -3.3000e-01, -2.3525e-01, -8.9399e-02,  2.0654e-01, -1.4491e-01,\n",
       "          -4.1638e-02,  2.2768e-01,  1.7223e-01, -1.9337e-01,  5.4533e-01,\n",
       "          -1.8066e-01,  4.5702e-02, -1.8615e-01, -5.7819e-01,  3.1169e-02,\n",
       "           1.2292e-02, -3.2395e-01,  5.6787e-01, -3.4982e-01, -1.6895e-01,\n",
       "          -4.9290e-01,  1.7113e-01,  1.2016e-01,  6.5679e-01,  5.1149e-01,\n",
       "           1.3962e-01, -1.4009e-02,  8.0457e-01, -1.6864e-03,  4.5210e-01,\n",
       "           3.1804e-01, -6.4181e-02, -3.4219e-01, -1.9089e-01, -5.3410e-01,\n",
       "          -5.3507e-01, -5.3707e-02, -5.6995e-02,  4.4185e-02,  2.6180e-01,\n",
       "          -5.3361e-01, -5.3296e-01, -3.7533e-01,  5.2777e-01, -4.5300e-02,\n",
       "          -1.5200e-01, -1.8493e-01, -1.5568e-01,  1.2953e-01,  2.1748e-01,\n",
       "          -4.5083e-01,  1.0669e-01, -2.5463e-01,  2.3678e-01,  5.8170e-03,\n",
       "           1.8802e-01, -9.0083e-02, -9.5030e-01, -3.3442e-01, -4.9307e-01,\n",
       "          -7.0966e-02, -5.5689e-01,  3.3150e-01,  1.6025e-01, -2.6116e-01,\n",
       "           6.1389e-02, -5.4729e-01, -1.4360e-01, -3.0535e-01, -3.5357e-01,\n",
       "          -6.2616e-01, -2.2439e-02, -2.0740e-01,  2.1965e-01,  4.6478e-01,\n",
       "          -1.3669e-01, -1.0145e-01,  1.0842e-01,  5.0507e-03,  2.9468e-01,\n",
       "          -2.0390e-02,  7.9607e-02, -5.0416e-03, -9.2473e-02,  3.1267e-01,\n",
       "           5.8991e-01,  1.1121e+00,  3.1630e-02, -4.6566e-02, -3.7113e-01,\n",
       "          -8.5519e-02, -2.8330e-02, -2.7766e-02, -4.7535e-01, -6.3396e-01,\n",
       "          -7.5772e-02, -3.1621e-03,  3.7479e-02,  1.3982e-01, -5.8328e-02,\n",
       "           3.6327e-01, -4.1039e-02, -9.3303e-02,  6.7044e-02,  3.8285e-01,\n",
       "          -3.5371e-01,  5.5917e-01, -3.6336e-01,  7.0457e-03,  4.0882e-01,\n",
       "           3.3558e-01,  3.1595e-01,  2.6091e-01,  3.2326e-01,  7.1776e-02,\n",
       "          -2.9163e-01,  1.2613e-01,  1.7375e-01, -5.7487e-01,  3.6839e-01,\n",
       "           2.2042e-01,  3.1725e-01, -4.6196e-01, -5.9088e-02, -6.9970e-01,\n",
       "           2.6686e-02, -4.2077e-01,  4.2432e-01, -1.1535e-01, -6.1969e-02],\n",
       "         [-9.2930e-01, -9.2135e-03,  4.2353e-01,  1.6734e-01,  4.4009e-01,\n",
       "          -2.2296e-01,  4.1152e-01, -2.6278e-02, -1.5302e-01, -6.8330e-01,\n",
       "           3.6782e-02, -2.5053e-01, -4.6807e-01, -3.0188e-02,  4.0027e-01,\n",
       "          -1.6179e-01,  2.3475e-01, -7.3425e-01,  2.2147e-01, -1.4457e-01,\n",
       "           2.6918e-01,  3.8407e-01, -3.5920e-03,  2.0688e-02,  4.6355e-01,\n",
       "          -1.3068e-01, -6.8304e-01,  9.9794e-02,  2.4794e-01, -6.1920e-01,\n",
       "          -1.8514e-01,  3.2041e-01, -4.5715e-02,  4.9203e-01, -3.6473e-01,\n",
       "          -8.9405e-02, -5.0924e-01, -2.8503e-01, -4.3668e-02,  5.4589e-02,\n",
       "          -1.9689e-01, -1.8938e-01, -2.5355e-01,  3.2896e-01, -5.9109e-01,\n",
       "          -1.6952e-01,  5.4394e-02,  1.8031e-01, -4.9719e-01, -4.6237e-01,\n",
       "          -3.4295e-01, -1.6637e-01, -1.4335e-01, -2.1018e-01,  6.0869e-01,\n",
       "           1.1496e-02, -2.5030e-01, -3.8457e-01,  6.3989e-01,  3.2420e-01,\n",
       "           2.5028e-01, -1.5940e-02, -2.7241e-01,  2.6412e-01, -2.0011e-02,\n",
       "          -3.2361e-01, -1.4562e-01,  9.0856e-02, -8.9571e-02, -1.0344e-01,\n",
       "          -3.1588e-01,  1.2845e-01,  6.7745e-01,  4.8054e-02, -2.1955e-01,\n",
       "          -2.4669e-01, -6.4512e-02, -2.9918e-01,  9.8561e-02, -1.1140e-02,\n",
       "          -3.8459e-01, -6.3687e-01, -1.6964e-01, -1.2895e-01,  2.6887e-01,\n",
       "          -9.9578e-03,  1.3532e-01,  3.2465e-01, -5.0565e-01, -3.5743e-01,\n",
       "          -3.5666e-01, -5.7944e-01, -9.2758e-01,  2.5207e-01,  2.6595e-01,\n",
       "          -5.2561e-02,  1.3124e-01, -6.2583e-02,  6.1142e-01,  1.6781e-01,\n",
       "          -1.7657e-02,  9.4858e-01, -2.3061e-01,  1.2464e-01, -2.9930e-01,\n",
       "           1.7431e-01,  2.0563e-01, -2.1501e-01,  9.2296e-02,  2.3111e-01,\n",
       "           2.4290e-02, -2.6300e-02,  5.1902e-02,  4.2089e-01, -3.9684e-01,\n",
       "           3.1285e-01, -4.0177e-01, -1.8316e-01, -2.0869e-01, -2.0673e-01,\n",
       "           2.7119e-01,  1.2501e+00, -1.8888e-01,  5.6085e-02,  1.1932e-01,\n",
       "           3.8024e-01, -5.7864e-01,  1.5977e-01,  2.5242e-01, -6.8198e-01,\n",
       "           3.0098e-01,  4.1073e-01,  3.8200e-01,  3.5848e-01, -5.0015e-01,\n",
       "          -4.7571e-01, -2.5609e-01,  3.1861e-01, -5.3759e-01,  2.0168e-01,\n",
       "          -7.4545e-02,  4.7931e-01, -9.3755e-02,  2.8486e-01,  5.1019e-01,\n",
       "          -1.2590e-01, -1.9318e-01, -1.8902e-01, -2.8591e-01, -1.5392e-01,\n",
       "           2.5834e-02,  4.5554e-01,  1.1442e-01,  3.7914e-02,  3.3602e-01,\n",
       "          -5.3507e-01,  3.0078e-01,  1.9925e-01,  9.2256e-01, -5.3039e-01,\n",
       "           2.3933e-01, -6.3226e-01, -6.8685e-02,  4.7607e-01,  7.1693e-01,\n",
       "          -6.6275e-01, -2.5663e-01,  3.7022e-01,  5.6524e-01,  1.0342e+00,\n",
       "           5.6138e-02,  4.7404e-01, -3.1056e-02, -2.3646e-01,  5.3365e-01,\n",
       "          -2.9200e-03,  1.9827e-01, -3.2508e-01, -3.8627e-01,  2.2466e-01,\n",
       "           1.5874e-01, -6.3870e-02, -1.0380e-01,  1.0451e-01,  8.9899e-02,\n",
       "          -4.0556e-01,  5.5388e-01,  3.8112e-02, -3.9193e-01,  4.7238e-01,\n",
       "          -6.6472e-02,  4.8676e-01,  5.2961e-02,  1.9852e-01,  6.0552e-01,\n",
       "          -1.1039e-01, -3.0687e-01,  2.9094e-01, -6.0585e-01, -3.0507e-01]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_input_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_input_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_input_tensor = torch.stack(multiple_input_vectors).to(DEVICE)\n",
    "\n",
    "multiple_output = Lstm(multiple_input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1192, 0.1045],\n",
       "        [0.0689, 0.1744],\n",
       "        [0.1400, 0.0759]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_loss = loss_function(\n",
    "  multiple_output,\n",
    "  LongTensor(TARGET_INPUTS).to(DEVICE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7245, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving for Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_softmax = torch.softmax(\n",
    "  multiple_output,\n",
    "  dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5037, 0.4963],\n",
       "        [0.4736, 0.5263],\n",
       "        [0.5160, 0.4840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log = -np.log(\n",
    "  multiple_softmax.cpu().detach().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.685826  , 0.70052224],\n",
       "       [0.7472867 , 0.64178896],\n",
       "       [0.6616183 , 0.7257025 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_sum = negative_log[0][1] + negative_log[1][0] + negative_log[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1735115"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_log_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7245038350423177"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_log_sum / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss.backward() and optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = OPTIMIZER(Lstm.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "lstm.weight_ih_l0\n",
      "Data:\n",
      "tensor([[ 0.0861,  0.0419, -0.1367,  ...,  0.0496,  0.0273,  0.1175],\n",
      "        [-0.0919, -0.0815,  0.0015,  ...,  0.0817, -0.0987, -0.0244],\n",
      "        [ 0.1151,  0.0516, -0.0128,  ..., -0.0300, -0.1070,  0.1008],\n",
      "        ...,\n",
      "        [-0.0939,  0.1310,  0.0965,  ...,  0.1209, -0.0100,  0.0700],\n",
      "        [ 0.0940,  0.1238,  0.0672,  ..., -0.1023, -0.0976,  0.0371],\n",
      "        [ 0.0433, -0.1408,  0.0970,  ..., -0.0617,  0.0175,  0.1103]],\n",
      "       device='cuda:0')\n",
      "Grad:\n",
      "None\n",
      "Shape: torch.Size([200, 200])\n",
      "-----\n",
      "lstm.weight_hh_l0\n",
      "Data:\n",
      "tensor([[ 0.0126,  0.0527, -0.1170,  ..., -0.0755, -0.1405, -0.0456],\n",
      "        [-0.0288,  0.1148,  0.0838,  ...,  0.0740, -0.0059,  0.0544],\n",
      "        [ 0.0362, -0.0175,  0.0392,  ..., -0.0416, -0.0988,  0.1178],\n",
      "        ...,\n",
      "        [ 0.1031, -0.0318, -0.0704,  ..., -0.0499,  0.0895,  0.0116],\n",
      "        [ 0.0801,  0.1109, -0.0494,  ...,  0.0293, -0.1004, -0.0431],\n",
      "        [ 0.0087, -0.0532,  0.1160,  ...,  0.0101,  0.0443, -0.0656]],\n",
      "       device='cuda:0')\n",
      "Grad:\n",
      "None\n",
      "Shape: torch.Size([200, 50])\n",
      "-----\n",
      "lstm.bias_ih_l0\n",
      "Data:\n",
      "tensor([ 0.0387,  0.1371, -0.0188, -0.0116,  0.0424,  0.0965,  0.0203, -0.0370,\n",
      "        -0.1031, -0.0423, -0.1297, -0.0802,  0.1015,  0.0611,  0.0374, -0.0510,\n",
      "         0.0585, -0.1289,  0.0193,  0.0963, -0.0218,  0.1175,  0.0813,  0.1222,\n",
      "        -0.1349, -0.0437,  0.1294, -0.0843,  0.0870, -0.0141,  0.0453,  0.1041,\n",
      "         0.0773, -0.0624,  0.1339, -0.0570,  0.1135, -0.0921, -0.0146, -0.0107,\n",
      "        -0.0043, -0.0162,  0.0244, -0.0928, -0.0706, -0.0497, -0.0969,  0.0574,\n",
      "        -0.0104, -0.1245,  0.0563,  0.0942, -0.1331,  0.0641, -0.0836,  0.0340,\n",
      "         0.1332, -0.0658,  0.0868,  0.0891, -0.0119, -0.0451, -0.0358, -0.0501,\n",
      "         0.0586,  0.1270,  0.0872, -0.0490,  0.0900, -0.0683, -0.1259, -0.0292,\n",
      "         0.0244,  0.0243,  0.0367, -0.1299, -0.0711,  0.0493,  0.0019,  0.0958,\n",
      "         0.0951,  0.0922, -0.0687, -0.1220,  0.1302, -0.0391,  0.0909, -0.0043,\n",
      "        -0.0171, -0.0023, -0.1300, -0.1142, -0.1354, -0.1025, -0.0981,  0.0225,\n",
      "         0.0394,  0.0041,  0.0868, -0.1290, -0.0561, -0.0987,  0.0527,  0.0804,\n",
      "        -0.0763, -0.0448,  0.0307, -0.0962, -0.0780,  0.0775,  0.0057, -0.0248,\n",
      "         0.0545, -0.0018,  0.0886,  0.0090, -0.0191, -0.0056, -0.0748,  0.0219,\n",
      "        -0.1290,  0.0187,  0.0272, -0.1187,  0.0660,  0.1355, -0.0345, -0.1383,\n",
      "        -0.1048,  0.0474,  0.0858,  0.1065, -0.0507, -0.0260, -0.0307, -0.1160,\n",
      "        -0.1340, -0.1258,  0.0321, -0.0669,  0.0807,  0.1157,  0.0231, -0.0599,\n",
      "        -0.0271, -0.0512,  0.0173, -0.0703,  0.1198,  0.0379, -0.1233,  0.1262,\n",
      "        -0.0682, -0.0366,  0.1176, -0.0908, -0.0226, -0.0871, -0.0848,  0.0640,\n",
      "         0.1324, -0.0891,  0.0796,  0.1091,  0.0825,  0.0706, -0.0594,  0.0641,\n",
      "         0.0459, -0.0406, -0.0277,  0.0704, -0.1142,  0.1288,  0.0073,  0.0589,\n",
      "        -0.0409, -0.0996,  0.1083, -0.1363,  0.0487,  0.0528,  0.0179, -0.1217,\n",
      "        -0.0113,  0.0974,  0.1319, -0.0869, -0.1373, -0.0327,  0.1113,  0.0291,\n",
      "         0.0900,  0.0945,  0.0531,  0.1148, -0.1228,  0.0184, -0.0491, -0.1248],\n",
      "       device='cuda:0')\n",
      "Grad:\n",
      "None\n",
      "Shape: torch.Size([200])\n",
      "-----\n",
      "lstm.bias_hh_l0\n",
      "Data:\n",
      "tensor([-0.0022, -0.0548, -0.0820,  0.0472,  0.0885, -0.1339, -0.1163, -0.0689,\n",
      "        -0.0371,  0.1380,  0.1047, -0.1190,  0.0444, -0.0186,  0.0189, -0.0381,\n",
      "        -0.0924, -0.1305, -0.1335,  0.0386, -0.0450,  0.0892,  0.1018,  0.0434,\n",
      "         0.0642,  0.0457,  0.0926,  0.0901, -0.0892, -0.0403,  0.0238,  0.1310,\n",
      "         0.0302, -0.1281,  0.0447,  0.0755, -0.0734,  0.0630,  0.0931, -0.0540,\n",
      "        -0.0630,  0.1105,  0.0672, -0.0289, -0.0182, -0.0725,  0.1242, -0.1097,\n",
      "         0.0378, -0.1261,  0.1315,  0.0048, -0.0584, -0.0364, -0.1181,  0.0579,\n",
      "         0.0638,  0.0676, -0.0690, -0.1327,  0.0399,  0.0302, -0.0913,  0.0255,\n",
      "        -0.1085,  0.0093, -0.1317,  0.0128, -0.0263, -0.1011,  0.0766,  0.0912,\n",
      "        -0.1348,  0.0704,  0.0984,  0.0368, -0.0805, -0.0709, -0.0923,  0.0684,\n",
      "        -0.0420,  0.1405, -0.0877,  0.0981, -0.1100, -0.0452, -0.0231,  0.1399,\n",
      "         0.0144,  0.0440,  0.0804,  0.0107, -0.0744, -0.0109, -0.0454,  0.0216,\n",
      "        -0.1096, -0.0270,  0.0548,  0.0806, -0.1218,  0.0425, -0.0444, -0.1232,\n",
      "         0.0349, -0.1069,  0.0895,  0.0144,  0.0295, -0.0607,  0.1354,  0.0053,\n",
      "        -0.0191,  0.0644,  0.1013,  0.1068, -0.1236, -0.0187,  0.0200,  0.1325,\n",
      "         0.0816,  0.0382, -0.0437, -0.1327,  0.0493,  0.1325,  0.0969,  0.1371,\n",
      "        -0.0683, -0.1394,  0.0078,  0.0415, -0.0302,  0.0745,  0.0071, -0.0465,\n",
      "         0.0938,  0.0766, -0.0190, -0.1319,  0.0180,  0.0889, -0.1403,  0.1227,\n",
      "         0.1006, -0.1041,  0.0216,  0.0981,  0.0773, -0.0982, -0.0342,  0.0680,\n",
      "         0.0511,  0.1198,  0.0767, -0.0575,  0.0044, -0.0532, -0.0521, -0.0731,\n",
      "        -0.0121, -0.0100, -0.0892, -0.0141, -0.1318,  0.0527, -0.0894, -0.1402,\n",
      "        -0.1237,  0.1105, -0.0943, -0.1410, -0.0438,  0.0714, -0.0551,  0.0095,\n",
      "         0.0566,  0.1105, -0.0196, -0.1024, -0.1348,  0.1260, -0.0674,  0.0844,\n",
      "        -0.1142, -0.0520,  0.0361, -0.0760,  0.0315, -0.1340, -0.1412,  0.0342,\n",
      "        -0.1106, -0.0142,  0.1025, -0.0194,  0.1303,  0.0943, -0.0814, -0.1100],\n",
      "       device='cuda:0')\n",
      "Grad:\n",
      "None\n",
      "Shape: torch.Size([200])\n",
      "-----\n",
      "linear.weight\n",
      "Data:\n",
      "tensor([[-0.1167, -0.1213, -0.1174,  0.0250,  0.0793,  0.0886,  0.0337,  0.0933,\n",
      "          0.0632, -0.0156, -0.0388, -0.1288,  0.0059,  0.0654,  0.0872,  0.0092,\n",
      "         -0.0803, -0.0248, -0.0821, -0.0365,  0.0457, -0.0313,  0.0147, -0.1014,\n",
      "          0.1051,  0.0473,  0.0560, -0.0637, -0.1150, -0.0079, -0.0235, -0.0784,\n",
      "          0.0343,  0.0823,  0.0049, -0.1124, -0.0760,  0.1391, -0.0243, -0.0617,\n",
      "          0.0747, -0.1365, -0.1141, -0.0076, -0.0009, -0.0289, -0.0311, -0.0516,\n",
      "          0.0210,  0.0946],\n",
      "        [ 0.1289,  0.0538, -0.1256, -0.0383,  0.1222, -0.1356,  0.1273,  0.1238,\n",
      "         -0.0169, -0.0788,  0.0172,  0.0007,  0.0994, -0.1335, -0.1007,  0.0299,\n",
      "          0.1412,  0.0414, -0.0363,  0.0951,  0.0670, -0.0714, -0.1199,  0.0622,\n",
      "          0.0819,  0.0924, -0.1068, -0.0484, -0.1044,  0.0042,  0.1336, -0.1199,\n",
      "         -0.0888,  0.1161,  0.0801, -0.0822, -0.1238,  0.0364, -0.0959, -0.0645,\n",
      "         -0.0487,  0.0234,  0.1357, -0.0301, -0.0649, -0.1182, -0.1214, -0.0482,\n",
      "         -0.0551, -0.1346]], device='cuda:0')\n",
      "Grad:\n",
      "None\n",
      "Shape: torch.Size([2, 50])\n",
      "-----\n",
      "linear.bias\n",
      "Data:\n",
      "tensor([0.0392, 0.0442], device='cuda:0')\n",
      "Grad:\n",
      "None\n",
      "Shape: torch.Size([2])\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(\"-----\")\n",
    "for name, param in Lstm.named_parameters():\n",
    "  if param.requires_grad: \n",
    "    print(name)\n",
    "    print(\"Data:\")\n",
    "    print(param.data)\n",
    "    print(\"Grad:\")\n",
    "    print(param.grad)\n",
    "    print(f\"Shape: {param.data.shape}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate gradients in parameters\n",
    "output_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "lstm.weight_ih_l0\n",
      "Data:\n",
      "tensor([[ 0.0861,  0.0419, -0.1367,  ...,  0.0496,  0.0273,  0.1175],\n",
      "        [-0.0919, -0.0815,  0.0015,  ...,  0.0817, -0.0987, -0.0244],\n",
      "        [ 0.1151,  0.0516, -0.0128,  ..., -0.0300, -0.1070,  0.1008],\n",
      "        ...,\n",
      "        [-0.0939,  0.1310,  0.0965,  ...,  0.1209, -0.0100,  0.0700],\n",
      "        [ 0.0940,  0.1238,  0.0672,  ..., -0.1023, -0.0976,  0.0371],\n",
      "        [ 0.0433, -0.1408,  0.0970,  ..., -0.0617,  0.0175,  0.1103]],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200, 200])\n",
      "-----\n",
      "Grad:\n",
      "tensor([[-1.0380e-03,  4.2402e-03,  1.0249e-03,  ...,  9.9626e-03,\n",
      "         -7.3713e-04, -4.7700e-04],\n",
      "        [-1.3368e-03,  3.9806e-03, -4.5642e-03,  ...,  6.7374e-04,\n",
      "          4.1394e-03,  1.7101e-03],\n",
      "        [ 1.4198e-05, -8.4338e-05,  1.3620e-04,  ...,  8.9446e-05,\n",
      "         -1.2793e-04, -6.5994e-05],\n",
      "        ...,\n",
      "        [-1.3210e-04,  6.8898e-05,  2.1496e-05,  ...,  3.9388e-04,\n",
      "         -3.8631e-05, -1.2061e-04],\n",
      "        [ 3.8427e-05, -7.0212e-04,  7.5171e-04,  ...,  2.0043e-04,\n",
      "         -7.2524e-04, -4.5628e-04],\n",
      "        [-2.5168e-05,  3.3328e-04, -1.9289e-05,  ...,  5.4954e-04,\n",
      "          5.4513e-05,  8.4352e-05]], device='cuda:0')\n",
      "Grad Shape: torch.Size([200, 200])\n",
      "---------\n",
      "lstm.weight_hh_l0\n",
      "Data:\n",
      "tensor([[ 0.0126,  0.0527, -0.1170,  ..., -0.0755, -0.1405, -0.0456],\n",
      "        [-0.0288,  0.1148,  0.0838,  ...,  0.0740, -0.0059,  0.0544],\n",
      "        [ 0.0362, -0.0175,  0.0392,  ..., -0.0416, -0.0988,  0.1178],\n",
      "        ...,\n",
      "        [ 0.1031, -0.0318, -0.0704,  ..., -0.0499,  0.0895,  0.0116],\n",
      "        [ 0.0801,  0.1109, -0.0494,  ...,  0.0293, -0.1004, -0.0431],\n",
      "        [ 0.0087, -0.0532,  0.1160,  ...,  0.0101,  0.0443, -0.0656]],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200, 50])\n",
      "-----\n",
      "Grad:\n",
      "tensor([[-7.7188e-04,  1.4524e-04, -2.4418e-04,  ..., -7.6579e-05,\n",
      "         -6.9937e-04,  1.5936e-04],\n",
      "        [ 6.0789e-04,  5.0090e-04, -1.8055e-04,  ...,  1.8080e-03,\n",
      "         -1.8552e-04,  3.0596e-04],\n",
      "        [-2.7918e-05, -1.0984e-05,  6.7570e-07,  ..., -4.7479e-05,\n",
      "          1.9100e-06, -3.7915e-06],\n",
      "        ...,\n",
      "        [-5.5004e-05, -2.0386e-05, -2.9297e-07,  ...,  1.9466e-05,\n",
      "          2.7494e-05,  3.2866e-05],\n",
      "        [-1.4864e-04, -8.4944e-05,  1.5793e-05,  ..., -2.5988e-04,\n",
      "          6.7558e-05, -9.3009e-06],\n",
      "        [-1.2966e-05,  4.4297e-05, -2.1628e-05,  ...,  3.9382e-05,\n",
      "         -5.3162e-05,  6.3772e-06]], device='cuda:0')\n",
      "Grad Shape: torch.Size([200, 50])\n",
      "---------\n",
      "lstm.bias_ih_l0\n",
      "Data:\n",
      "tensor([ 0.0387,  0.1371, -0.0188, -0.0116,  0.0424,  0.0965,  0.0203, -0.0370,\n",
      "        -0.1031, -0.0423, -0.1297, -0.0802,  0.1015,  0.0611,  0.0374, -0.0510,\n",
      "         0.0585, -0.1289,  0.0193,  0.0963, -0.0218,  0.1175,  0.0813,  0.1222,\n",
      "        -0.1349, -0.0437,  0.1294, -0.0843,  0.0870, -0.0141,  0.0453,  0.1041,\n",
      "         0.0773, -0.0624,  0.1339, -0.0570,  0.1135, -0.0921, -0.0146, -0.0107,\n",
      "        -0.0043, -0.0162,  0.0244, -0.0928, -0.0706, -0.0497, -0.0969,  0.0574,\n",
      "        -0.0104, -0.1245,  0.0563,  0.0942, -0.1331,  0.0641, -0.0836,  0.0340,\n",
      "         0.1332, -0.0658,  0.0868,  0.0891, -0.0119, -0.0451, -0.0358, -0.0501,\n",
      "         0.0586,  0.1270,  0.0872, -0.0490,  0.0900, -0.0683, -0.1259, -0.0292,\n",
      "         0.0244,  0.0243,  0.0367, -0.1299, -0.0711,  0.0493,  0.0019,  0.0958,\n",
      "         0.0951,  0.0922, -0.0687, -0.1220,  0.1302, -0.0391,  0.0909, -0.0043,\n",
      "        -0.0171, -0.0023, -0.1300, -0.1142, -0.1354, -0.1025, -0.0981,  0.0225,\n",
      "         0.0394,  0.0041,  0.0868, -0.1290, -0.0561, -0.0987,  0.0527,  0.0804,\n",
      "        -0.0763, -0.0448,  0.0307, -0.0962, -0.0780,  0.0775,  0.0057, -0.0248,\n",
      "         0.0545, -0.0018,  0.0886,  0.0090, -0.0191, -0.0056, -0.0748,  0.0219,\n",
      "        -0.1290,  0.0187,  0.0272, -0.1187,  0.0660,  0.1355, -0.0345, -0.1383,\n",
      "        -0.1048,  0.0474,  0.0858,  0.1065, -0.0507, -0.0260, -0.0307, -0.1160,\n",
      "        -0.1340, -0.1258,  0.0321, -0.0669,  0.0807,  0.1157,  0.0231, -0.0599,\n",
      "        -0.0271, -0.0512,  0.0173, -0.0703,  0.1198,  0.0379, -0.1233,  0.1262,\n",
      "        -0.0682, -0.0366,  0.1176, -0.0908, -0.0226, -0.0871, -0.0848,  0.0640,\n",
      "         0.1324, -0.0891,  0.0796,  0.1091,  0.0825,  0.0706, -0.0594,  0.0641,\n",
      "         0.0459, -0.0406, -0.0277,  0.0704, -0.1142,  0.1288,  0.0073,  0.0589,\n",
      "        -0.0409, -0.0996,  0.1083, -0.1363,  0.0487,  0.0528,  0.0179, -0.1217,\n",
      "        -0.0113,  0.0974,  0.1319, -0.0869, -0.1373, -0.0327,  0.1113,  0.0291,\n",
      "         0.0900,  0.0945,  0.0531,  0.1148, -0.1228,  0.0184, -0.0491, -0.1248],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200])\n",
      "-----\n",
      "Grad:\n",
      "tensor([ 7.1035e-03, -5.3793e-03,  1.9460e-04,  2.6430e-03, -6.4563e-04,\n",
      "        -1.0372e-03, -2.8241e-03, -6.1618e-04,  9.0679e-04, -2.5431e-03,\n",
      "         2.9554e-04, -3.1750e-04,  4.6441e-04,  6.0487e-03, -2.2954e-03,\n",
      "         7.3221e-05, -6.2189e-03, -2.4223e-03, -1.3726e-03, -2.7884e-03,\n",
      "        -3.1891e-03,  4.6410e-04, -6.3917e-03,  6.3034e-03, -1.4766e-03,\n",
      "        -3.1210e-03, -2.3016e-04,  3.4949e-04,  2.9623e-05, -1.9741e-04,\n",
      "         2.9767e-03,  1.6285e-03,  2.8826e-03, -8.0699e-04,  5.9785e-04,\n",
      "        -1.8049e-04, -1.8258e-03,  3.4121e-03, -2.8504e-05,  6.2162e-04,\n",
      "        -6.1432e-04,  1.7644e-04,  4.2530e-03, -2.5707e-04, -1.8258e-03,\n",
      "        -5.5428e-03, -4.5107e-03,  5.7056e-05,  3.5388e-04, -5.5813e-05,\n",
      "         8.6084e-03,  6.8618e-04, -1.7069e-06,  2.1086e-03, -1.4081e-04,\n",
      "        -2.6804e-03, -4.2064e-03, -5.2198e-04,  3.0682e-03, -1.7730e-03,\n",
      "         8.7104e-04, -4.0226e-03, -3.8615e-03,  3.4125e-03,  1.9516e-03,\n",
      "        -1.9160e-04, -1.8739e-03, -1.3454e-04,  1.1315e-03, -5.1597e-03,\n",
      "        -2.8085e-03, -1.4393e-04, -3.8538e-03,  1.3190e-02, -4.5070e-04,\n",
      "        -7.4094e-04,  7.2363e-04,  5.1822e-05,  4.3872e-04,  3.2349e-04,\n",
      "        -2.0433e-03,  1.5869e-03,  6.4123e-03, -1.4474e-04,  5.0079e-03,\n",
      "        -2.4224e-04, -1.1138e-04,  1.8857e-03,  2.1100e-04,  7.7618e-04,\n",
      "        -3.0900e-03, -7.7233e-03,  8.0648e-03, -7.4954e-04,  9.5439e-05,\n",
      "        -1.4855e-03, -2.0431e-03, -1.0379e-04, -1.4338e-03, -1.4452e-03,\n",
      "        -6.4544e-02, -4.8632e-02,  9.6993e-04,  9.4667e-03, -2.6548e-02,\n",
      "         3.9856e-02, -1.4510e-02, -4.4787e-03,  2.9227e-02,  4.4885e-03,\n",
      "        -1.2822e-02, -3.0446e-02, -1.4225e-02,  2.6738e-02,  3.9543e-02,\n",
      "        -3.7006e-03, -2.9164e-02, -2.0096e-02, -4.7475e-03, -1.3491e-02,\n",
      "         8.7264e-03,  1.2972e-02,  1.6337e-02, -2.9246e-02,  7.8535e-03,\n",
      "        -1.3728e-02,  3.0117e-02, -5.3120e-03, -1.3817e-03, -8.8174e-04,\n",
      "        -1.3053e-02,  1.8073e-02,  1.7737e-02, -1.8225e-03, -4.0056e-02,\n",
      "        -3.1047e-03,  3.2545e-03,  1.2244e-02,  9.7896e-03, -3.6653e-03,\n",
      "         2.2873e-02, -3.9614e-02, -8.1695e-02,  1.3264e-02,  7.4885e-03,\n",
      "         1.9359e-02,  1.6953e-02,  7.2353e-05,  2.5699e-02,  2.4687e-02,\n",
      "         4.9351e-03, -2.7004e-03,  1.6740e-04,  3.8830e-03, -7.2689e-04,\n",
      "        -1.6033e-03, -5.0127e-03, -8.4379e-04,  1.6060e-03, -2.2338e-03,\n",
      "         1.4090e-04, -1.7143e-03,  2.0276e-04,  5.8184e-03, -2.4847e-03,\n",
      "         4.4732e-04, -9.6007e-03, -1.7285e-03, -1.8605e-03, -5.5675e-03,\n",
      "        -2.3374e-03,  5.4337e-04, -5.8304e-03,  8.0169e-03, -1.7643e-03,\n",
      "        -3.0076e-03, -2.8745e-04,  3.8139e-04,  2.5344e-05, -4.1362e-04,\n",
      "         2.6254e-03,  1.9261e-03,  5.4270e-03, -1.1875e-03,  1.2474e-03,\n",
      "        -1.7802e-04, -3.9149e-03,  5.2183e-03, -6.1268e-05,  6.8893e-04,\n",
      "        -8.3409e-04,  8.6783e-04,  5.8296e-03, -1.1908e-07, -1.1290e-03,\n",
      "        -6.3027e-03, -2.8565e-03,  1.0822e-04,  7.5131e-04,  1.6673e-04],\n",
      "       device='cuda:0')\n",
      "Grad Shape: torch.Size([200])\n",
      "---------\n",
      "lstm.bias_hh_l0\n",
      "Data:\n",
      "tensor([-0.0022, -0.0548, -0.0820,  0.0472,  0.0885, -0.1339, -0.1163, -0.0689,\n",
      "        -0.0371,  0.1380,  0.1047, -0.1190,  0.0444, -0.0186,  0.0189, -0.0381,\n",
      "        -0.0924, -0.1305, -0.1335,  0.0386, -0.0450,  0.0892,  0.1018,  0.0434,\n",
      "         0.0642,  0.0457,  0.0926,  0.0901, -0.0892, -0.0403,  0.0238,  0.1310,\n",
      "         0.0302, -0.1281,  0.0447,  0.0755, -0.0734,  0.0630,  0.0931, -0.0540,\n",
      "        -0.0630,  0.1105,  0.0672, -0.0289, -0.0182, -0.0725,  0.1242, -0.1097,\n",
      "         0.0378, -0.1261,  0.1315,  0.0048, -0.0584, -0.0364, -0.1181,  0.0579,\n",
      "         0.0638,  0.0676, -0.0690, -0.1327,  0.0399,  0.0302, -0.0913,  0.0255,\n",
      "        -0.1085,  0.0093, -0.1317,  0.0128, -0.0263, -0.1011,  0.0766,  0.0912,\n",
      "        -0.1348,  0.0704,  0.0984,  0.0368, -0.0805, -0.0709, -0.0923,  0.0684,\n",
      "        -0.0420,  0.1405, -0.0877,  0.0981, -0.1100, -0.0452, -0.0231,  0.1399,\n",
      "         0.0144,  0.0440,  0.0804,  0.0107, -0.0744, -0.0109, -0.0454,  0.0216,\n",
      "        -0.1096, -0.0270,  0.0548,  0.0806, -0.1218,  0.0425, -0.0444, -0.1232,\n",
      "         0.0349, -0.1069,  0.0895,  0.0144,  0.0295, -0.0607,  0.1354,  0.0053,\n",
      "        -0.0191,  0.0644,  0.1013,  0.1068, -0.1236, -0.0187,  0.0200,  0.1325,\n",
      "         0.0816,  0.0382, -0.0437, -0.1327,  0.0493,  0.1325,  0.0969,  0.1371,\n",
      "        -0.0683, -0.1394,  0.0078,  0.0415, -0.0302,  0.0745,  0.0071, -0.0465,\n",
      "         0.0938,  0.0766, -0.0190, -0.1319,  0.0180,  0.0889, -0.1403,  0.1227,\n",
      "         0.1006, -0.1041,  0.0216,  0.0981,  0.0773, -0.0982, -0.0342,  0.0680,\n",
      "         0.0511,  0.1198,  0.0767, -0.0575,  0.0044, -0.0532, -0.0521, -0.0731,\n",
      "        -0.0121, -0.0100, -0.0892, -0.0141, -0.1318,  0.0527, -0.0894, -0.1402,\n",
      "        -0.1237,  0.1105, -0.0943, -0.1410, -0.0438,  0.0714, -0.0551,  0.0095,\n",
      "         0.0566,  0.1105, -0.0196, -0.1024, -0.1348,  0.1260, -0.0674,  0.0844,\n",
      "        -0.1142, -0.0520,  0.0361, -0.0760,  0.0315, -0.1340, -0.1412,  0.0342,\n",
      "        -0.1106, -0.0142,  0.1025, -0.0194,  0.1303,  0.0943, -0.0814, -0.1100],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200])\n",
      "-----\n",
      "Grad:\n",
      "tensor([ 7.1035e-03, -5.3793e-03,  1.9460e-04,  2.6430e-03, -6.4563e-04,\n",
      "        -1.0372e-03, -2.8241e-03, -6.1618e-04,  9.0679e-04, -2.5431e-03,\n",
      "         2.9554e-04, -3.1750e-04,  4.6441e-04,  6.0487e-03, -2.2954e-03,\n",
      "         7.3221e-05, -6.2189e-03, -2.4223e-03, -1.3726e-03, -2.7884e-03,\n",
      "        -3.1891e-03,  4.6410e-04, -6.3917e-03,  6.3034e-03, -1.4766e-03,\n",
      "        -3.1210e-03, -2.3016e-04,  3.4949e-04,  2.9623e-05, -1.9741e-04,\n",
      "         2.9767e-03,  1.6285e-03,  2.8826e-03, -8.0699e-04,  5.9785e-04,\n",
      "        -1.8049e-04, -1.8258e-03,  3.4121e-03, -2.8504e-05,  6.2162e-04,\n",
      "        -6.1432e-04,  1.7644e-04,  4.2530e-03, -2.5707e-04, -1.8258e-03,\n",
      "        -5.5428e-03, -4.5107e-03,  5.7056e-05,  3.5388e-04, -5.5813e-05,\n",
      "         8.6084e-03,  6.8618e-04, -1.7069e-06,  2.1086e-03, -1.4081e-04,\n",
      "        -2.6804e-03, -4.2064e-03, -5.2198e-04,  3.0682e-03, -1.7730e-03,\n",
      "         8.7104e-04, -4.0226e-03, -3.8615e-03,  3.4125e-03,  1.9516e-03,\n",
      "        -1.9160e-04, -1.8739e-03, -1.3454e-04,  1.1315e-03, -5.1597e-03,\n",
      "        -2.8085e-03, -1.4393e-04, -3.8538e-03,  1.3190e-02, -4.5070e-04,\n",
      "        -7.4094e-04,  7.2363e-04,  5.1822e-05,  4.3872e-04,  3.2349e-04,\n",
      "        -2.0433e-03,  1.5869e-03,  6.4123e-03, -1.4474e-04,  5.0079e-03,\n",
      "        -2.4224e-04, -1.1138e-04,  1.8857e-03,  2.1100e-04,  7.7618e-04,\n",
      "        -3.0900e-03, -7.7233e-03,  8.0648e-03, -7.4954e-04,  9.5439e-05,\n",
      "        -1.4855e-03, -2.0431e-03, -1.0379e-04, -1.4338e-03, -1.4452e-03,\n",
      "        -6.4544e-02, -4.8632e-02,  9.6993e-04,  9.4667e-03, -2.6548e-02,\n",
      "         3.9856e-02, -1.4510e-02, -4.4787e-03,  2.9227e-02,  4.4885e-03,\n",
      "        -1.2822e-02, -3.0446e-02, -1.4225e-02,  2.6738e-02,  3.9543e-02,\n",
      "        -3.7006e-03, -2.9164e-02, -2.0096e-02, -4.7475e-03, -1.3491e-02,\n",
      "         8.7264e-03,  1.2972e-02,  1.6337e-02, -2.9246e-02,  7.8535e-03,\n",
      "        -1.3728e-02,  3.0117e-02, -5.3120e-03, -1.3817e-03, -8.8174e-04,\n",
      "        -1.3053e-02,  1.8073e-02,  1.7737e-02, -1.8225e-03, -4.0056e-02,\n",
      "        -3.1047e-03,  3.2545e-03,  1.2244e-02,  9.7896e-03, -3.6653e-03,\n",
      "         2.2873e-02, -3.9614e-02, -8.1695e-02,  1.3264e-02,  7.4885e-03,\n",
      "         1.9359e-02,  1.6953e-02,  7.2353e-05,  2.5699e-02,  2.4687e-02,\n",
      "         4.9351e-03, -2.7004e-03,  1.6740e-04,  3.8830e-03, -7.2689e-04,\n",
      "        -1.6033e-03, -5.0127e-03, -8.4379e-04,  1.6060e-03, -2.2338e-03,\n",
      "         1.4090e-04, -1.7143e-03,  2.0276e-04,  5.8184e-03, -2.4847e-03,\n",
      "         4.4732e-04, -9.6007e-03, -1.7285e-03, -1.8605e-03, -5.5675e-03,\n",
      "        -2.3374e-03,  5.4337e-04, -5.8304e-03,  8.0169e-03, -1.7643e-03,\n",
      "        -3.0076e-03, -2.8745e-04,  3.8139e-04,  2.5344e-05, -4.1362e-04,\n",
      "         2.6254e-03,  1.9261e-03,  5.4270e-03, -1.1875e-03,  1.2474e-03,\n",
      "        -1.7802e-04, -3.9149e-03,  5.2183e-03, -6.1268e-05,  6.8893e-04,\n",
      "        -8.3409e-04,  8.6783e-04,  5.8296e-03, -1.1908e-07, -1.1290e-03,\n",
      "        -6.3027e-03, -2.8565e-03,  1.0822e-04,  7.5131e-04,  1.6673e-04],\n",
      "       device='cuda:0')\n",
      "Grad Shape: torch.Size([200])\n",
      "---------\n",
      "linear.weight\n",
      "Data:\n",
      "tensor([[-0.1167, -0.1213, -0.1174,  0.0250,  0.0793,  0.0886,  0.0337,  0.0933,\n",
      "          0.0632, -0.0156, -0.0388, -0.1288,  0.0059,  0.0654,  0.0872,  0.0092,\n",
      "         -0.0803, -0.0248, -0.0821, -0.0365,  0.0457, -0.0313,  0.0147, -0.1014,\n",
      "          0.1051,  0.0473,  0.0560, -0.0637, -0.1150, -0.0079, -0.0235, -0.0784,\n",
      "          0.0343,  0.0823,  0.0049, -0.1124, -0.0760,  0.1391, -0.0243, -0.0617,\n",
      "          0.0747, -0.1365, -0.1141, -0.0076, -0.0009, -0.0289, -0.0311, -0.0516,\n",
      "          0.0210,  0.0946],\n",
      "        [ 0.1289,  0.0538, -0.1256, -0.0383,  0.1222, -0.1356,  0.1273,  0.1238,\n",
      "         -0.0169, -0.0788,  0.0172,  0.0007,  0.0994, -0.1335, -0.1007,  0.0299,\n",
      "          0.1412,  0.0414, -0.0363,  0.0951,  0.0670, -0.0714, -0.1199,  0.0622,\n",
      "          0.0819,  0.0924, -0.1068, -0.0484, -0.1044,  0.0042,  0.1336, -0.1199,\n",
      "         -0.0888,  0.1161,  0.0801, -0.0822, -0.1238,  0.0364, -0.0959, -0.0645,\n",
      "         -0.0487,  0.0234,  0.1357, -0.0301, -0.0649, -0.1182, -0.1214, -0.0482,\n",
      "         -0.0551, -0.1346]], device='cuda:0')\n",
      "Data Shape: torch.Size([2, 50])\n",
      "-----\n",
      "Grad:\n",
      "tensor([[-0.0370,  0.0472,  0.0317,  0.0854,  0.0372, -0.0123,  0.0837,  0.0423,\n",
      "          0.0096, -0.0847, -0.0076,  0.0103,  0.0166,  0.0547, -0.0242, -0.0440,\n",
      "          0.0809,  0.0458,  0.0266,  0.1649, -0.1088,  0.0252, -0.1716, -0.0897,\n",
      "         -0.0762,  0.1398, -0.0022, -0.0430, -0.0512, -0.0474, -0.0234,  0.0646,\n",
      "          0.0932,  0.0681, -0.0142,  0.0127, -0.1754,  0.0568,  0.0005, -0.1462,\n",
      "         -0.0039, -0.0181, -0.0291,  0.0261, -0.0426, -0.1465, -0.0761, -0.0431,\n",
      "          0.0233, -0.0020],\n",
      "        [ 0.0370, -0.0472, -0.0317, -0.0854, -0.0372,  0.0123, -0.0837, -0.0423,\n",
      "         -0.0096,  0.0847,  0.0076, -0.0103, -0.0166, -0.0547,  0.0242,  0.0440,\n",
      "         -0.0809, -0.0458, -0.0266, -0.1649,  0.1088, -0.0252,  0.1716,  0.0897,\n",
      "          0.0762, -0.1398,  0.0022,  0.0430,  0.0512,  0.0474,  0.0234, -0.0646,\n",
      "         -0.0932, -0.0681,  0.0142, -0.0127,  0.1754, -0.0568, -0.0005,  0.1462,\n",
      "          0.0039,  0.0181,  0.0291, -0.0261,  0.0426,  0.1465,  0.0761,  0.0431,\n",
      "         -0.0233,  0.0020]], device='cuda:0')\n",
      "Grad Shape: torch.Size([2, 50])\n",
      "---------\n",
      "linear.bias\n",
      "Data:\n",
      "tensor([0.0392, 0.0442], device='cuda:0')\n",
      "Data Shape: torch.Size([2])\n",
      "-----\n",
      "Grad:\n",
      "tensor([ 0.4666, -0.4666], device='cuda:0')\n",
      "Grad Shape: torch.Size([2])\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Note that each parameter has a grad value\n",
    "\n",
    "print(\"---------\")\n",
    "for name, param in Lstm.named_parameters():\n",
    "  if param.requires_grad: \n",
    "    print(name)\n",
    "    print(\"Data:\")\n",
    "    print(param.data)\n",
    "    print(f\"Data Shape: {param.data.shape}\")\n",
    "    print(\"-----\")\n",
    "    print(\"Grad:\")\n",
    "    print(param.grad)\n",
    "    print(f\"Grad Shape: {param.grad.shape}\")\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "lstm.weight_ih_l0\n",
      "Data:\n",
      "tensor([[ 9.6144e-02,  3.1924e-02, -1.4674e-01,  ...,  3.9571e-02,\n",
      "          3.7325e-02,  1.2754e-01],\n",
      "        [-8.1926e-02, -9.1543e-02,  1.1522e-02,  ...,  7.1677e-02,\n",
      "         -1.0867e-01, -3.4445e-02],\n",
      "        [ 1.0513e-01,  6.1566e-02, -2.2773e-02,  ..., -4.0040e-02,\n",
      "         -9.6986e-02,  1.1082e-01],\n",
      "        ...,\n",
      "        [-8.3885e-02,  1.2098e-01,  8.6485e-02,  ...,  1.1091e-01,\n",
      "         -2.3675e-05,  7.9966e-02],\n",
      "        [ 8.4030e-02,  1.3378e-01,  5.7184e-02,  ..., -1.1233e-01,\n",
      "         -8.7619e-02,  4.7074e-02],\n",
      "        [ 5.3339e-02, -1.5082e-01,  1.0702e-01,  ..., -7.1652e-02,\n",
      "          7.5266e-03,  1.0026e-01]], device='cuda:0')\n",
      "Data Shape: torch.Size([200, 200])\n",
      "-----\n",
      "Grad:\n",
      "tensor([[-1.0380e-03,  4.2402e-03,  1.0249e-03,  ...,  9.9626e-03,\n",
      "         -7.3713e-04, -4.7700e-04],\n",
      "        [-1.3368e-03,  3.9806e-03, -4.5642e-03,  ...,  6.7374e-04,\n",
      "          4.1394e-03,  1.7101e-03],\n",
      "        [ 1.4198e-05, -8.4338e-05,  1.3620e-04,  ...,  8.9446e-05,\n",
      "         -1.2793e-04, -6.5994e-05],\n",
      "        ...,\n",
      "        [-1.3210e-04,  6.8898e-05,  2.1496e-05,  ...,  3.9388e-04,\n",
      "         -3.8631e-05, -1.2061e-04],\n",
      "        [ 3.8427e-05, -7.0212e-04,  7.5171e-04,  ...,  2.0043e-04,\n",
      "         -7.2524e-04, -4.5628e-04],\n",
      "        [-2.5168e-05,  3.3328e-04, -1.9289e-05,  ...,  5.4954e-04,\n",
      "          5.4513e-05,  8.4352e-05]], device='cuda:0')\n",
      "Grad Shape: torch.Size([200, 200])\n",
      "---------\n",
      "lstm.weight_hh_l0\n",
      "Data:\n",
      "tensor([[ 2.2553e-02,  4.2683e-02, -1.0702e-01,  ..., -6.5505e-02,\n",
      "         -1.3047e-01, -5.5627e-02],\n",
      "        [-3.8799e-02,  1.0480e-01,  9.3752e-02,  ...,  6.3976e-02,\n",
      "          4.1062e-03,  4.4390e-02],\n",
      "        [ 4.6242e-02, -7.5558e-03,  2.9297e-02,  ..., -3.1606e-02,\n",
      "         -1.0879e-01,  1.2779e-01],\n",
      "        ...,\n",
      "        [ 1.1310e-01, -2.1795e-02, -6.0686e-02,  ..., -5.9940e-02,\n",
      "          7.9476e-02,  1.6362e-03],\n",
      "        [ 9.0066e-02,  1.2092e-01, -5.9364e-02,  ...,  3.9267e-02,\n",
      "         -1.1037e-01, -3.3127e-02],\n",
      "        [ 1.8704e-02, -6.3236e-02,  1.2598e-01,  ...,  1.1927e-04,\n",
      "          5.4286e-02, -7.5575e-02]], device='cuda:0')\n",
      "Data Shape: torch.Size([200, 50])\n",
      "-----\n",
      "Grad:\n",
      "tensor([[-7.7188e-04,  1.4524e-04, -2.4418e-04,  ..., -7.6579e-05,\n",
      "         -6.9937e-04,  1.5936e-04],\n",
      "        [ 6.0789e-04,  5.0090e-04, -1.8055e-04,  ...,  1.8080e-03,\n",
      "         -1.8552e-04,  3.0596e-04],\n",
      "        [-2.7918e-05, -1.0984e-05,  6.7570e-07,  ..., -4.7479e-05,\n",
      "          1.9100e-06, -3.7915e-06],\n",
      "        ...,\n",
      "        [-5.5004e-05, -2.0386e-05, -2.9297e-07,  ...,  1.9466e-05,\n",
      "          2.7494e-05,  3.2866e-05],\n",
      "        [-1.4864e-04, -8.4944e-05,  1.5793e-05,  ..., -2.5988e-04,\n",
      "          6.7558e-05, -9.3009e-06],\n",
      "        [-1.2966e-05,  4.4297e-05, -2.1628e-05,  ...,  3.9382e-05,\n",
      "         -5.3162e-05,  6.3772e-06]], device='cuda:0')\n",
      "Grad Shape: torch.Size([200, 50])\n",
      "---------\n",
      "lstm.bias_ih_l0\n",
      "Data:\n",
      "tensor([ 0.0287,  0.1471, -0.0288, -0.0216,  0.0524,  0.1065,  0.0303, -0.0270,\n",
      "        -0.1131, -0.0323, -0.1397, -0.0702,  0.0915,  0.0511,  0.0474, -0.0610,\n",
      "         0.0685, -0.1189,  0.0293,  0.1063, -0.0118,  0.1075,  0.0913,  0.1122,\n",
      "        -0.1249, -0.0337,  0.1394, -0.0943,  0.0770, -0.0041,  0.0353,  0.0941,\n",
      "         0.0673, -0.0524,  0.1239, -0.0470,  0.1235, -0.1021, -0.0046, -0.0207,\n",
      "         0.0057, -0.0262,  0.0144, -0.0828, -0.0606, -0.0397, -0.0869,  0.0474,\n",
      "        -0.0204, -0.1145,  0.0463,  0.0842, -0.1231,  0.0541, -0.0736,  0.0440,\n",
      "         0.1432, -0.0558,  0.0768,  0.0991, -0.0219, -0.0351, -0.0258, -0.0601,\n",
      "         0.0486,  0.1370,  0.0972, -0.0390,  0.0800, -0.0583, -0.1159, -0.0192,\n",
      "         0.0344,  0.0143,  0.0467, -0.1199, -0.0811,  0.0393, -0.0081,  0.0858,\n",
      "         0.1051,  0.0822, -0.0787, -0.1120,  0.1202, -0.0291,  0.1009, -0.0143,\n",
      "        -0.0271, -0.0123, -0.1200, -0.1042, -0.1454, -0.0925, -0.1081,  0.0325,\n",
      "         0.0494,  0.0141,  0.0968, -0.1190, -0.0461, -0.0887,  0.0427,  0.0704,\n",
      "        -0.0663, -0.0548,  0.0407, -0.0862, -0.0880,  0.0675,  0.0157, -0.0148,\n",
      "         0.0645, -0.0118,  0.0786,  0.0190, -0.0091,  0.0044, -0.0648,  0.0319,\n",
      "        -0.1390,  0.0087,  0.0172, -0.1087,  0.0560,  0.1455, -0.0445, -0.1283,\n",
      "        -0.0948,  0.0574,  0.0958,  0.0965, -0.0607, -0.0160, -0.0207, -0.1060,\n",
      "        -0.1440, -0.1358,  0.0221, -0.0569,  0.0707,  0.1257,  0.0331, -0.0699,\n",
      "        -0.0371, -0.0612,  0.0073, -0.0803,  0.1098,  0.0279, -0.1333,  0.1362,\n",
      "        -0.0782, -0.0466,  0.1276, -0.0808, -0.0126, -0.0771, -0.0948,  0.0740,\n",
      "         0.1224, -0.0791,  0.0696,  0.0991,  0.0925,  0.0606, -0.0494,  0.0741,\n",
      "         0.0559, -0.0306, -0.0177,  0.0604, -0.1042,  0.1188,  0.0173,  0.0689,\n",
      "        -0.0309, -0.1096,  0.0983, -0.1263,  0.0387,  0.0428,  0.0079, -0.1117,\n",
      "        -0.0213,  0.1074,  0.1419, -0.0969, -0.1273, -0.0427,  0.1213,  0.0191,\n",
      "         0.0800,  0.1038,  0.0631,  0.1248, -0.1128,  0.0084, -0.0591, -0.1348],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200])\n",
      "-----\n",
      "Grad:\n",
      "tensor([ 7.1035e-03, -5.3793e-03,  1.9460e-04,  2.6430e-03, -6.4563e-04,\n",
      "        -1.0372e-03, -2.8241e-03, -6.1618e-04,  9.0679e-04, -2.5431e-03,\n",
      "         2.9554e-04, -3.1750e-04,  4.6441e-04,  6.0487e-03, -2.2954e-03,\n",
      "         7.3221e-05, -6.2189e-03, -2.4223e-03, -1.3726e-03, -2.7884e-03,\n",
      "        -3.1891e-03,  4.6410e-04, -6.3917e-03,  6.3034e-03, -1.4766e-03,\n",
      "        -3.1210e-03, -2.3016e-04,  3.4949e-04,  2.9623e-05, -1.9741e-04,\n",
      "         2.9767e-03,  1.6285e-03,  2.8826e-03, -8.0699e-04,  5.9785e-04,\n",
      "        -1.8049e-04, -1.8258e-03,  3.4121e-03, -2.8504e-05,  6.2162e-04,\n",
      "        -6.1432e-04,  1.7644e-04,  4.2530e-03, -2.5707e-04, -1.8258e-03,\n",
      "        -5.5428e-03, -4.5107e-03,  5.7056e-05,  3.5388e-04, -5.5813e-05,\n",
      "         8.6084e-03,  6.8618e-04, -1.7069e-06,  2.1086e-03, -1.4081e-04,\n",
      "        -2.6804e-03, -4.2064e-03, -5.2198e-04,  3.0682e-03, -1.7730e-03,\n",
      "         8.7104e-04, -4.0226e-03, -3.8615e-03,  3.4125e-03,  1.9516e-03,\n",
      "        -1.9160e-04, -1.8739e-03, -1.3454e-04,  1.1315e-03, -5.1597e-03,\n",
      "        -2.8085e-03, -1.4393e-04, -3.8538e-03,  1.3190e-02, -4.5070e-04,\n",
      "        -7.4094e-04,  7.2363e-04,  5.1822e-05,  4.3872e-04,  3.2349e-04,\n",
      "        -2.0433e-03,  1.5869e-03,  6.4123e-03, -1.4474e-04,  5.0079e-03,\n",
      "        -2.4224e-04, -1.1138e-04,  1.8857e-03,  2.1100e-04,  7.7618e-04,\n",
      "        -3.0900e-03, -7.7233e-03,  8.0648e-03, -7.4954e-04,  9.5439e-05,\n",
      "        -1.4855e-03, -2.0431e-03, -1.0379e-04, -1.4338e-03, -1.4452e-03,\n",
      "        -6.4544e-02, -4.8632e-02,  9.6993e-04,  9.4667e-03, -2.6548e-02,\n",
      "         3.9856e-02, -1.4510e-02, -4.4787e-03,  2.9227e-02,  4.4885e-03,\n",
      "        -1.2822e-02, -3.0446e-02, -1.4225e-02,  2.6738e-02,  3.9543e-02,\n",
      "        -3.7006e-03, -2.9164e-02, -2.0096e-02, -4.7475e-03, -1.3491e-02,\n",
      "         8.7264e-03,  1.2972e-02,  1.6337e-02, -2.9246e-02,  7.8535e-03,\n",
      "        -1.3728e-02,  3.0117e-02, -5.3120e-03, -1.3817e-03, -8.8174e-04,\n",
      "        -1.3053e-02,  1.8073e-02,  1.7737e-02, -1.8225e-03, -4.0056e-02,\n",
      "        -3.1047e-03,  3.2545e-03,  1.2244e-02,  9.7896e-03, -3.6653e-03,\n",
      "         2.2873e-02, -3.9614e-02, -8.1695e-02,  1.3264e-02,  7.4885e-03,\n",
      "         1.9359e-02,  1.6953e-02,  7.2353e-05,  2.5699e-02,  2.4687e-02,\n",
      "         4.9351e-03, -2.7004e-03,  1.6740e-04,  3.8830e-03, -7.2689e-04,\n",
      "        -1.6033e-03, -5.0127e-03, -8.4379e-04,  1.6060e-03, -2.2338e-03,\n",
      "         1.4090e-04, -1.7143e-03,  2.0276e-04,  5.8184e-03, -2.4847e-03,\n",
      "         4.4732e-04, -9.6007e-03, -1.7285e-03, -1.8605e-03, -5.5675e-03,\n",
      "        -2.3374e-03,  5.4337e-04, -5.8304e-03,  8.0169e-03, -1.7643e-03,\n",
      "        -3.0076e-03, -2.8745e-04,  3.8139e-04,  2.5344e-05, -4.1362e-04,\n",
      "         2.6254e-03,  1.9261e-03,  5.4270e-03, -1.1875e-03,  1.2474e-03,\n",
      "        -1.7802e-04, -3.9149e-03,  5.2183e-03, -6.1268e-05,  6.8893e-04,\n",
      "        -8.3409e-04,  8.6783e-04,  5.8296e-03, -1.1908e-07, -1.1290e-03,\n",
      "        -6.3027e-03, -2.8565e-03,  1.0822e-04,  7.5131e-04,  1.6673e-04],\n",
      "       device='cuda:0')\n",
      "Grad Shape: torch.Size([200])\n",
      "---------\n",
      "lstm.bias_hh_l0\n",
      "Data:\n",
      "tensor([-1.2226e-02, -4.4776e-02, -9.1999e-02,  3.7187e-02,  9.8503e-02,\n",
      "        -1.2391e-01, -1.0633e-01, -5.8942e-02, -4.7124e-02,  1.4805e-01,\n",
      "         9.4704e-02, -1.0898e-01,  3.4363e-02, -2.8587e-02,  2.8940e-02,\n",
      "        -4.8094e-02, -8.2364e-02, -1.2046e-01, -1.2346e-01,  4.8594e-02,\n",
      "        -3.5018e-02,  7.9216e-02,  1.1183e-01,  3.3365e-02,  7.4195e-02,\n",
      "         5.5725e-02,  1.0263e-01,  8.0133e-02, -9.9195e-02, -3.0258e-02,\n",
      "         1.3783e-02,  1.2104e-01,  2.0199e-02, -1.1808e-01,  3.4746e-02,\n",
      "         8.5482e-02, -6.3412e-02,  5.3024e-02,  1.0314e-01, -6.4032e-02,\n",
      "        -5.3004e-02,  1.0049e-01,  5.7235e-02, -1.8891e-02, -8.1560e-03,\n",
      "        -6.2542e-02,  1.3424e-01, -1.1971e-01,  2.7788e-02, -1.1610e-01,\n",
      "         1.2147e-01, -5.1899e-03, -4.8472e-02, -4.6363e-02, -1.0810e-01,\n",
      "         6.7864e-02,  7.3804e-02,  7.7626e-02, -7.8975e-02, -1.2272e-01,\n",
      "         2.9890e-02,  4.0232e-02, -8.1317e-02,  1.5496e-02, -1.1851e-01,\n",
      "         1.9283e-02, -1.2174e-01,  2.2824e-02, -3.6319e-02, -9.1119e-02,\n",
      "         8.6605e-02,  1.0117e-01, -1.2480e-01,  6.0412e-02,  1.0845e-01,\n",
      "         4.6817e-02, -9.0523e-02, -8.0867e-02, -1.0229e-01,  5.8374e-02,\n",
      "        -3.2049e-02,  1.3050e-01, -9.7738e-02,  1.0808e-01, -1.2004e-01,\n",
      "        -3.5221e-02, -1.3116e-02,  1.2991e-01,  4.3915e-03,  3.3953e-02,\n",
      "         9.0378e-02,  2.0663e-02, -8.4414e-02, -8.5706e-04, -5.5415e-02,\n",
      "         3.1640e-02, -9.9586e-02, -1.6972e-02,  6.4834e-02,  9.0578e-02,\n",
      "        -1.1181e-01,  5.2526e-02, -5.4401e-02, -1.3325e-01,  4.4925e-02,\n",
      "        -1.1688e-01,  9.9519e-02,  2.4442e-02,  1.9494e-02, -7.0657e-02,\n",
      "         1.4536e-01,  1.5278e-02, -9.0543e-03,  5.4382e-02,  9.1276e-02,\n",
      "         1.1680e-01, -1.1363e-01, -8.6555e-03,  3.0021e-02,  1.4251e-01,\n",
      "         7.1557e-02,  2.8202e-02, -5.3692e-02, -1.2269e-01,  3.9322e-02,\n",
      "         1.4249e-01,  8.6920e-02,  1.4710e-01, -5.8349e-02, -1.2937e-01,\n",
      "         1.7842e-02,  3.1470e-02, -4.0205e-02,  8.4500e-02,  1.7098e-02,\n",
      "        -3.6502e-02,  8.3835e-02,  6.6554e-02, -2.9011e-02, -1.2189e-01,\n",
      "         8.0135e-03,  9.8922e-02, -1.3029e-01,  1.1272e-01,  9.0637e-02,\n",
      "        -1.1411e-01,  1.1576e-02,  8.8137e-02,  6.7311e-02, -1.0823e-01,\n",
      "        -4.4221e-02,  7.7970e-02,  4.1124e-02,  1.0980e-01,  8.6670e-02,\n",
      "        -4.7528e-02,  1.4398e-02, -4.3206e-02, -6.2110e-02, -6.3124e-02,\n",
      "        -2.2082e-02, -3.0167e-05, -9.9234e-02, -2.4107e-02, -1.2177e-01,\n",
      "         4.2678e-02, -7.9356e-02, -1.3024e-01, -1.1367e-01,  1.2055e-01,\n",
      "        -8.4324e-02, -1.5104e-01, -3.3802e-02,  6.1402e-02, -4.5063e-02,\n",
      "         1.9548e-02,  6.6639e-02,  1.0051e-01, -2.9565e-02, -9.2353e-02,\n",
      "        -1.4476e-01,  1.1602e-01, -7.7375e-02,  9.4414e-02, -1.2420e-01,\n",
      "        -4.1961e-02,  4.6058e-02, -8.6039e-02,  4.1533e-02, -1.4401e-01,\n",
      "        -1.3116e-01,  2.4151e-02, -1.2062e-01, -4.9458e-03,  1.1252e-01,\n",
      "        -9.4490e-03,  1.4027e-01,  8.4309e-02, -9.1374e-02, -1.2004e-01],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200])\n",
      "-----\n",
      "Grad:\n",
      "tensor([ 7.1035e-03, -5.3793e-03,  1.9460e-04,  2.6430e-03, -6.4563e-04,\n",
      "        -1.0372e-03, -2.8241e-03, -6.1618e-04,  9.0679e-04, -2.5431e-03,\n",
      "         2.9554e-04, -3.1750e-04,  4.6441e-04,  6.0487e-03, -2.2954e-03,\n",
      "         7.3221e-05, -6.2189e-03, -2.4223e-03, -1.3726e-03, -2.7884e-03,\n",
      "        -3.1891e-03,  4.6410e-04, -6.3917e-03,  6.3034e-03, -1.4766e-03,\n",
      "        -3.1210e-03, -2.3016e-04,  3.4949e-04,  2.9623e-05, -1.9741e-04,\n",
      "         2.9767e-03,  1.6285e-03,  2.8826e-03, -8.0699e-04,  5.9785e-04,\n",
      "        -1.8049e-04, -1.8258e-03,  3.4121e-03, -2.8504e-05,  6.2162e-04,\n",
      "        -6.1432e-04,  1.7644e-04,  4.2530e-03, -2.5707e-04, -1.8258e-03,\n",
      "        -5.5428e-03, -4.5107e-03,  5.7056e-05,  3.5388e-04, -5.5813e-05,\n",
      "         8.6084e-03,  6.8618e-04, -1.7069e-06,  2.1086e-03, -1.4081e-04,\n",
      "        -2.6804e-03, -4.2064e-03, -5.2198e-04,  3.0682e-03, -1.7730e-03,\n",
      "         8.7104e-04, -4.0226e-03, -3.8615e-03,  3.4125e-03,  1.9516e-03,\n",
      "        -1.9160e-04, -1.8739e-03, -1.3454e-04,  1.1315e-03, -5.1597e-03,\n",
      "        -2.8085e-03, -1.4393e-04, -3.8538e-03,  1.3190e-02, -4.5070e-04,\n",
      "        -7.4094e-04,  7.2363e-04,  5.1822e-05,  4.3872e-04,  3.2349e-04,\n",
      "        -2.0433e-03,  1.5869e-03,  6.4123e-03, -1.4474e-04,  5.0079e-03,\n",
      "        -2.4224e-04, -1.1138e-04,  1.8857e-03,  2.1100e-04,  7.7618e-04,\n",
      "        -3.0900e-03, -7.7233e-03,  8.0648e-03, -7.4954e-04,  9.5439e-05,\n",
      "        -1.4855e-03, -2.0431e-03, -1.0379e-04, -1.4338e-03, -1.4452e-03,\n",
      "        -6.4544e-02, -4.8632e-02,  9.6993e-04,  9.4667e-03, -2.6548e-02,\n",
      "         3.9856e-02, -1.4510e-02, -4.4787e-03,  2.9227e-02,  4.4885e-03,\n",
      "        -1.2822e-02, -3.0446e-02, -1.4225e-02,  2.6738e-02,  3.9543e-02,\n",
      "        -3.7006e-03, -2.9164e-02, -2.0096e-02, -4.7475e-03, -1.3491e-02,\n",
      "         8.7264e-03,  1.2972e-02,  1.6337e-02, -2.9246e-02,  7.8535e-03,\n",
      "        -1.3728e-02,  3.0117e-02, -5.3120e-03, -1.3817e-03, -8.8174e-04,\n",
      "        -1.3053e-02,  1.8073e-02,  1.7737e-02, -1.8225e-03, -4.0056e-02,\n",
      "        -3.1047e-03,  3.2545e-03,  1.2244e-02,  9.7896e-03, -3.6653e-03,\n",
      "         2.2873e-02, -3.9614e-02, -8.1695e-02,  1.3264e-02,  7.4885e-03,\n",
      "         1.9359e-02,  1.6953e-02,  7.2353e-05,  2.5699e-02,  2.4687e-02,\n",
      "         4.9351e-03, -2.7004e-03,  1.6740e-04,  3.8830e-03, -7.2689e-04,\n",
      "        -1.6033e-03, -5.0127e-03, -8.4379e-04,  1.6060e-03, -2.2338e-03,\n",
      "         1.4090e-04, -1.7143e-03,  2.0276e-04,  5.8184e-03, -2.4847e-03,\n",
      "         4.4732e-04, -9.6007e-03, -1.7285e-03, -1.8605e-03, -5.5675e-03,\n",
      "        -2.3374e-03,  5.4337e-04, -5.8304e-03,  8.0169e-03, -1.7643e-03,\n",
      "        -3.0076e-03, -2.8745e-04,  3.8139e-04,  2.5344e-05, -4.1362e-04,\n",
      "         2.6254e-03,  1.9261e-03,  5.4270e-03, -1.1875e-03,  1.2474e-03,\n",
      "        -1.7802e-04, -3.9149e-03,  5.2183e-03, -6.1268e-05,  6.8893e-04,\n",
      "        -8.3409e-04,  8.6783e-04,  5.8296e-03, -1.1908e-07, -1.1290e-03,\n",
      "        -6.3027e-03, -2.8565e-03,  1.0822e-04,  7.5131e-04,  1.6673e-04],\n",
      "       device='cuda:0')\n",
      "Grad Shape: torch.Size([200])\n",
      "---------\n",
      "linear.weight\n",
      "Data:\n",
      "tensor([[-0.1067, -0.1313, -0.1274,  0.0150,  0.0693,  0.0986,  0.0237,  0.0833,\n",
      "          0.0532, -0.0056, -0.0288, -0.1388, -0.0041,  0.0554,  0.0972,  0.0192,\n",
      "         -0.0903, -0.0348, -0.0921, -0.0465,  0.0557, -0.0413,  0.0247, -0.0914,\n",
      "          0.1151,  0.0373,  0.0660, -0.0537, -0.1050,  0.0021, -0.0135, -0.0884,\n",
      "          0.0243,  0.0723,  0.0149, -0.1224, -0.0660,  0.1291, -0.0343, -0.0517,\n",
      "          0.0847, -0.1265, -0.1041, -0.0176,  0.0091, -0.0189, -0.0211, -0.0416,\n",
      "          0.0110,  0.1046],\n",
      "        [ 0.1189,  0.0638, -0.1156, -0.0283,  0.1322, -0.1456,  0.1373,  0.1338,\n",
      "         -0.0069, -0.0888,  0.0072,  0.0107,  0.1094, -0.1235, -0.1107,  0.0199,\n",
      "          0.1512,  0.0514, -0.0263,  0.1051,  0.0570, -0.0614, -0.1299,  0.0522,\n",
      "          0.0719,  0.1024, -0.1168, -0.0584, -0.1144, -0.0058,  0.1236, -0.1099,\n",
      "         -0.0788,  0.1261,  0.0701, -0.0722, -0.1338,  0.0464, -0.0859, -0.0745,\n",
      "         -0.0587,  0.0134,  0.1257, -0.0201, -0.0749, -0.1282, -0.1314, -0.0582,\n",
      "         -0.0451, -0.1446]], device='cuda:0')\n",
      "Data Shape: torch.Size([2, 50])\n",
      "-----\n",
      "Grad:\n",
      "tensor([[-0.0370,  0.0472,  0.0317,  0.0854,  0.0372, -0.0123,  0.0837,  0.0423,\n",
      "          0.0096, -0.0847, -0.0076,  0.0103,  0.0166,  0.0547, -0.0242, -0.0440,\n",
      "          0.0809,  0.0458,  0.0266,  0.1649, -0.1088,  0.0252, -0.1716, -0.0897,\n",
      "         -0.0762,  0.1398, -0.0022, -0.0430, -0.0512, -0.0474, -0.0234,  0.0646,\n",
      "          0.0932,  0.0681, -0.0142,  0.0127, -0.1754,  0.0568,  0.0005, -0.1462,\n",
      "         -0.0039, -0.0181, -0.0291,  0.0261, -0.0426, -0.1465, -0.0761, -0.0431,\n",
      "          0.0233, -0.0020],\n",
      "        [ 0.0370, -0.0472, -0.0317, -0.0854, -0.0372,  0.0123, -0.0837, -0.0423,\n",
      "         -0.0096,  0.0847,  0.0076, -0.0103, -0.0166, -0.0547,  0.0242,  0.0440,\n",
      "         -0.0809, -0.0458, -0.0266, -0.1649,  0.1088, -0.0252,  0.1716,  0.0897,\n",
      "          0.0762, -0.1398,  0.0022,  0.0430,  0.0512,  0.0474,  0.0234, -0.0646,\n",
      "         -0.0932, -0.0681,  0.0142, -0.0127,  0.1754, -0.0568, -0.0005,  0.1462,\n",
      "          0.0039,  0.0181,  0.0291, -0.0261,  0.0426,  0.1465,  0.0761,  0.0431,\n",
      "         -0.0233,  0.0020]], device='cuda:0')\n",
      "Grad Shape: torch.Size([2, 50])\n",
      "---------\n",
      "linear.bias\n",
      "Data:\n",
      "tensor([0.0292, 0.0542], device='cuda:0')\n",
      "Data Shape: torch.Size([2])\n",
      "-----\n",
      "Grad:\n",
      "tensor([ 0.4666, -0.4666], device='cuda:0')\n",
      "Grad Shape: torch.Size([2])\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Note that the parameter data has changed\n",
    "\n",
    "print(\"---------\")\n",
    "for name, param in Lstm.named_parameters():\n",
    "  if param.requires_grad: \n",
    "    print(name)\n",
    "    print(\"Data:\")\n",
    "    print(param.data)\n",
    "    print(f\"Data Shape: {param.data.shape}\")\n",
    "    print(\"-----\")\n",
    "    print(\"Grad:\")\n",
    "    print(param.grad)\n",
    "    print(f\"Grad Shape: {param.grad.shape}\")\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "lstm.weight_ih_l0\n",
      "Data:\n",
      "tensor([[ 9.6144e-02,  3.1924e-02, -1.4674e-01,  ...,  3.9571e-02,\n",
      "          3.7325e-02,  1.2754e-01],\n",
      "        [-8.1926e-02, -9.1543e-02,  1.1522e-02,  ...,  7.1677e-02,\n",
      "         -1.0867e-01, -3.4445e-02],\n",
      "        [ 1.0513e-01,  6.1566e-02, -2.2773e-02,  ..., -4.0040e-02,\n",
      "         -9.6986e-02,  1.1082e-01],\n",
      "        ...,\n",
      "        [-8.3885e-02,  1.2098e-01,  8.6485e-02,  ...,  1.1091e-01,\n",
      "         -2.3675e-05,  7.9966e-02],\n",
      "        [ 8.4030e-02,  1.3378e-01,  5.7184e-02,  ..., -1.1233e-01,\n",
      "         -8.7619e-02,  4.7074e-02],\n",
      "        [ 5.3339e-02, -1.5082e-01,  1.0702e-01,  ..., -7.1652e-02,\n",
      "          7.5266e-03,  1.0026e-01]], device='cuda:0')\n",
      "Data Shape: torch.Size([200, 200])\n",
      "-----\n",
      "Grad:\n",
      "None\n",
      "---------\n",
      "lstm.weight_hh_l0\n",
      "Data:\n",
      "tensor([[ 2.2553e-02,  4.2683e-02, -1.0702e-01,  ..., -6.5505e-02,\n",
      "         -1.3047e-01, -5.5627e-02],\n",
      "        [-3.8799e-02,  1.0480e-01,  9.3752e-02,  ...,  6.3976e-02,\n",
      "          4.1062e-03,  4.4390e-02],\n",
      "        [ 4.6242e-02, -7.5558e-03,  2.9297e-02,  ..., -3.1606e-02,\n",
      "         -1.0879e-01,  1.2779e-01],\n",
      "        ...,\n",
      "        [ 1.1310e-01, -2.1795e-02, -6.0686e-02,  ..., -5.9940e-02,\n",
      "          7.9476e-02,  1.6362e-03],\n",
      "        [ 9.0066e-02,  1.2092e-01, -5.9364e-02,  ...,  3.9267e-02,\n",
      "         -1.1037e-01, -3.3127e-02],\n",
      "        [ 1.8704e-02, -6.3236e-02,  1.2598e-01,  ...,  1.1927e-04,\n",
      "          5.4286e-02, -7.5575e-02]], device='cuda:0')\n",
      "Data Shape: torch.Size([200, 50])\n",
      "-----\n",
      "Grad:\n",
      "None\n",
      "---------\n",
      "lstm.bias_ih_l0\n",
      "Data:\n",
      "tensor([ 0.0287,  0.1471, -0.0288, -0.0216,  0.0524,  0.1065,  0.0303, -0.0270,\n",
      "        -0.1131, -0.0323, -0.1397, -0.0702,  0.0915,  0.0511,  0.0474, -0.0610,\n",
      "         0.0685, -0.1189,  0.0293,  0.1063, -0.0118,  0.1075,  0.0913,  0.1122,\n",
      "        -0.1249, -0.0337,  0.1394, -0.0943,  0.0770, -0.0041,  0.0353,  0.0941,\n",
      "         0.0673, -0.0524,  0.1239, -0.0470,  0.1235, -0.1021, -0.0046, -0.0207,\n",
      "         0.0057, -0.0262,  0.0144, -0.0828, -0.0606, -0.0397, -0.0869,  0.0474,\n",
      "        -0.0204, -0.1145,  0.0463,  0.0842, -0.1231,  0.0541, -0.0736,  0.0440,\n",
      "         0.1432, -0.0558,  0.0768,  0.0991, -0.0219, -0.0351, -0.0258, -0.0601,\n",
      "         0.0486,  0.1370,  0.0972, -0.0390,  0.0800, -0.0583, -0.1159, -0.0192,\n",
      "         0.0344,  0.0143,  0.0467, -0.1199, -0.0811,  0.0393, -0.0081,  0.0858,\n",
      "         0.1051,  0.0822, -0.0787, -0.1120,  0.1202, -0.0291,  0.1009, -0.0143,\n",
      "        -0.0271, -0.0123, -0.1200, -0.1042, -0.1454, -0.0925, -0.1081,  0.0325,\n",
      "         0.0494,  0.0141,  0.0968, -0.1190, -0.0461, -0.0887,  0.0427,  0.0704,\n",
      "        -0.0663, -0.0548,  0.0407, -0.0862, -0.0880,  0.0675,  0.0157, -0.0148,\n",
      "         0.0645, -0.0118,  0.0786,  0.0190, -0.0091,  0.0044, -0.0648,  0.0319,\n",
      "        -0.1390,  0.0087,  0.0172, -0.1087,  0.0560,  0.1455, -0.0445, -0.1283,\n",
      "        -0.0948,  0.0574,  0.0958,  0.0965, -0.0607, -0.0160, -0.0207, -0.1060,\n",
      "        -0.1440, -0.1358,  0.0221, -0.0569,  0.0707,  0.1257,  0.0331, -0.0699,\n",
      "        -0.0371, -0.0612,  0.0073, -0.0803,  0.1098,  0.0279, -0.1333,  0.1362,\n",
      "        -0.0782, -0.0466,  0.1276, -0.0808, -0.0126, -0.0771, -0.0948,  0.0740,\n",
      "         0.1224, -0.0791,  0.0696,  0.0991,  0.0925,  0.0606, -0.0494,  0.0741,\n",
      "         0.0559, -0.0306, -0.0177,  0.0604, -0.1042,  0.1188,  0.0173,  0.0689,\n",
      "        -0.0309, -0.1096,  0.0983, -0.1263,  0.0387,  0.0428,  0.0079, -0.1117,\n",
      "        -0.0213,  0.1074,  0.1419, -0.0969, -0.1273, -0.0427,  0.1213,  0.0191,\n",
      "         0.0800,  0.1038,  0.0631,  0.1248, -0.1128,  0.0084, -0.0591, -0.1348],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200])\n",
      "-----\n",
      "Grad:\n",
      "None\n",
      "---------\n",
      "lstm.bias_hh_l0\n",
      "Data:\n",
      "tensor([-1.2226e-02, -4.4776e-02, -9.1999e-02,  3.7187e-02,  9.8503e-02,\n",
      "        -1.2391e-01, -1.0633e-01, -5.8942e-02, -4.7124e-02,  1.4805e-01,\n",
      "         9.4704e-02, -1.0898e-01,  3.4363e-02, -2.8587e-02,  2.8940e-02,\n",
      "        -4.8094e-02, -8.2364e-02, -1.2046e-01, -1.2346e-01,  4.8594e-02,\n",
      "        -3.5018e-02,  7.9216e-02,  1.1183e-01,  3.3365e-02,  7.4195e-02,\n",
      "         5.5725e-02,  1.0263e-01,  8.0133e-02, -9.9195e-02, -3.0258e-02,\n",
      "         1.3783e-02,  1.2104e-01,  2.0199e-02, -1.1808e-01,  3.4746e-02,\n",
      "         8.5482e-02, -6.3412e-02,  5.3024e-02,  1.0314e-01, -6.4032e-02,\n",
      "        -5.3004e-02,  1.0049e-01,  5.7235e-02, -1.8891e-02, -8.1560e-03,\n",
      "        -6.2542e-02,  1.3424e-01, -1.1971e-01,  2.7788e-02, -1.1610e-01,\n",
      "         1.2147e-01, -5.1899e-03, -4.8472e-02, -4.6363e-02, -1.0810e-01,\n",
      "         6.7864e-02,  7.3804e-02,  7.7626e-02, -7.8975e-02, -1.2272e-01,\n",
      "         2.9890e-02,  4.0232e-02, -8.1317e-02,  1.5496e-02, -1.1851e-01,\n",
      "         1.9283e-02, -1.2174e-01,  2.2824e-02, -3.6319e-02, -9.1119e-02,\n",
      "         8.6605e-02,  1.0117e-01, -1.2480e-01,  6.0412e-02,  1.0845e-01,\n",
      "         4.6817e-02, -9.0523e-02, -8.0867e-02, -1.0229e-01,  5.8374e-02,\n",
      "        -3.2049e-02,  1.3050e-01, -9.7738e-02,  1.0808e-01, -1.2004e-01,\n",
      "        -3.5221e-02, -1.3116e-02,  1.2991e-01,  4.3915e-03,  3.3953e-02,\n",
      "         9.0378e-02,  2.0663e-02, -8.4414e-02, -8.5706e-04, -5.5415e-02,\n",
      "         3.1640e-02, -9.9586e-02, -1.6972e-02,  6.4834e-02,  9.0578e-02,\n",
      "        -1.1181e-01,  5.2526e-02, -5.4401e-02, -1.3325e-01,  4.4925e-02,\n",
      "        -1.1688e-01,  9.9519e-02,  2.4442e-02,  1.9494e-02, -7.0657e-02,\n",
      "         1.4536e-01,  1.5278e-02, -9.0543e-03,  5.4382e-02,  9.1276e-02,\n",
      "         1.1680e-01, -1.1363e-01, -8.6555e-03,  3.0021e-02,  1.4251e-01,\n",
      "         7.1557e-02,  2.8202e-02, -5.3692e-02, -1.2269e-01,  3.9322e-02,\n",
      "         1.4249e-01,  8.6920e-02,  1.4710e-01, -5.8349e-02, -1.2937e-01,\n",
      "         1.7842e-02,  3.1470e-02, -4.0205e-02,  8.4500e-02,  1.7098e-02,\n",
      "        -3.6502e-02,  8.3835e-02,  6.6554e-02, -2.9011e-02, -1.2189e-01,\n",
      "         8.0135e-03,  9.8922e-02, -1.3029e-01,  1.1272e-01,  9.0637e-02,\n",
      "        -1.1411e-01,  1.1576e-02,  8.8137e-02,  6.7311e-02, -1.0823e-01,\n",
      "        -4.4221e-02,  7.7970e-02,  4.1124e-02,  1.0980e-01,  8.6670e-02,\n",
      "        -4.7528e-02,  1.4398e-02, -4.3206e-02, -6.2110e-02, -6.3124e-02,\n",
      "        -2.2082e-02, -3.0167e-05, -9.9234e-02, -2.4107e-02, -1.2177e-01,\n",
      "         4.2678e-02, -7.9356e-02, -1.3024e-01, -1.1367e-01,  1.2055e-01,\n",
      "        -8.4324e-02, -1.5104e-01, -3.3802e-02,  6.1402e-02, -4.5063e-02,\n",
      "         1.9548e-02,  6.6639e-02,  1.0051e-01, -2.9565e-02, -9.2353e-02,\n",
      "        -1.4476e-01,  1.1602e-01, -7.7375e-02,  9.4414e-02, -1.2420e-01,\n",
      "        -4.1961e-02,  4.6058e-02, -8.6039e-02,  4.1533e-02, -1.4401e-01,\n",
      "        -1.3116e-01,  2.4151e-02, -1.2062e-01, -4.9458e-03,  1.1252e-01,\n",
      "        -9.4490e-03,  1.4027e-01,  8.4309e-02, -9.1374e-02, -1.2004e-01],\n",
      "       device='cuda:0')\n",
      "Data Shape: torch.Size([200])\n",
      "-----\n",
      "Grad:\n",
      "None\n",
      "---------\n",
      "linear.weight\n",
      "Data:\n",
      "tensor([[-0.1067, -0.1313, -0.1274,  0.0150,  0.0693,  0.0986,  0.0237,  0.0833,\n",
      "          0.0532, -0.0056, -0.0288, -0.1388, -0.0041,  0.0554,  0.0972,  0.0192,\n",
      "         -0.0903, -0.0348, -0.0921, -0.0465,  0.0557, -0.0413,  0.0247, -0.0914,\n",
      "          0.1151,  0.0373,  0.0660, -0.0537, -0.1050,  0.0021, -0.0135, -0.0884,\n",
      "          0.0243,  0.0723,  0.0149, -0.1224, -0.0660,  0.1291, -0.0343, -0.0517,\n",
      "          0.0847, -0.1265, -0.1041, -0.0176,  0.0091, -0.0189, -0.0211, -0.0416,\n",
      "          0.0110,  0.1046],\n",
      "        [ 0.1189,  0.0638, -0.1156, -0.0283,  0.1322, -0.1456,  0.1373,  0.1338,\n",
      "         -0.0069, -0.0888,  0.0072,  0.0107,  0.1094, -0.1235, -0.1107,  0.0199,\n",
      "          0.1512,  0.0514, -0.0263,  0.1051,  0.0570, -0.0614, -0.1299,  0.0522,\n",
      "          0.0719,  0.1024, -0.1168, -0.0584, -0.1144, -0.0058,  0.1236, -0.1099,\n",
      "         -0.0788,  0.1261,  0.0701, -0.0722, -0.1338,  0.0464, -0.0859, -0.0745,\n",
      "         -0.0587,  0.0134,  0.1257, -0.0201, -0.0749, -0.1282, -0.1314, -0.0582,\n",
      "         -0.0451, -0.1446]], device='cuda:0')\n",
      "Data Shape: torch.Size([2, 50])\n",
      "-----\n",
      "Grad:\n",
      "None\n",
      "---------\n",
      "linear.bias\n",
      "Data:\n",
      "tensor([0.0292, 0.0542], device='cuda:0')\n",
      "Data Shape: torch.Size([2])\n",
      "-----\n",
      "Grad:\n",
      "None\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Note that the gradients have been cleared\n",
    "\n",
    "print(\"---------\")\n",
    "for name, param in Lstm.named_parameters():\n",
    "  if param.requires_grad: \n",
    "    print(name)\n",
    "    print(\"Data:\")\n",
    "    print(param.data)\n",
    "    print(f\"Data Shape: {param.data.shape}\")\n",
    "    print(\"-----\")\n",
    "    print(\"Grad:\")\n",
    "    print(param.grad)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0\n",
      "lstm.weight_hh_l0\n",
      "lstm.bias_ih_l0\n",
      "lstm.bias_hh_l0\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in Lstm.named_parameters():\n",
    "  print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
