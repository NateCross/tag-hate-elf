{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Visualization Notebook\n",
    "\n",
    "This notebook processes a CSV file containing text data, cleans the text by removing markdown formatting and URLs, filters out non-Tagalog or Taglish text based on a specified confidence threshold, and visualizes the results. The goal is to identify and analyze Filipino phrases in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from lingua import Language, LanguageDetectorBuilder  # Placeholder for actual import if using a different library\n",
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "from tqdm.auto import tqdm # Import tqdm for fancy progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the path to the CSV file and the threshold for identifying Tagalog content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "CSV_FILENAME = 'data-Philippines-20240214-225413-hot.csv'  # Path to the CSV file\n",
    "TAGALOG_THRESHOLD = 0.95  # Threshold for Tagalog content detection\n",
    "\n",
    "# Initialize the language detector with the specified languages\n",
    "languages = [Language.ENGLISH, Language.TAGALOG]\n",
    "\n",
    "# Ensure the Tagalog threshold is within the valid range\n",
    "if not (0.00 <= TAGALOG_THRESHOLD <= 1.00):\n",
    "    print(\"ERROR: Tagalog threshold must be between 0.0 and 1.0\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions\n",
    "\n",
    "Define functions to clean the text data by removing markdown formatting, URLs, and Reddit usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert markdown formatted text to plain text\n",
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "# Patching the Markdown library to add a plain text output format\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "# Wrapper function to convert markdown text to plain text\n",
    "def unmark(text):\n",
    "    return __md.convert(text)\n",
    "\n",
    "# Function to remove URLs from a text string and replace them with '[LINK]'\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http\\S+', re.IGNORECASE)\n",
    "    return url_pattern.sub('[LINK]', text)\n",
    "\n",
    "# Function to remove Reddit usernames from a text string and replace them with '[USERNAME]'\n",
    "def remove_usernames(text):\n",
    "    username_pattern = re.compile(r\"/?u/[A-Za-z0-9_-]+\", re.IGNORECASE)\n",
    "    return username_pattern.sub('[USERNAME]', text)\n",
    "\n",
    "# Function to remove emojis using the emoji python library\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, '')\n",
    "\n",
    "# Replace escape sequences with a space\n",
    "def remove_escape_sequences(text):\n",
    "    escape_pattern = re.compile(r'[\\r\\n\\t]', re.IGNORECASE)\n",
    "    return escape_pattern.sub(' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Data Processing & Saving Processed Data\n",
    "\n",
    "Read the CSV file, process the text data, filter based on the Tagalog content threshold, and count the number of Filipino phrases identified. Save the filtered and processed data to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "try:\n",
    "    csv_data = pd.read_csv(CSV_FILENAME, lineterminator='\\n', header=0, names=[\n",
    "        'submission_name',\n",
    "        'submission_text',\n",
    "        'body',\n",
    "    ])\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: File not found\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Before Filtering\n",
    "Start by capturing the count of rows in your DataFrame before applying any filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows before filtering\n",
    "count_before_filtering = len(csv_data)\n",
    "\n",
    "print(f\"Count before filtering: {count_before_filtering}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a blank 'label' column for annotation\n",
    "csv_data['label'] = ''\n",
    "\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filipino_phrases = 0  # Counter for Filipino phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(csv_data.itertuples(), total=count_before_filtering, desc=\"LINES\"):  # Iterate over each row in the CSV\n",
    "    text = str(row.body)  # Convert to string to handle non-string data\n",
    "\n",
    "    # Clean the text by removing markdown, URLs, usernames, emojis, \n",
    "    # and trailing whitespace\n",
    "    text = unmark(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_usernames(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_escape_sequences(text)\n",
    "    text = text.rstrip()\n",
    "\n",
    "    # Compute the confidence of the text being Tagalog\n",
    "    result = detector.compute_language_confidence(text, Language.TAGALOG)\n",
    "    \n",
    "    # If the confidence is above the threshold, consider it a Filipino phrase\n",
    "    if result >= TAGALOG_THRESHOLD:\n",
    "        filipino_phrases += 1\n",
    "        csv_data.at[row.Index, 'body'] = text  # Update the text in the CSV\n",
    "    else:\n",
    "        csv_data.drop(row.Index, inplace=True)  # Drop rows that don't meet the threshold\n",
    "\n",
    "print(\"Finished cleaning and filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total: {filipino_phrases} / {count_before_filtering}\")  # Print the total number of Filipino phrases found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered CSV\n",
    "filtered_filename = CSV_FILENAME.replace('.csv', '-filtered.csv')\n",
    "csv_data.to_csv(filtered_filename, index=False)  # Prevent index from being saved as a column\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Filtering Effect\n",
    "\n",
    "To understand the impact of our filtering based on the Tagalog content threshold, we will visualize the number of comments in the dataset before and after the filtering process. This will help us gauge the extent of data reduction and ensure that the filtering aligns with our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Before Filtering', 'After Filtering'], [count_before_filtering, filipino_phrases], color=['blue', 'green'])\n",
    "plt.title('Effect of Filtering on Dataset Size')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
