{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from Scratch\n",
    "\n",
    "Based on [this tutorial](http://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44664/242538996.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasetall.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/78F8812CF880EA28/Github/Hate-Speech-Detection/notebook/bayes.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/78F8812CF880EA28/Github/Hate-Speech-Detection/notebook/bayes.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/78F8812CF880EA28/Github/Hate-Speech-Detection/notebook/bayes.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m csv \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../datasetall.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, usecols\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/78F8812CF880EA28/Github/Hate-Speech-Detection/notebook/bayes.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m csv\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1879\u001b[0m     f,\n\u001b[1;32m   1880\u001b[0m     mode,\n\u001b[1;32m   1881\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1882\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1883\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1885\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1886\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1887\u001b[0m )\n\u001b[1;32m   1888\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasetall.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('../datasetall.csv', usecols=(0, 1))\n",
    "\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  csv['text'],\n",
    "  csv['label'],\n",
    "  test_size=0.2,\n",
    "  random_state=0,\n",
    "  stratify=csv['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686      Happy Friday PILI pinas espsa mga kaUniteam!Tw...\n",
       "23048    *binay's ad plays on tv nanay: *whispers pwet ...\n",
       "28045    samin may best in english grade 5. tapos pag b...\n",
       "12627    Di ko alam pero everytime makita ko muka ni ma...\n",
       "22606    Sarap patayin ng tv pag pinapalabas yung \"Baki...\n",
       "                               ...                        \n",
       "12067    If you won't vote for Dutertevote for Miriam. ...\n",
       "5776     Naka sale sa shoppee bili na mga kakampinksThe...\n",
       "3021     [USERNAME] Hello po[USERNAME] [USERNAME] [USER...\n",
       "2576     Bakit kaya ang mga pilipino uto-uto, konting s...\n",
       "7238     he has zero integrity. cant believe people are...\n",
       "Name: text, Length: 22768, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    11476\n",
       "0    11292\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer()\n",
    "\n",
    "transformed_text = countvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12426)\t1\n",
      "  (0, 10735)\t1\n",
      "  (0, 28459)\t1\n",
      "  (0, 28743)\t1\n",
      "  (0, 9683)\t1\n",
      "  (0, 21866)\t1\n",
      "  (0, 16523)\t1\n",
      "  (0, 37039)\t1\n",
      "  (0, 27778)\t1\n",
      "  (0, 32916)\t1\n",
      "  (0, 36289)\t1\n",
      "  (0, 34255)\t1\n",
      "  (0, 2440)\t1\n",
      "  (0, 7415)\t1\n",
      "  (0, 18054)\t1\n",
      "  (0, 36275)\t1\n",
      "  (0, 21554)\t1\n",
      "  (1, 3845)\t1\n",
      "  (1, 849)\t1\n",
      "  (1, 29055)\t1\n",
      "  (1, 26055)\t1\n",
      "  (1, 36999)\t1\n",
      "  (1, 24223)\t1\n",
      "  (1, 38573)\t1\n",
      "  (1, 30468)\t1\n",
      "  :\t:\n",
      "  (22766, 22736)\t2\n",
      "  (22766, 28492)\t1\n",
      "  (22766, 11035)\t1\n",
      "  (22766, 32027)\t1\n",
      "  (22766, 11054)\t1\n",
      "  (22766, 24334)\t1\n",
      "  (22766, 37656)\t2\n",
      "  (22766, 17092)\t1\n",
      "  (22766, 22640)\t1\n",
      "  (22767, 2180)\t1\n",
      "  (22767, 20985)\t3\n",
      "  (22767, 12582)\t1\n",
      "  (22767, 12475)\t1\n",
      "  (22767, 10571)\t1\n",
      "  (22767, 33475)\t1\n",
      "  (22767, 19502)\t1\n",
      "  (22767, 12847)\t1\n",
      "  (22767, 8882)\t1\n",
      "  (22767, 28125)\t1\n",
      "  (22767, 34327)\t1\n",
      "  (22767, 5372)\t1\n",
      "  (22767, 3519)\t1\n",
      "  (22767, 38097)\t1\n",
      "  (22767, 39389)\t1\n",
      "  (22767, 14336)\t1\n"
     ]
    }
   ],
   "source": [
    "print(transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test = countvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2846)\t1\n",
      "  (0, 3845)\t1\n",
      "  (0, 6371)\t1\n",
      "  (0, 18042)\t1\n",
      "  (0, 21523)\t1\n",
      "  (0, 22736)\t1\n",
      "  (0, 24091)\t1\n",
      "  (0, 33183)\t1\n",
      "  (1, 1452)\t1\n",
      "  (1, 14553)\t1\n",
      "  (1, 16716)\t1\n",
      "  (1, 16805)\t1\n",
      "  (1, 18096)\t2\n",
      "  (1, 18147)\t1\n",
      "  (1, 24662)\t1\n",
      "  (1, 31545)\t1\n",
      "  (1, 35772)\t1\n",
      "  (1, 36275)\t1\n",
      "  (1, 38358)\t1\n",
      "  (2, 8882)\t1\n",
      "  (2, 19502)\t1\n",
      "  (2, 20985)\t3\n",
      "  (2, 33475)\t1\n",
      "  (3, 7441)\t1\n",
      "  (3, 10571)\t1\n",
      "  :\t:\n",
      "  (5691, 17266)\t1\n",
      "  (5691, 17969)\t2\n",
      "  (5691, 18096)\t3\n",
      "  (5691, 18265)\t2\n",
      "  (5691, 22133)\t1\n",
      "  (5691, 22468)\t1\n",
      "  (5691, 22703)\t2\n",
      "  (5691, 24855)\t1\n",
      "  (5691, 25601)\t1\n",
      "  (5691, 27736)\t1\n",
      "  (5691, 31682)\t1\n",
      "  (5691, 32294)\t1\n",
      "  (5691, 35772)\t3\n",
      "  (5691, 35913)\t1\n",
      "  (5691, 36048)\t1\n",
      "  (5691, 36275)\t3\n",
      "  (5691, 38066)\t1\n",
      "  (5691, 38285)\t1\n",
      "  (5691, 38358)\t1\n",
      "  (5691, 38397)\t1\n",
      "  (5691, 38912)\t1\n",
      "  (5692, 2671)\t1\n",
      "  (5692, 3845)\t1\n",
      "  (5692, 33183)\t1\n",
      "  (5692, 33475)\t1\n"
     ]
    }
   ],
   "source": [
    "print(transformed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial (because it's close to Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class MultinomialNB():\n",
    "  def __init__(self, alpha=1.0):\n",
    "    # Smoothing parameter\n",
    "    # Used to prevent features from being totally 0 when getting feature counts\n",
    "    # Essentially tells NB that there's an extra count to all features\n",
    "    # which would otherwise zero things when in the case that a feature\n",
    "    # when predicting was not learned from fitting\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    if X.shape[0] != len(y):\n",
    "      print(\"ERROR: X and y have different lengths\")\n",
    "      return\n",
    "\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    X = X.toarray()\n",
    "\n",
    "    # Group X by class\n",
    "    # Use a default dict for this. This automatically makes a list\n",
    "    # when it encounters a key that is unknown. So we can iterate over\n",
    "    # each X and y, and append to the list of each respective class\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for X_sample, y_sample in zip(X, y):\n",
    "      grouped[y_sample].append(X_sample)\n",
    "\n",
    "    # STEP 1: Getting prior log probability of each class\n",
    "    # Essentially, the (log) probability of class being 0 or 1 based on train set\n",
    "    # Log scale is used to prevent underflow due to lack of floating point precision\n",
    "    self.class_log_prior_ = [\n",
    "      np.log(len(grouped[data_class]) / num_of_samples)\n",
    "      for data_class\n",
    "      in grouped.keys()\n",
    "    ]\n",
    "    # Output: array([-0.70126153, -0.68509814])\n",
    "    # These are log probabilities. To reverse this, we can use np.exp()\n",
    "    # and the result after np.exp() is\n",
    "    # Output: array([0.49595924, 0.50404076])\n",
    "    # We will consistently use these log probabilities elsewhere in calculations\n",
    "\n",
    "    # STEP 2: Count each word in each class. Add smoothing parameter (self.alpha)\n",
    "    # Make a list so it's easy to append to, numpy arrays are annoying to make 2D arrays\n",
    "    # from several 1D arrays\n",
    "    word_count_per_class = []\n",
    "\n",
    "    # Iterate over each class (0, then 1)\n",
    "    for data_class in grouped.keys():\n",
    "      # Sum the count of every word occurrence by column, going downward\n",
    "      # So if the array looks something like\n",
    "      # [[1 2 3]\n",
    "      #  [2 3 4]\n",
    "      #  [3 4 5]]\n",
    "      # sum(axis=0) will turn this into\n",
    "      # [6, 9, 12]\n",
    "      count_in_class = np.array(grouped[data_class]).sum(axis=0)\n",
    "\n",
    "      word_count_per_class.append(count_in_class)\n",
    "    # Make a final array by joining together the 2 lists to make it 2D\n",
    "    # then we add the alpha value to everything\n",
    "    word_count_per_class = np.array(word_count_per_class) + self.alpha\n",
    "\n",
    "    # Reshape to make it a 2D array where the sum corresponds to the row of a class\n",
    "    # [[244154], -> 0\n",
    "    #  [246500]] -> 1\n",
    "    sum_of_words_in_each_class = word_count_per_class.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # STEP 3: Calculate log probability of each word\n",
    "    self.feature_log_prob_ = np.log(word_count_per_class / sum_of_words_in_each_class)\n",
    "  \n",
    "  def predict_log_proba(self, X):\n",
    "    X = X.toarray()\n",
    "    samples = []\n",
    "    for sample in X:\n",
    "      feature_log_prob_by_count = self.feature_log_prob_ * sample\n",
    "      sum_of_feature_probabilities = feature_log_prob_by_count.sum(axis=1)\n",
    "      feature_probabilities_with_class_prior = sum_of_feature_probabilities + self.class_log_prior_\n",
    "      samples.append(feature_probabilities_with_class_prior)\n",
    "    return np.array(samples)\n",
    "    \n",
    "  def predict_proba(self, X):\n",
    "    outputs = self.predict_log_proba(X)\n",
    "    \n",
    "    results = []\n",
    "    for data_class in outputs:\n",
    "      output_max = data_class.max()\n",
    "      logsumexp_result = output_max + np.log(np.sum(np.exp(data_class - output_max)))\n",
    "      results.append(logsumexp_result)\n",
    "    results = np.array(results).reshape(-1, 1)\n",
    "\n",
    "    return np.exp(outputs - results)\n",
    "    # return np.exp(outputs - logsumexp_result.reshape(-1, 1))\n",
    "\n",
    "  def predict(self, X):\n",
    "    outputs = self.predict_log_proba(X)\n",
    "\n",
    "    return np.argmax(outputs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultinomialNB at 0x7f6e55d63880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(transformed_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.16214331e-04, 9.99583786e-01],\n",
       "       [9.99999977e-01, 2.34820687e-08],\n",
       "       [4.64053288e-08, 9.99999954e-01],\n",
       "       ...,\n",
       "       [4.62296229e-06, 9.99995377e-01],\n",
       "       [1.00000000e+00, 1.73354161e-18],\n",
       "       [1.64564609e-02, 9.83543539e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict_proba(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mnb.predict(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171438608817847"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom class log priors: [-0.7012615309562537, -0.6850981432283781]\n",
      "Sklearn class log priors: [-0.70126153 -0.68509814]\n",
      "Custom feature log probabilities: [[-10.32611291 -10.61379498 -11.71240727 ... -12.40555445 -12.40555445\n",
      "  -12.40555445]\n",
      " [-11.72197009 -11.31650498 -12.41511727 ... -11.02882291 -11.72197009\n",
      "  -11.72197009]]\n",
      "Sklearn feature log probabilities: [[-10.32611291 -10.61379498 -11.71240727 ... -12.40555445 -12.40555445\n",
      "  -12.40555445]\n",
      " [-11.72197009 -11.31650498 -12.41511727 ... -11.02882291 -11.72197009\n",
      "  -11.72197009]]\n",
      "Custom predictions: [1 0 1 ... 1 0 1]\n",
      "Sklearn predictions: [1 0 1 ... 1 0 1]\n",
      "Custom prediction probabilities: [[4.16214331e-04 9.99583786e-01]\n",
      " [9.99999977e-01 2.34820687e-08]\n",
      " [4.64053288e-08 9.99999954e-01]\n",
      " ...\n",
      " [4.62296229e-06 9.99995377e-01]\n",
      " [1.00000000e+00 1.73354161e-18]\n",
      " [1.64564609e-02 9.83543539e-01]]\n",
      "Sklearn prediction probabilities: [[4.16214331e-04 9.99583786e-01]\n",
      " [9.99999977e-01 2.34820687e-08]\n",
      " [4.64053288e-08 9.99999954e-01]\n",
      " ...\n",
      " [4.62296229e-06 9.99995377e-01]\n",
      " [1.00000000e+00 1.73354161e-18]\n",
      " [1.64564609e-02 9.83543539e-01]]\n",
      "Custom prediction log probabilities: [[ -56.21992501  -48.4360311 ]\n",
      " [ -73.65068345  -91.21771217]\n",
      " [ -47.99130491  -31.10545341]\n",
      " ...\n",
      " [-217.27468016 -204.99020991]\n",
      " [-333.22239849 -374.11876367]\n",
      " [ -26.78925124  -22.6988075 ]]\n",
      "Sklearn prediction log probabilities: [[-7.78431021e+00 -4.16300972e-04]\n",
      " [-2.34820732e-08 -1.75670287e+01]\n",
      " [-1.68858515e+01 -4.64053294e-08]\n",
      " ...\n",
      " [-1.22844749e+01 -4.62297297e-06]\n",
      " [ 0.00000000e+00 -4.08963652e+01]\n",
      " [-4.10703712e+00 -1.65933726e-02]]\n",
      "Custom accuracy: 0.8171438608817847\n",
      "Sklearn accuracy: 0.8171438608817847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as SklearnMultinomialNB\n",
    "\n",
    "# Fit scikit-learn implementation\n",
    "sklearn_nb = SklearnMultinomialNB()\n",
    "sklearn_nb.fit(transformed_text, y_train)\n",
    "\n",
    "# Compare class log priors\n",
    "print(\"Custom class log priors:\", mnb.class_log_prior_)\n",
    "print(\"Sklearn class log priors:\", sklearn_nb.class_log_prior_)\n",
    "\n",
    "# Compare feature log probabilities\n",
    "print(\"Custom feature log probabilities:\", mnb.feature_log_prob_)\n",
    "print(\"Sklearn feature log probabilities:\", sklearn_nb.feature_log_prob_)\n",
    "\n",
    "# Compare predictions\n",
    "custom_pred = mnb.predict(transformed_test)\n",
    "sklearn_pred = sklearn_nb.predict(transformed_test)\n",
    "print(\"Custom predictions:\", custom_pred)\n",
    "print(\"Sklearn predictions:\", sklearn_pred)\n",
    "\n",
    "# Compare prediction probabilities\n",
    "custom_proba = mnb.predict_proba(transformed_test)\n",
    "sklearn_proba = sklearn_nb.predict_proba(transformed_test)\n",
    "print(\"Custom prediction probabilities:\", custom_proba)\n",
    "print(\"Sklearn prediction probabilities:\", sklearn_proba)\n",
    "\n",
    "# Compare prediction log probabilities\n",
    "custom_log_proba = mnb.predict_log_proba(transformed_test)\n",
    "sklearn_log_proba = sklearn_nb.predict_log_proba(transformed_test)\n",
    "print(\"Custom prediction log probabilities:\", custom_log_proba)\n",
    "print(\"Sklearn prediction log probabilities:\", sklearn_log_proba)\n",
    "\n",
    "# Compare accuracy\n",
    "custom_accuracy = accuracy_score(y_test, custom_pred)\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "print(\"Custom accuracy:\", custom_accuracy)\n",
    "print(\"Sklearn accuracy:\", sklearn_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB():\n",
    "  def __init__(self, alpha=1.0):\n",
    "    # Smoothing parameter\n",
    "    # Used to prevent features from being totally 0 when getting feature counts\n",
    "    # Essentially tells NB that there's an extra count to all features\n",
    "    # which would otherwise zero things when in the case that a feature\n",
    "    # when predicting was not learned from fitting\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    if X.shape[0] != len(y):\n",
    "      print(\"ERROR: X and y have different lengths\")\n",
    "      return\n",
    "\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    X = X.toarray()\n",
    "\n",
    "    # Turn counts into binary values\n",
    "    # NOTE: This is the assumption of Bernoulli NB\n",
    "    X = np.where(X > 0, 1, 0)\n",
    "\n",
    "    # Group X by class\n",
    "    # Use a default dict for this. This automatically makes a list\n",
    "    # when it encounters a key that is unknown. So we can iterate over\n",
    "    # each X and y, and append to the list of each respective class\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for X_sample, y_sample in zip(X, y):\n",
    "      grouped[y_sample].append(X_sample)\n",
    "\n",
    "    # STEP 1: Getting prior log probability of each class\n",
    "    # Essentially, the (log) probability of class being 0 or 1 based on train set\n",
    "    # Log scale is used to prevent underflow due to lack of floating point precision\n",
    "    # Equation: Number of samples in class / number of overall samples\n",
    "    # If there are 10 samples, 4 are 0 and 6 are 1\n",
    "    # The equation then looks like\n",
    "    # np.log(4/10) for 0\n",
    "    # np.log(6/10) for 1\n",
    "    self.class_log_prior_ = [\n",
    "      np.log(len(grouped[data_class]) / num_of_samples)\n",
    "      for data_class\n",
    "      in grouped.keys()\n",
    "    ]\n",
    "    # Output: array([-0.70126153, -0.68509814])\n",
    "    # These are log probabilities. To reverse this, we can use np.exp()\n",
    "    # and the result after np.exp() is\n",
    "    # Output: array([0.49595924, 0.50404076])\n",
    "    # This is the same as:\n",
    "    # 11292 / (11476 + 11292)\n",
    "    # 11476 / (11476 + 11292)\n",
    "    # We will consistently use these log probabilities elsewhere in calculations\n",
    "\n",
    "    # STEP 2: Count each word in each class. Add smoothing parameter (self.alpha)\n",
    "    # The smoothing parameter is important to prevent divison by 0\n",
    "    # Make a list so it's easy to append to, numpy arrays are annoying to make 2D arrays\n",
    "    # from several 1D arrays\n",
    "    word_count_per_class = []\n",
    "\n",
    "    # Iterate over each class (0, then 1)\n",
    "    for data_class in grouped.keys():\n",
    "      # Sum the count of every word occurrence by column, going downward\n",
    "      # So if the array looks something like\n",
    "      # [[1 2 3]\n",
    "      #  [2 3 4]\n",
    "      #  [3 4 5]]\n",
    "      # sum(axis=0) will turn this into\n",
    "      # [6, 9, 12]\n",
    "      count_in_class = np.array(grouped[data_class]).sum(axis=0)\n",
    "\n",
    "      word_count_per_class.append(count_in_class)\n",
    "    # Make a final array by joining together the 2 lists to make it 2D\n",
    "    # then we add the alpha value to everything\n",
    "    # This alpha is important to prevent division by 0. You can observe this\n",
    "    # by setting the alpha to 0. You will get a lot of nan values\n",
    "    word_count_per_class = np.array(word_count_per_class) + self.alpha\n",
    "\n",
    "    # Reshape to make it a 2D array where the sum corresponds to the row of a class\n",
    "    # [[223218], -> 0\n",
    "    #  [225122]] -> 1\n",
    "    sum_of_words_in_each_class = word_count_per_class.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # STEP 3: Calculate log probability of each word\n",
    "    # NOTE: Not actually used in BNB\n",
    "    self.feature_log_prob_ = np.log(word_count_per_class / sum_of_words_in_each_class)\n",
    "\n",
    "    # STEP 4: BNB\n",
    "    # Smoothing parameter for classes\n",
    "    # Multiplied by 2 because of 2 classes, 0 and 1\n",
    "    smoothing = 2 * self.alpha\n",
    "    \n",
    "    # Add the smoothing to number of documents\n",
    "    num_of_documents_with_smoothing = np.array([\n",
    "      len(grouped[data_class]) + smoothing\n",
    "      for data_class \n",
    "      in grouped.keys()\n",
    "    ])\n",
    "\n",
    "    # STEP 5: Feature Probabilities\n",
    "    # (Word count in class [0 or 1] + alpha) / (value count [11292, 11476] + alpha * 2)\n",
    "    # Given an alpha of 1,\n",
    "    # If you plug in 1.77085178e-04 * 11294, you get 1.999~. This means the original word\n",
    "    # appeared once (2 - alpha = 1) in the 0 class\n",
    "    self.feature_prob_ = word_count_per_class / num_of_documents_with_smoothing.reshape(-1, 1)\n",
    "\n",
    "    return self\n",
    "  \n",
    "  def predict_log_proba(self, X):\n",
    "    \"\"\"\n",
    "    This function gets the numerator part of Bernoulli NB\n",
    "    Read carefully\n",
    "    \"\"\"\n",
    "    X = X.toarray()\n",
    "    # Turn counts into binary values\n",
    "    X = np.where(X > 0, 1, 0)\n",
    "\n",
    "    samples = []\n",
    "    for sample in X:\n",
    "      # This part is solving for the numerator\n",
    "      # For each sample of data,\n",
    "      # Sum the log probabilities of all the features that appear in the class\n",
    "      # and add that to the log probabilities that the features do not appear in the class\n",
    "      # then add once again to the log probability of the class (this is the class prior)\n",
    "      negative_prob = 1 - self.feature_prob_\n",
    "\n",
    "      # Invert from 0 to 1, or 1 to 0\n",
    "      inverted_sample = np.abs(sample - 1)\n",
    "\n",
    "      # We multiply feature prob to sample to essentially\n",
    "      # zero out all features that do not appear\n",
    "      # This leaves us with the features that appear as the only nonzero values\n",
    "      # so we can simply add them; this is the same as\n",
    "      # multiplying all the probabilities of all the features\n",
    "      # but we can just add because we use the log scale\n",
    "      # The inverted probabilities are based on the Bernoulli rule\n",
    "      # P(x_i∣y) = P(i∣y)x_i + (1 − P(i∣y)) * (1 − x_i)\n",
    "      # sample probabilities = P(i∣y)x_i\n",
    "      # and the inverted probabilities represent the \n",
    "      # (1 − P(i∣y)) * (1 − x_i)\n",
    "      # portion\n",
    "      sample_probabilities = np.log(self.feature_prob_) * sample\n",
    "      inverted_probabilities = np.log(negative_prob) * inverted_sample\n",
    "\n",
    "      log_sum_of_probabilities = (sample_probabilities + inverted_probabilities).sum(axis=1)\n",
    "      feature_probabilities_with_class_prior = log_sum_of_probabilities + self.class_log_prior_\n",
    "      samples.append(feature_probabilities_with_class_prior)\n",
    "    return np.array(samples)\n",
    "    \n",
    "  def predict_proba(self, X):\n",
    "    # Get the numerator part\n",
    "    outputs = self.predict_log_proba(X)\n",
    "    \n",
    "    results = []\n",
    "    for data_class in outputs:\n",
    "      # The following two lines are a manual implementation\n",
    "      # of the logsumexp function\n",
    "      # The main purpose is to prevent numerical underflow\n",
    "      # in the denominator due to the need for summation in the formula\n",
    "      # Reference: https://stats.stackexchange.com/questions/105602/example-of-how-the-log-sum-exp-trick-works-in-naive-bayes\n",
    "      output_max = data_class.max()\n",
    "      logsumexp_result = output_max + np.log(np.sum(np.exp(data_class - output_max)))\n",
    "\n",
    "      results.append(logsumexp_result)\n",
    "    results = np.array(results).reshape(-1, 1)\n",
    "\n",
    "    return np.exp(outputs - results)\n",
    "\n",
    "  def predict(self, X):\n",
    "    outputs = self.predict_log_proba(X)\n",
    "\n",
    "    return np.argmax(outputs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom class log priors: [-0.7012615309562537, -0.6850981432283781]\n",
      "Sklearn class log priors: [-0.70126153 -0.68509814]\n",
      "Custom feature log probabilities: [[-7.38611674 -7.54026742 -8.63887971 ... -9.33202689 -9.33202689\n",
      "  -9.33202689]\n",
      " [-8.65504026 -8.65504026 -9.34818744 ... -7.96189308 -8.65504026\n",
      "  -8.65504026]]\n",
      "Sklearn feature log probabilities: [[-7.38611674 -7.54026742 -8.63887971 ... -9.33202689 -9.33202689\n",
      "  -9.33202689]\n",
      " [-8.65504026 -8.65504026 -9.34818744 ... -7.96189308 -8.65504026\n",
      "  -8.65504026]]\n",
      "Custom predictions: [1 0 1 ... 1 0 1]\n",
      "Sklearn predictions: [1 0 1 ... 1 0 1]\n",
      "Custom prediction probabilities: [[2.11246308e-04 9.99788754e-01]\n",
      " [9.99999931e-01 6.87523480e-08]\n",
      " [7.01404124e-06 9.99992986e-01]\n",
      " ...\n",
      " [1.11873689e-05 9.99988813e-01]\n",
      " [1.00000000e+00 8.02383436e-14]\n",
      " [9.58222418e-03 9.90417776e-01]]\n",
      "Sklearn prediction probabilities: [[2.11246308e-04 9.99788754e-01]\n",
      " [9.99999931e-01 6.87523480e-08]\n",
      " [7.01404124e-06 9.99992986e-01]\n",
      " ...\n",
      " [1.11873689e-05 9.99988813e-01]\n",
      " [1.00000000e+00 8.02383436e-14]\n",
      " [9.58222418e-03 9.90417776e-01]]\n",
      "Custom prediction log probabilities: [[ -52.26164384  -43.79936934]\n",
      " [ -56.52623352  -73.0189884 ]\n",
      " [ -42.79396165  -30.92637213]\n",
      " ...\n",
      " [-147.19190621 -135.79119221]\n",
      " [-178.35263829 -208.50641318]\n",
      " [ -34.96897868  -30.33076157]]\n",
      "Sklearn prediction log probabilities: [[-8.46248577e+00 -2.11268623e-04]\n",
      " [-6.87523496e-08 -1.64927549e+01]\n",
      " [-1.18675965e+01 -7.01406584e-06]\n",
      " ...\n",
      " [-1.14007252e+01 -1.11874315e-05]\n",
      " [-8.52651283e-14 -3.01537749e+01]\n",
      " [-4.64784554e+00 -9.62842909e-03]]\n",
      "Custom accuracy: 0.8157386263832777\n",
      "Sklearn accuracy: 0.8157386263832777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB as SklearnBernoulliNB\n",
    "\n",
    "custom_bnb = BernoulliNB()\n",
    "custom_bnb.fit(transformed_text, y_train)\n",
    "\n",
    "# Fit scikit-learn implementation\n",
    "sklearn_bnb = SklearnBernoulliNB()\n",
    "sklearn_bnb.fit(transformed_text, y_train)\n",
    "\n",
    "# Compare class log priors\n",
    "print(\"Custom class log priors:\", custom_bnb.class_log_prior_)\n",
    "print(\"Sklearn class log priors:\", sklearn_bnb.class_log_prior_)\n",
    "\n",
    "# Compare feature log probabilities\n",
    "print(\"Custom feature log probabilities:\", np.log(custom_bnb.feature_prob_))\n",
    "print(\"Sklearn feature log probabilities:\", sklearn_bnb.feature_log_prob_)\n",
    "\n",
    "# Compare predictions\n",
    "custom_pred = custom_bnb.predict(transformed_test)\n",
    "sklearn_pred = sklearn_bnb.predict(transformed_test)\n",
    "print(\"Custom predictions:\", custom_pred)\n",
    "print(\"Sklearn predictions:\", sklearn_pred)\n",
    "\n",
    "# Compare prediction probabilities\n",
    "custom_proba = custom_bnb.predict_proba(transformed_test)\n",
    "sklearn_proba = sklearn_bnb.predict_proba(transformed_test)\n",
    "print(\"Custom prediction probabilities:\", custom_proba)\n",
    "print(\"Sklearn prediction probabilities:\", sklearn_proba)\n",
    "\n",
    "# Compare prediction log probabilities\n",
    "custom_log_proba = custom_bnb.predict_log_proba(transformed_test)\n",
    "sklearn_log_proba = sklearn_bnb.predict_log_proba(transformed_test)\n",
    "print(\"Custom prediction log probabilities:\", custom_log_proba)\n",
    "print(\"Sklearn prediction log probabilities:\", sklearn_log_proba)\n",
    "\n",
    "# Compare accuracy\n",
    "custom_accuracy = accuracy_score(y_test, custom_pred)\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "print(\"Custom accuracy:\", custom_accuracy)\n",
    "print(\"Sklearn accuracy:\", sklearn_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tests below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.369994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10.524145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-11.217292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-10.706466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-10.929610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-10.929610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-10.929610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-8.760556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-10.524145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-11.217292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-11.217292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-12.315904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-9.224862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-11.622757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0  -10.369994\n",
       "1  -10.524145\n",
       "2  -11.622757\n",
       "3  -12.315904\n",
       "4  -11.622757\n",
       "5  -11.622757\n",
       "6  -11.622757\n",
       "7  -11.217292\n",
       "8  -11.622757\n",
       "9  -10.706466\n",
       "10 -11.622757\n",
       "11 -11.622757\n",
       "12 -12.315904\n",
       "13 -10.929610\n",
       "14 -11.622757\n",
       "15 -11.622757\n",
       "16 -12.315904\n",
       "17 -11.622757\n",
       "18 -11.622757\n",
       "19 -11.622757\n",
       "20 -11.622757\n",
       "21 -11.622757\n",
       "22 -10.929610\n",
       "23 -11.622757\n",
       "24 -10.929610\n",
       "25 -11.622757\n",
       "26 -12.315904\n",
       "27 -12.315904\n",
       "28  -8.760556\n",
       "29 -10.524145\n",
       "30 -12.315904\n",
       "31 -11.622757\n",
       "32 -12.315904\n",
       "33 -12.315904\n",
       "34 -12.315904\n",
       "35 -12.315904\n",
       "36 -11.622757\n",
       "37 -12.315904\n",
       "38 -11.622757\n",
       "39 -11.217292\n",
       "40 -12.315904\n",
       "41 -11.217292\n",
       "42 -12.315904\n",
       "43 -11.622757\n",
       "44 -11.622757\n",
       "45 -11.622757\n",
       "46  -9.224862\n",
       "47 -11.622757\n",
       "48 -11.622757\n",
       "49 -11.622757"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(custom_bnb.feature_log_prob_[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.386117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.540267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-8.233415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7.722589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-7.945733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-7.945733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-7.945733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-5.776679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-7.540267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-8.233415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-8.233415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-9.332027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-6.240984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-8.638880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  -7.386117\n",
       "1  -7.540267\n",
       "2  -8.638880\n",
       "3  -9.332027\n",
       "4  -8.638880\n",
       "5  -8.638880\n",
       "6  -8.638880\n",
       "7  -8.233415\n",
       "8  -8.638880\n",
       "9  -7.722589\n",
       "10 -8.638880\n",
       "11 -8.638880\n",
       "12 -9.332027\n",
       "13 -7.945733\n",
       "14 -8.638880\n",
       "15 -8.638880\n",
       "16 -9.332027\n",
       "17 -8.638880\n",
       "18 -8.638880\n",
       "19 -8.638880\n",
       "20 -8.638880\n",
       "21 -8.638880\n",
       "22 -7.945733\n",
       "23 -8.638880\n",
       "24 -7.945733\n",
       "25 -8.638880\n",
       "26 -9.332027\n",
       "27 -9.332027\n",
       "28 -5.776679\n",
       "29 -7.540267\n",
       "30 -9.332027\n",
       "31 -8.638880\n",
       "32 -9.332027\n",
       "33 -9.332027\n",
       "34 -9.332027\n",
       "35 -9.332027\n",
       "36 -8.638880\n",
       "37 -9.332027\n",
       "38 -8.638880\n",
       "39 -8.233415\n",
       "40 -9.332027\n",
       "41 -8.233415\n",
       "42 -9.332027\n",
       "43 -8.638880\n",
       "44 -8.638880\n",
       "45 -8.638880\n",
       "46 -6.240984\n",
       "47 -8.638880\n",
       "48 -8.638880\n",
       "49 -8.638880"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sklearn_bnb.feature_log_prob_[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.369994002343624"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(7.0/223218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.19798123e-04, 5.31255534e-04, 1.77085178e-04, 8.85425890e-05,\n",
       "       1.77085178e-04, 1.77085178e-04, 1.77085178e-04, 2.65627767e-04,\n",
       "       1.77085178e-04, 4.42712945e-04, 1.77085178e-04, 1.77085178e-04,\n",
       "       8.85425890e-05, 3.54170356e-04, 1.77085178e-04, 1.77085178e-04,\n",
       "       8.85425890e-05, 1.77085178e-04, 1.77085178e-04, 1.77085178e-04,\n",
       "       1.77085178e-04, 1.77085178e-04, 3.54170356e-04, 1.77085178e-04,\n",
       "       3.54170356e-04, 1.77085178e-04, 8.85425890e-05, 8.85425890e-05,\n",
       "       3.09899061e-03, 5.31255534e-04, 8.85425890e-05, 1.77085178e-04,\n",
       "       8.85425890e-05, 8.85425890e-05, 8.85425890e-05, 8.85425890e-05,\n",
       "       1.77085178e-04, 8.85425890e-05, 1.77085178e-04, 2.65627767e-04,\n",
       "       8.85425890e-05, 2.65627767e-04, 8.85425890e-05, 1.77085178e-04,\n",
       "       1.77085178e-04, 1.77085178e-04, 1.94793696e-03, 1.77085178e-04,\n",
       "       1.77085178e-04, 1.77085178e-04])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bnb.feature_prob_[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9993802 , 0.99946874, 0.99982291, 0.99991146, 0.99982291,\n",
       "       0.99982291, 0.99982291, 0.99973437, 0.99982291, 0.99955729,\n",
       "       0.99982291, 0.99982291, 0.99991146, 0.99964583, 0.99982291,\n",
       "       0.99982291, 0.99991146, 0.99982291, 0.99982291, 0.99982291,\n",
       "       0.99982291, 0.99982291, 0.99964583, 0.99982291, 0.99964583,\n",
       "       0.99982291, 0.99991146, 0.99991146, 0.99690101, 0.99946874,\n",
       "       0.99991146, 0.99982291, 0.99991146, 0.99991146, 0.99991146,\n",
       "       0.99991146, 0.99982291, 0.99991146, 0.99982291, 0.99973437,\n",
       "       0.99991146, 0.99973437, 0.99991146, 0.99982291, 0.99982291,\n",
       "       0.99982291, 0.99805206, 0.99982291, 0.99982291, 0.99982291])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - custom_bnb.feature_prob_[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00053126, 0.00017709, 0.        , 0.00017709,\n",
       "       0.        , 0.        , 0.        , 0.00017709, 0.00044271])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_normal_prob = custom_bnb.feature_prob_[0][:10] * np.array([\n",
    "  0, 1, 1, 0, 1,\n",
    "  0, 0, 0, 1, 1\n",
    "])\n",
    "test_normal_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9993802 , 0.        , 0.        , 0.99991146, 0.        ,\n",
       "       0.99982291, 0.99982291, 0.99973437, 0.        , 0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inverted_prob = (1 - custom_bnb.feature_prob_[0][:10]) * np.array([\n",
    "  1, 0, 0, 1, 0,\n",
    "  1, 1, 1, 0, 0\n",
    "])\n",
    "test_inverted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99380202e-01, 5.31255534e-04, 1.77085178e-04, 9.99911457e-01,\n",
       "       1.77085178e-04, 9.99822915e-01, 9.99822915e-01, 9.99734372e-01,\n",
       "       1.77085178e-04, 4.42712945e-04])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_normal_prob + test_inverted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.00017708517797"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_normal_prob + test_inverted_prob).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.00017708517797"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_normal_prob + test_inverted_prob).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 1.74246384e-04, 8.71231922e-05, 0.00000000e+00,\n",
       "       8.71231922e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.71231922e-05, 8.71231922e-05])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bnb.feature_prob_[1][:10] * np.array([\n",
    "  0, 1, 1, 0, 1,\n",
    "  0, 0, 0, 1, 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006737946999085467"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
