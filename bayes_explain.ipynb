{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TRAIN = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The sky is blue and the sun is shining.\",\n",
    "    \"Cats are wonderful pets to have around the house.\",\n",
    "    \"Soccer is a popular sport played all around the world.\",\n",
    "    \"Pizza topped with cheese and pepperoni is my favorite.\",\n",
    "    \"Learning a new language can be challenging but rewarding.\",\n",
    "    \"Reading books is a great way to expand your knowledge.\",\n",
    "    \"The internet has revolutionized the way we communicate.\",\n",
    "    \"Music has the power to evoke strong emotions in people.\",\n",
    "    \"Exercise is important for maintaining good health.\",\n",
    "    \"Science has made incredible advancements in recent years.\",\n",
    "    \"Coffee is the fuel that keeps many people going throughout the day.\",\n",
    "    \"Traveling allows you to experience different cultures and cuisines.\",\n",
    "    \"The importance of education cannot be overstated.\",\n",
    "    \"Technology continues to progress at a rapid pace.\",\n",
    "    \"Dogs are known for their loyalty and companionship.\",\n",
    "    \"Cooking homemade meals can be a fun and rewarding activity.\",\n",
    "    \"Nature has a way of calming the mind and soothing the soul.\",\n",
    "    \"Artistic expression comes in many forms, from painting to music.\",\n",
    "    \"Happiness is often found in the simplest of things.\"\n",
    "]\n",
    "\n",
    "SAMPLE_TRAIN_Y = [1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "SAMPLE_TEST = [\n",
    "    \"TF-IDF, short for term frequency-inverse document frequency, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents or a corpus.\",\n",
    "    \"The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word.\",\n",
    "    \"TF-IDF is commonly used in information retrieval and text mining as a weighting factor for scoring the relevance of a word to a document in a corpus.\",\n",
    "    \"The intuition behind TF-IDF is that words that occur frequently in a document but rarely in the corpus are more important in describing the content of the document.\",\n",
    "    \"In contrast, words that occur frequently in both the document and the corpus are less important as they are likely to be common words with less discriminative power.\",\n",
    "    \"To calculate the TF-IDF score for a word in a document, you multiply the term frequency (TF) by the inverse document frequency (IDF).\",\n",
    "    \"TF is the frequency of a term in a document, while IDF is the logarithmically scaled inverse fraction of the documents that contain the word.\",\n",
    "    \"By weighting the TF by IDF, the TF-IDF score penalizes words that are common across documents and emphasizes words that are unique to a specific document.\",\n",
    "    \"Once you have computed the TF-IDF scores for all words in a document, you can represent the document as a vector where each dimension corresponds to a word and the value corresponds to its TF-IDF score.\",\n",
    "    \"TF-IDF vectorization is a fundamental step in many natural language processing tasks, such as document classification, clustering, and information retrieval.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45952/3485358109.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(norm=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(norm=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(norm=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm=None)\n",
    "\n",
    "sklearn_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 111)\t2.0\n",
      "  (0, 93)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 43)\t1.0\n",
      "  (0, 62)\t1.0\n",
      "  (0, 81)\t1.0\n",
      "  (0, 67)\t1.0\n",
      "  (0, 30)\t1.0\n",
      "  (1, 111)\t2.0\n",
      "  (1, 102)\t1.0\n",
      "  (1, 61)\t2.0\n",
      "  (1, 10)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 108)\t1.0\n",
      "  (1, 100)\t1.0\n",
      "  (2, 111)\t1.0\n",
      "  (2, 17)\t1.0\n",
      "  (2, 5)\t1.0\n",
      "  (2, 121)\t1.0\n",
      "  (2, 87)\t1.0\n",
      "  (2, 115)\t1.0\n",
      "  (2, 52)\t1.0\n",
      "  (2, 6)\t1.0\n",
      "  (2, 55)\t1.0\n",
      "  (3, 111)\t1.0\n",
      "  :\t:\n",
      "  (17, 79)\t1.0\n",
      "  (17, 77)\t1.0\n",
      "  (17, 14)\t1.0\n",
      "  (17, 74)\t1.0\n",
      "  (17, 104)\t1.0\n",
      "  (17, 105)\t1.0\n",
      "  (18, 115)\t1.0\n",
      "  (18, 75)\t1.0\n",
      "  (18, 58)\t1.0\n",
      "  (18, 72)\t1.0\n",
      "  (18, 7)\t1.0\n",
      "  (18, 38)\t1.0\n",
      "  (18, 21)\t1.0\n",
      "  (18, 41)\t1.0\n",
      "  (18, 44)\t1.0\n",
      "  (18, 84)\t1.0\n",
      "  (19, 111)\t1.0\n",
      "  (19, 61)\t1.0\n",
      "  (19, 58)\t1.0\n",
      "  (19, 79)\t1.0\n",
      "  (19, 50)\t1.0\n",
      "  (19, 80)\t1.0\n",
      "  (19, 42)\t1.0\n",
      "  (19, 101)\t1.0\n",
      "  (19, 113)\t1.0\n"
     ]
    }
   ],
   "source": [
    "sklearn_result = sklearn_tfidf.fit_transform(SAMPLE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 30)\t3.3513752571634776\n",
      "  (0, 67)\t3.3513752571634776\n",
      "  (0, 81)\t3.3513752571634776\n",
      "  (0, 62)\t3.3513752571634776\n",
      "  (0, 43)\t3.3513752571634776\n",
      "  (0, 12)\t3.3513752571634776\n",
      "  (0, 93)\t3.3513752571634776\n",
      "  (0, 111)\t3.2932543298501047\n",
      "  (1, 100)\t3.3513752571634776\n",
      "  (1, 108)\t3.3513752571634776\n",
      "  (1, 4)\t2.09861228866811\n",
      "  (1, 10)\t3.3513752571634776\n",
      "  (1, 61)\t3.930161792087174\n",
      "  (1, 102)\t3.3513752571634776\n",
      "  (1, 111)\t3.2932543298501047\n",
      "  (2, 55)\t3.3513752571634776\n",
      "  (2, 6)\t2.9459101490553135\n",
      "  (2, 52)\t3.3513752571634776\n",
      "  (2, 115)\t2.09861228866811\n",
      "  (2, 87)\t3.3513752571634776\n",
      "  (2, 121)\t3.3513752571634776\n",
      "  (2, 5)\t2.9459101490553135\n",
      "  (2, 17)\t3.3513752571634776\n",
      "  (2, 111)\t1.6466271649250523\n",
      "  (3, 122)\t3.3513752571634776\n",
      "  :\t:\n",
      "  (17, 77)\t3.3513752571634776\n",
      "  (17, 79)\t2.6582280766035327\n",
      "  (17, 51)\t2.435084525289323\n",
      "  (17, 118)\t2.6582280766035327\n",
      "  (17, 4)\t2.09861228866811\n",
      "  (17, 111)\t3.2932543298501047\n",
      "  (18, 84)\t3.3513752571634776\n",
      "  (18, 44)\t3.3513752571634776\n",
      "  (18, 41)\t3.3513752571634776\n",
      "  (18, 21)\t3.3513752571634776\n",
      "  (18, 38)\t3.3513752571634776\n",
      "  (18, 7)\t3.3513752571634776\n",
      "  (18, 72)\t2.9459101490553135\n",
      "  (18, 58)\t2.435084525289323\n",
      "  (18, 75)\t2.9459101490553135\n",
      "  (18, 115)\t2.09861228866811\n",
      "  (19, 113)\t3.3513752571634776\n",
      "  (19, 101)\t3.3513752571634776\n",
      "  (19, 42)\t3.3513752571634776\n",
      "  (19, 80)\t3.3513752571634776\n",
      "  (19, 50)\t3.3513752571634776\n",
      "  (19, 79)\t2.6582280766035327\n",
      "  (19, 58)\t2.435084525289323\n",
      "  (19, 61)\t1.965080896043587\n",
      "  (19, 111)\t1.6466271649250523\n"
     ]
    }
   ],
   "source": [
    "print(sklearn_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sklearn_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 111,\n",
       " 'quick': 93,\n",
       " 'brown': 12,\n",
       " 'fox': 43,\n",
       " 'jumps': 62,\n",
       " 'over': 81,\n",
       " 'lazy': 67,\n",
       " 'dog': 30,\n",
       " 'sky': 102,\n",
       " 'is': 61,\n",
       " 'blue': 10,\n",
       " 'and': 4,\n",
       " 'sun': 108,\n",
       " 'shining': 100,\n",
       " 'cats': 17,\n",
       " 'are': 5,\n",
       " 'wonderful': 121,\n",
       " 'pets': 87,\n",
       " 'to': 115,\n",
       " 'have': 52,\n",
       " 'around': 6,\n",
       " 'house': 55,\n",
       " 'soccer': 103,\n",
       " 'popular': 90,\n",
       " 'sport': 106,\n",
       " 'played': 89,\n",
       " 'all': 2,\n",
       " 'world': 122,\n",
       " 'pizza': 88,\n",
       " 'topped': 116,\n",
       " 'with': 120,\n",
       " 'cheese': 19,\n",
       " 'pepperoni': 86,\n",
       " 'my': 76,\n",
       " 'favorite': 39,\n",
       " 'learning': 68,\n",
       " 'new': 78,\n",
       " 'language': 66,\n",
       " 'can': 15,\n",
       " 'be': 9,\n",
       " 'challenging': 18,\n",
       " 'but': 13,\n",
       " 'rewarding': 98,\n",
       " 'reading': 95,\n",
       " 'books': 11,\n",
       " 'great': 49,\n",
       " 'way': 118,\n",
       " 'expand': 36,\n",
       " 'your': 125,\n",
       " 'knowledge': 64,\n",
       " 'internet': 60,\n",
       " 'has': 51,\n",
       " 'revolutionized': 97,\n",
       " 'we': 119,\n",
       " 'communicate': 22,\n",
       " 'music': 75,\n",
       " 'power': 91,\n",
       " 'evoke': 34,\n",
       " 'strong': 107,\n",
       " 'emotions': 33,\n",
       " 'in': 58,\n",
       " 'people': 85,\n",
       " 'exercise': 35,\n",
       " 'important': 57,\n",
       " 'for': 40,\n",
       " 'maintaining': 71,\n",
       " 'good': 48,\n",
       " 'health': 53,\n",
       " 'science': 99,\n",
       " 'made': 70,\n",
       " 'incredible': 59,\n",
       " 'advancements': 1,\n",
       " 'recent': 96,\n",
       " 'years': 123,\n",
       " 'coffee': 20,\n",
       " 'fuel': 45,\n",
       " 'that': 110,\n",
       " 'keeps': 63,\n",
       " 'many': 72,\n",
       " 'going': 47,\n",
       " 'throughout': 114,\n",
       " 'day': 28,\n",
       " 'traveling': 117,\n",
       " 'allows': 3,\n",
       " 'you': 124,\n",
       " 'experience': 37,\n",
       " 'different': 29,\n",
       " 'cultures': 27,\n",
       " 'cuisines': 26,\n",
       " 'importance': 56,\n",
       " 'of': 79,\n",
       " 'education': 32,\n",
       " 'cannot': 16,\n",
       " 'overstated': 82,\n",
       " 'technology': 109,\n",
       " 'continues': 24,\n",
       " 'progress': 92,\n",
       " 'at': 8,\n",
       " 'rapid': 94,\n",
       " 'pace': 83,\n",
       " 'dogs': 31,\n",
       " 'known': 65,\n",
       " 'their': 112,\n",
       " 'loyalty': 69,\n",
       " 'companionship': 23,\n",
       " 'cooking': 25,\n",
       " 'homemade': 54,\n",
       " 'meals': 73,\n",
       " 'fun': 46,\n",
       " 'activity': 0,\n",
       " 'nature': 77,\n",
       " 'calming': 14,\n",
       " 'mind': 74,\n",
       " 'soothing': 104,\n",
       " 'soul': 105,\n",
       " 'artistic': 7,\n",
       " 'expression': 38,\n",
       " 'comes': 21,\n",
       " 'forms': 41,\n",
       " 'from': 44,\n",
       " 'painting': 84,\n",
       " 'happiness': 50,\n",
       " 'often': 80,\n",
       " 'found': 42,\n",
       " 'simplest': 101,\n",
       " 'things': 113}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lazy</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jumps</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fox</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brown</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quick</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text  index\n",
       "0    dog     30\n",
       "1   lazy     67\n",
       "2   over     81\n",
       "3  jumps     62\n",
       "4    fox     43\n",
       "5  brown     12\n",
       "6  quick     93\n",
       "7    the    111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = sklearn_tfidf.get_feature_names_out()\n",
    "sentence_indices = [30, 67, 81, 62, 43, 12, 93, 111]\n",
    "vocabulary[[sentence_indices]]\n",
    "pd.DataFrame({\n",
    "  'text': vocabulary[sentence_indices],\n",
    "  'index': sentence_indices,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEST_TRANSFORMED = sklearn_tfidf.transform(SAMPLE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Lowercase text\n",
    "preprocess_train = [\n",
    "  text.lower()\n",
    "  for text \n",
    "  in SAMPLE_TRAIN\n",
    "]\n",
    "\n",
    "# Remove punctuation\n",
    "preprocess_train = [\n",
    "  re.sub(r'[^\\w\\s]', '', text)\n",
    "  for text\n",
    "  in preprocess_train\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique word to set\n",
    "UNIQUE_WORDS = set()\n",
    "\n",
    "for sample in preprocess_train:\n",
    "  for word in sample.split(' '):\n",
    "    # Only add unique words of 2 or more characters\n",
    "    # SKLearn instead uses this regex pattern when\n",
    "    # processing text:\n",
    "    # r”(?u)\\b\\w\\w+\\b”\n",
    "    if len(word) < 2: continue\n",
    "    UNIQUE_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activity',\n",
       " 'advancements',\n",
       " 'all',\n",
       " 'allows',\n",
       " 'and',\n",
       " 'are',\n",
       " 'around',\n",
       " 'artistic',\n",
       " 'at',\n",
       " 'be',\n",
       " 'blue',\n",
       " 'books',\n",
       " 'brown',\n",
       " 'but',\n",
       " 'calming',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cats',\n",
       " 'challenging',\n",
       " 'cheese',\n",
       " 'coffee',\n",
       " 'comes',\n",
       " 'communicate',\n",
       " 'companionship',\n",
       " 'continues',\n",
       " 'cooking',\n",
       " 'cuisines',\n",
       " 'cultures',\n",
       " 'day',\n",
       " 'different',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'education',\n",
       " 'emotions',\n",
       " 'evoke',\n",
       " 'exercise',\n",
       " 'expand',\n",
       " 'experience',\n",
       " 'expression',\n",
       " 'favorite',\n",
       " 'for',\n",
       " 'forms',\n",
       " 'found',\n",
       " 'fox',\n",
       " 'from',\n",
       " 'fuel',\n",
       " 'fun',\n",
       " 'going',\n",
       " 'good',\n",
       " 'great',\n",
       " 'happiness',\n",
       " 'has',\n",
       " 'have',\n",
       " 'health',\n",
       " 'homemade',\n",
       " 'house',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'in',\n",
       " 'incredible',\n",
       " 'internet',\n",
       " 'is',\n",
       " 'jumps',\n",
       " 'keeps',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'language',\n",
       " 'lazy',\n",
       " 'learning',\n",
       " 'loyalty',\n",
       " 'made',\n",
       " 'maintaining',\n",
       " 'many',\n",
       " 'meals',\n",
       " 'mind',\n",
       " 'music',\n",
       " 'my',\n",
       " 'nature',\n",
       " 'new',\n",
       " 'of',\n",
       " 'often',\n",
       " 'over',\n",
       " 'overstated',\n",
       " 'pace',\n",
       " 'painting',\n",
       " 'people',\n",
       " 'pepperoni',\n",
       " 'pets',\n",
       " 'pizza',\n",
       " 'played',\n",
       " 'popular',\n",
       " 'power',\n",
       " 'progress',\n",
       " 'quick',\n",
       " 'rapid',\n",
       " 'reading',\n",
       " 'recent',\n",
       " 'revolutionized',\n",
       " 'rewarding',\n",
       " 'science',\n",
       " 'shining',\n",
       " 'simplest',\n",
       " 'sky',\n",
       " 'soccer',\n",
       " 'soothing',\n",
       " 'soul',\n",
       " 'sport',\n",
       " 'strong',\n",
       " 'sun',\n",
       " 'technology',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'things',\n",
       " 'throughout',\n",
       " 'to',\n",
       " 'topped',\n",
       " 'traveling',\n",
       " 'way',\n",
       " 'we',\n",
       " 'with',\n",
       " 'wonderful',\n",
       " 'world',\n",
       " 'years',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_WORDS == set(sklearn_tfidf.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our dictionaries\n",
    "WORD_TO_INDEX = {}\n",
    "INDEX_TO_WORD = {}\n",
    "\n",
    "for index, word in enumerate(sorted(list(UNIQUE_WORDS))):\n",
    "  WORD_TO_INDEX[word] = index\n",
    "  INDEX_TO_WORD[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activity': 0,\n",
       " 'advancements': 1,\n",
       " 'all': 2,\n",
       " 'allows': 3,\n",
       " 'and': 4,\n",
       " 'are': 5,\n",
       " 'around': 6,\n",
       " 'artistic': 7,\n",
       " 'at': 8,\n",
       " 'be': 9,\n",
       " 'blue': 10,\n",
       " 'books': 11,\n",
       " 'brown': 12,\n",
       " 'but': 13,\n",
       " 'calming': 14,\n",
       " 'can': 15,\n",
       " 'cannot': 16,\n",
       " 'cats': 17,\n",
       " 'challenging': 18,\n",
       " 'cheese': 19,\n",
       " 'coffee': 20,\n",
       " 'comes': 21,\n",
       " 'communicate': 22,\n",
       " 'companionship': 23,\n",
       " 'continues': 24,\n",
       " 'cooking': 25,\n",
       " 'cuisines': 26,\n",
       " 'cultures': 27,\n",
       " 'day': 28,\n",
       " 'different': 29,\n",
       " 'dog': 30,\n",
       " 'dogs': 31,\n",
       " 'education': 32,\n",
       " 'emotions': 33,\n",
       " 'evoke': 34,\n",
       " 'exercise': 35,\n",
       " 'expand': 36,\n",
       " 'experience': 37,\n",
       " 'expression': 38,\n",
       " 'favorite': 39,\n",
       " 'for': 40,\n",
       " 'forms': 41,\n",
       " 'found': 42,\n",
       " 'fox': 43,\n",
       " 'from': 44,\n",
       " 'fuel': 45,\n",
       " 'fun': 46,\n",
       " 'going': 47,\n",
       " 'good': 48,\n",
       " 'great': 49,\n",
       " 'happiness': 50,\n",
       " 'has': 51,\n",
       " 'have': 52,\n",
       " 'health': 53,\n",
       " 'homemade': 54,\n",
       " 'house': 55,\n",
       " 'importance': 56,\n",
       " 'important': 57,\n",
       " 'in': 58,\n",
       " 'incredible': 59,\n",
       " 'internet': 60,\n",
       " 'is': 61,\n",
       " 'jumps': 62,\n",
       " 'keeps': 63,\n",
       " 'knowledge': 64,\n",
       " 'known': 65,\n",
       " 'language': 66,\n",
       " 'lazy': 67,\n",
       " 'learning': 68,\n",
       " 'loyalty': 69,\n",
       " 'made': 70,\n",
       " 'maintaining': 71,\n",
       " 'many': 72,\n",
       " 'meals': 73,\n",
       " 'mind': 74,\n",
       " 'music': 75,\n",
       " 'my': 76,\n",
       " 'nature': 77,\n",
       " 'new': 78,\n",
       " 'of': 79,\n",
       " 'often': 80,\n",
       " 'over': 81,\n",
       " 'overstated': 82,\n",
       " 'pace': 83,\n",
       " 'painting': 84,\n",
       " 'people': 85,\n",
       " 'pepperoni': 86,\n",
       " 'pets': 87,\n",
       " 'pizza': 88,\n",
       " 'played': 89,\n",
       " 'popular': 90,\n",
       " 'power': 91,\n",
       " 'progress': 92,\n",
       " 'quick': 93,\n",
       " 'rapid': 94,\n",
       " 'reading': 95,\n",
       " 'recent': 96,\n",
       " 'revolutionized': 97,\n",
       " 'rewarding': 98,\n",
       " 'science': 99,\n",
       " 'shining': 100,\n",
       " 'simplest': 101,\n",
       " 'sky': 102,\n",
       " 'soccer': 103,\n",
       " 'soothing': 104,\n",
       " 'soul': 105,\n",
       " 'sport': 106,\n",
       " 'strong': 107,\n",
       " 'sun': 108,\n",
       " 'technology': 109,\n",
       " 'that': 110,\n",
       " 'the': 111,\n",
       " 'their': 112,\n",
       " 'things': 113,\n",
       " 'throughout': 114,\n",
       " 'to': 115,\n",
       " 'topped': 116,\n",
       " 'traveling': 117,\n",
       " 'way': 118,\n",
       " 'we': 119,\n",
       " 'with': 120,\n",
       " 'wonderful': 121,\n",
       " 'world': 122,\n",
       " 'years': 123,\n",
       " 'you': 124,\n",
       " 'your': 125}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidf.vocabulary_ == WORD_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'activity',\n",
       " 1: 'advancements',\n",
       " 2: 'all',\n",
       " 3: 'allows',\n",
       " 4: 'and',\n",
       " 5: 'are',\n",
       " 6: 'around',\n",
       " 7: 'artistic',\n",
       " 8: 'at',\n",
       " 9: 'be',\n",
       " 10: 'blue',\n",
       " 11: 'books',\n",
       " 12: 'brown',\n",
       " 13: 'but',\n",
       " 14: 'calming',\n",
       " 15: 'can',\n",
       " 16: 'cannot',\n",
       " 17: 'cats',\n",
       " 18: 'challenging',\n",
       " 19: 'cheese',\n",
       " 20: 'coffee',\n",
       " 21: 'comes',\n",
       " 22: 'communicate',\n",
       " 23: 'companionship',\n",
       " 24: 'continues',\n",
       " 25: 'cooking',\n",
       " 26: 'cuisines',\n",
       " 27: 'cultures',\n",
       " 28: 'day',\n",
       " 29: 'different',\n",
       " 30: 'dog',\n",
       " 31: 'dogs',\n",
       " 32: 'education',\n",
       " 33: 'emotions',\n",
       " 34: 'evoke',\n",
       " 35: 'exercise',\n",
       " 36: 'expand',\n",
       " 37: 'experience',\n",
       " 38: 'expression',\n",
       " 39: 'favorite',\n",
       " 40: 'for',\n",
       " 41: 'forms',\n",
       " 42: 'found',\n",
       " 43: 'fox',\n",
       " 44: 'from',\n",
       " 45: 'fuel',\n",
       " 46: 'fun',\n",
       " 47: 'going',\n",
       " 48: 'good',\n",
       " 49: 'great',\n",
       " 50: 'happiness',\n",
       " 51: 'has',\n",
       " 52: 'have',\n",
       " 53: 'health',\n",
       " 54: 'homemade',\n",
       " 55: 'house',\n",
       " 56: 'importance',\n",
       " 57: 'important',\n",
       " 58: 'in',\n",
       " 59: 'incredible',\n",
       " 60: 'internet',\n",
       " 61: 'is',\n",
       " 62: 'jumps',\n",
       " 63: 'keeps',\n",
       " 64: 'knowledge',\n",
       " 65: 'known',\n",
       " 66: 'language',\n",
       " 67: 'lazy',\n",
       " 68: 'learning',\n",
       " 69: 'loyalty',\n",
       " 70: 'made',\n",
       " 71: 'maintaining',\n",
       " 72: 'many',\n",
       " 73: 'meals',\n",
       " 74: 'mind',\n",
       " 75: 'music',\n",
       " 76: 'my',\n",
       " 77: 'nature',\n",
       " 78: 'new',\n",
       " 79: 'of',\n",
       " 80: 'often',\n",
       " 81: 'over',\n",
       " 82: 'overstated',\n",
       " 83: 'pace',\n",
       " 84: 'painting',\n",
       " 85: 'people',\n",
       " 86: 'pepperoni',\n",
       " 87: 'pets',\n",
       " 88: 'pizza',\n",
       " 89: 'played',\n",
       " 90: 'popular',\n",
       " 91: 'power',\n",
       " 92: 'progress',\n",
       " 93: 'quick',\n",
       " 94: 'rapid',\n",
       " 95: 'reading',\n",
       " 96: 'recent',\n",
       " 97: 'revolutionized',\n",
       " 98: 'rewarding',\n",
       " 99: 'science',\n",
       " 100: 'shining',\n",
       " 101: 'simplest',\n",
       " 102: 'sky',\n",
       " 103: 'soccer',\n",
       " 104: 'soothing',\n",
       " 105: 'soul',\n",
       " 106: 'sport',\n",
       " 107: 'strong',\n",
       " 108: 'sun',\n",
       " 109: 'technology',\n",
       " 110: 'that',\n",
       " 111: 'the',\n",
       " 112: 'their',\n",
       " 113: 'things',\n",
       " 114: 'throughout',\n",
       " 115: 'to',\n",
       " 116: 'topped',\n",
       " 117: 'traveling',\n",
       " 118: 'way',\n",
       " 119: 'we',\n",
       " 120: 'with',\n",
       " 121: 'wonderful',\n",
       " 122: 'world',\n",
       " 123: 'years',\n",
       " 124: 'you',\n",
       " 125: 'your'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_TO_WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# The three axes of our matrix\n",
    "row = []\n",
    "column = []\n",
    "value = []\n",
    "\n",
    "for index, sample in enumerate(preprocess_train):\n",
    "\n",
    "  # Get the count of every word in each sample\n",
    "  word_count = dict(Counter(sample.split(' ')))\n",
    "\n",
    "  for word, count in word_count.items():\n",
    "    # Finding the vocab index of each word in the sample\n",
    "    # This will represent the column to add the count to\n",
    "    vocab_index = WORD_TO_INDEX.get(word)\n",
    "\n",
    "    # When word is found in vocabulary\n",
    "    if vocab_index is not None:\n",
    "      # Determines what's added to the matrix\n",
    "      # Let the matrix shape be (x, y)\n",
    "      # At a certain index common to all\n",
    "      # it will add the sample index to the row (x)\n",
    "      # then it will add which column the value will be (y)\n",
    "      # And it tells what is the value, the count, to be added at position (x, y)\n",
    "      # when we append value\n",
    "      row.append(index)\n",
    "      column.append(vocab_index)\n",
    "      value.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY = csr_matrix(\n",
    "  (value, (row, column)),\n",
    "  shape=(len(preprocess_train), len(WORD_TO_INDEX))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 62)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 93)\t1\n",
      "  (0, 111)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 61)\t2\n",
      "  (1, 100)\t1\n",
      "  (1, 102)\t1\n",
      "  (1, 108)\t1\n",
      "  (1, 111)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 52)\t1\n",
      "  (2, 55)\t1\n",
      "  (2, 87)\t1\n",
      "  (2, 111)\t1\n",
      "  (2, 115)\t1\n",
      "  (2, 121)\t1\n",
      "  (3, 2)\t1\n",
      "  :\t:\n",
      "  (17, 77)\t1\n",
      "  (17, 79)\t1\n",
      "  (17, 104)\t1\n",
      "  (17, 105)\t1\n",
      "  (17, 111)\t2\n",
      "  (17, 118)\t1\n",
      "  (18, 7)\t1\n",
      "  (18, 21)\t1\n",
      "  (18, 38)\t1\n",
      "  (18, 41)\t1\n",
      "  (18, 44)\t1\n",
      "  (18, 58)\t1\n",
      "  (18, 72)\t1\n",
      "  (18, 75)\t1\n",
      "  (18, 84)\t1\n",
      "  (18, 115)\t1\n",
      "  (19, 42)\t1\n",
      "  (19, 50)\t1\n",
      "  (19, 58)\t1\n",
      "  (19, 61)\t1\n",
      "  (19, 79)\t1\n",
      "  (19, 80)\t1\n",
      "  (19, 101)\t1\n",
      "  (19, 111)\t1\n",
      "  (19, 113)\t1\n"
     ]
    }
   ],
   "source": [
    "print(VOCABULARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF(t) = (number of times term t appears in a document) / (total number of terms in a document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of times a term at a given vocab index\n",
    "# occurred in the input\n",
    "DOCUMENT_FREQUENCY = np.bincount(\n",
    "  VOCABULARY.indices,\n",
    "  minlength=VOCABULARY.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  6,  2,  2,  1,  1,  3,  1,  1,  1,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        4,  1,  1,  1,  1,  1,  1,  4,  1,  1,  7,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  2,  1,  1,  2,  1,  1,  1,  3,  1,  1,  1,  1,  1,\n",
       "        2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, 10,  1,  1,  1,  6,  1,  1,  3,\n",
       "        1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proof of doc frequency\n",
    "\n",
    "print(DOCUMENT_FREQUENCY[4]) # -> 6\n",
    "print(INDEX_TO_WORD[4]) # -> and\n",
    "\n",
    "word_count = 0\n",
    "for text in preprocess_train:\n",
    "  for term in text.split():\n",
    "    if term == INDEX_TO_WORD[4]:\n",
    "      word_count += 1\n",
    "word_count == DOCUMENT_FREQUENCY[4] # -> True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth document frequency by adding 1 to doc freq\n",
    "# Prevents zero division\n",
    "# Note that this can be seen in index 0, for example\n",
    "\n",
    "NUM_OF_SAMPLES, NUM_OF_TERMS = VOCABULARY.shape\n",
    "\n",
    "DOCUMENT_FREQUENCY_SMOOTH = DOCUMENT_FREQUENCY + 1\n",
    "NUM_OF_SAMPLES_SMOOTH = NUM_OF_SAMPLES + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  7,  3,  3,  2,  2,  4,  2,  2,  2,  2,  2,  3,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        5,  2,  2,  2,  2,  2,  2,  5,  2,  2,  8,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  3,  2,  2,  3,  2,  2,  2,  4,  2,  2,  2,  2,  2,\n",
       "        3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2, 11,  2,  2,  2,  7,  2,  2,  4,\n",
       "        2,  2,  2,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCY_SMOOTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF(t) = log(total number of documents) / (number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = np.log(NUM_OF_SAMPLES_SMOOTH / DOCUMENT_FREQUENCY_SMOOTH) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF == sklearn_tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "IDF_SPARSE_MATRIX = sp.diags(\n",
    "  IDF,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_RESULT = VOCABULARY * IDF_SPARSE_MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 111)\t3.2932543298501047\n",
      "  (0, 93)\t3.3513752571634776\n",
      "  (0, 81)\t3.3513752571634776\n",
      "  (0, 67)\t3.3513752571634776\n",
      "  (0, 62)\t3.3513752571634776\n",
      "  (0, 43)\t3.3513752571634776\n",
      "  (0, 30)\t3.3513752571634776\n",
      "  (0, 12)\t3.3513752571634776\n",
      "  (1, 111)\t3.2932543298501047\n",
      "  (1, 108)\t3.3513752571634776\n",
      "  (1, 102)\t3.3513752571634776\n",
      "  (1, 100)\t3.3513752571634776\n",
      "  (1, 61)\t3.930161792087174\n",
      "  (1, 10)\t3.3513752571634776\n",
      "  (1, 4)\t2.09861228866811\n",
      "  (2, 121)\t3.3513752571634776\n",
      "  (2, 115)\t2.09861228866811\n",
      "  (2, 111)\t1.6466271649250523\n",
      "  (2, 87)\t3.3513752571634776\n",
      "  (2, 55)\t3.3513752571634776\n",
      "  (2, 52)\t3.3513752571634776\n",
      "  (2, 17)\t3.3513752571634776\n",
      "  (2, 6)\t2.9459101490553135\n",
      "  (2, 5)\t2.9459101490553135\n",
      "  (3, 122)\t3.3513752571634776\n",
      "  :\t:\n",
      "  (17, 79)\t2.6582280766035327\n",
      "  (17, 77)\t3.3513752571634776\n",
      "  (17, 74)\t3.3513752571634776\n",
      "  (17, 51)\t2.435084525289323\n",
      "  (17, 14)\t3.3513752571634776\n",
      "  (17, 4)\t2.09861228866811\n",
      "  (18, 115)\t2.09861228866811\n",
      "  (18, 84)\t3.3513752571634776\n",
      "  (18, 75)\t2.9459101490553135\n",
      "  (18, 72)\t2.9459101490553135\n",
      "  (18, 58)\t2.435084525289323\n",
      "  (18, 44)\t3.3513752571634776\n",
      "  (18, 41)\t3.3513752571634776\n",
      "  (18, 38)\t3.3513752571634776\n",
      "  (18, 21)\t3.3513752571634776\n",
      "  (18, 7)\t3.3513752571634776\n",
      "  (19, 113)\t3.3513752571634776\n",
      "  (19, 111)\t1.6466271649250523\n",
      "  (19, 101)\t3.3513752571634776\n",
      "  (19, 80)\t3.3513752571634776\n",
      "  (19, 79)\t2.6582280766035327\n",
      "  (19, 61)\t1.965080896043587\n",
      "  (19, 58)\t2.435084525289323\n",
      "  (19, 50)\t3.3513752571634776\n",
      "  (19, 42)\t3.3513752571634776\n"
     ]
    }
   ],
   "source": [
    "print(TFIDF_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(TFIDF_RESULT.toarray(), sklearn_result.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "sklearn_nb = BernoulliNB()\n",
    "sklearn_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_nb.fit(TFIDF_RESULT, SAMPLE_TRAIN_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_nb.predict(SAMPLE_TEST_TRANSFORMED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class CustomBernoulliNB():\n",
    "  def __init__(self, alpha=1.0):\n",
    "    # Smoothing parameter\n",
    "    # Used to prevent features from being totally 0 when getting feature counts\n",
    "    # Essentially tells NB that there's an extra count to all features\n",
    "    # which would otherwise zero things when in the case that a feature\n",
    "    # when predicting was not learned from fitting\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    if X.shape[0] != len(y):\n",
    "      print(\"ERROR: X and y have different lengths\")\n",
    "      return\n",
    "\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    X = X.toarray()\n",
    "\n",
    "    # Turn counts into binary values\n",
    "    # NOTE: This is the assumption of Bernoulli NB\n",
    "    X = np.where(X != 0, 1, 0)\n",
    "\n",
    "    # Group X by class\n",
    "    # Use a default dict for this. This automatically makes a list\n",
    "    # when it encounters a key that is unknown. So we can iterate over\n",
    "    # each X and y, and append to the list of each respective class\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for X_sample, y_sample in zip(X, y):\n",
    "      grouped[y_sample].append(X_sample)\n",
    "\n",
    "    # Fixes bug where classes were inverted\n",
    "    # in the event that the first y sample\n",
    "    # is 1 instead of 0\n",
    "    ordered_keys = sorted(grouped.keys())\n",
    "\n",
    "    # STEP 1: Getting prior log probability of each class\n",
    "    # Essentially, the (log) probability of class being 0 or 1 based on train set\n",
    "    # Log scale is used to prevent underflow due to lack of floating point precision\n",
    "    # Equation: Number of samples in class / number of overall samples\n",
    "    # If there are 10 samples, 4 are 0 and 6 are 1\n",
    "    # The equation then looks like\n",
    "    # np.log(4/10) for 0\n",
    "    # np.log(6/10) for 1\n",
    "    self.class_log_prior_ = [\n",
    "      np.log(len(grouped[data_class]) / num_of_samples)\n",
    "      for data_class\n",
    "      in ordered_keys\n",
    "    ]\n",
    "    # Output: array([-0.70126153, -0.68509814])\n",
    "    # These are log probabilities. To reverse this, we can use np.exp()\n",
    "    # and the result after np.exp() is\n",
    "    # Output: array([0.49595924, 0.50404076])\n",
    "    # This is the same as:\n",
    "    # 11292 / (11476 + 11292)\n",
    "    # 11476 / (11476 + 11292)\n",
    "    # We will consistently use these log probabilities elsewhere in calculations\n",
    "\n",
    "    # STEP 2: Count each word in each class. Add smoothing parameter (self.alpha)\n",
    "    # The smoothing parameter is important to prevent divison by 0\n",
    "    # Make a list so it's easy to append to, numpy arrays are annoying to make 2D arrays\n",
    "    # from several 1D arrays\n",
    "    word_count_per_class = []\n",
    "\n",
    "    # Iterate over each class (0, then 1)\n",
    "    for data_class in ordered_keys:\n",
    "      # Sum the count of every word occurrence by column, going downward\n",
    "      # So if the array looks something like\n",
    "      # [[1 2 3]\n",
    "      #  [2 3 4]\n",
    "      #  [3 4 5]]\n",
    "      # sum(axis=0) will turn this into\n",
    "      # [6, 9, 12]\n",
    "      count_in_class = np.array(grouped[data_class]).sum(axis=0)\n",
    "\n",
    "      word_count_per_class.append(count_in_class)\n",
    "    # Make a final array by joining together the 2 lists to make it 2D\n",
    "    # then we add the alpha value to everything\n",
    "    # This alpha is important to prevent division by 0. You can observe this\n",
    "    # by setting the alpha to 0. You will get a lot of nan values\n",
    "    word_count_per_class = np.array(word_count_per_class) + self.alpha\n",
    "\n",
    "    # Reshape to make it a 2D array where the sum corresponds to the row of a class\n",
    "    # [[223218], -> 0\n",
    "    #  [225122]] -> 1\n",
    "    sum_of_words_in_each_class = word_count_per_class.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # STEP 3: Calculate log probability of each word\n",
    "    # NOTE: Not actually used in BNB\n",
    "    self.feature_log_prob_ = np.log(word_count_per_class / sum_of_words_in_each_class)\n",
    "\n",
    "    # STEP 4: BNB\n",
    "    # Smoothing parameter for classes\n",
    "    # Multiplied by 2 because of 2 classes, 0 and 1\n",
    "    smoothing = 2 * self.alpha\n",
    "    \n",
    "    # Add the smoothing to number of documents\n",
    "    num_of_documents_with_smoothing = np.array([\n",
    "      len(grouped[data_class]) + smoothing\n",
    "      for data_class \n",
    "      in ordered_keys\n",
    "    ])\n",
    "\n",
    "    # STEP 5: Feature Probabilities\n",
    "    # (Word count in class [0 or 1] + alpha) / (value count [11292, 11476] + alpha * 2)\n",
    "    # Given an alpha of 1,\n",
    "    # If you plug in 1.77085178e-04 * 11294, you get 1.999~. This means the original word\n",
    "    # appeared once (2 - alpha = 1) in the 0 class\n",
    "    self.feature_prob_ = word_count_per_class / num_of_documents_with_smoothing.reshape(-1, 1)\n",
    "\n",
    "    return self\n",
    "  \n",
    "  def predict_log_proba(self, X):\n",
    "    \"\"\"\n",
    "    This function gets the numerator part of Bernoulli NB\n",
    "    Read carefully\n",
    "    \"\"\"\n",
    "    X = X.toarray()\n",
    "    # Turn counts into binary values\n",
    "    X = np.where(X != 0, 1, 0)\n",
    "\n",
    "    samples = []\n",
    "    for sample in X:\n",
    "      # This part is solving for the numerator\n",
    "      # For each sample of data,\n",
    "      # Sum the log probabilities of all the features that appear in the class\n",
    "      # and add that to the log probabilities that the features do not appear in the class\n",
    "      # then add once again to the log probability of the class (this is the class prior)\n",
    "      negative_prob = 1 - self.feature_prob_\n",
    "\n",
    "      # Invert from 0 to 1, or 1 to 0\n",
    "      inverted_sample = np.abs(sample - 1)\n",
    "\n",
    "      # We multiply feature prob to sample to essentially\n",
    "      # zero out all features that do not appear\n",
    "      # This leaves us with the features that appear as the only nonzero values\n",
    "      # so we can simply add them; this is the same as\n",
    "      # multiplying all the probabilities of all the features\n",
    "      # but we can just add because we use the log scale\n",
    "      # The inverted probabilities are based on the Bernoulli rule\n",
    "      # P(x_i∣y) = P(i∣y)x_i + (1 − P(i∣y)) * (1 − x_i)\n",
    "      # sample probabilities = P(i∣y)x_i\n",
    "      # and the inverted probabilities represent the \n",
    "      # (1 − P(i∣y)) * (1 − x_i)\n",
    "      # portion\n",
    "      sample_probabilities = np.log(self.feature_prob_) * sample\n",
    "      inverted_probabilities = np.log(negative_prob) * inverted_sample\n",
    "\n",
    "      log_sum_of_probabilities = (sample_probabilities + inverted_probabilities).sum(axis=1)\n",
    "      feature_probabilities_with_class_prior = log_sum_of_probabilities + self.class_log_prior_\n",
    "      samples.append(feature_probabilities_with_class_prior)\n",
    "    return np.array(samples)\n",
    "    \n",
    "  def predict_proba(self, X):\n",
    "    # Get the numerator part\n",
    "    outputs = self.predict_log_proba(X)\n",
    "    \n",
    "    results = []\n",
    "    for data_class in outputs:\n",
    "      # The following two lines are a manual implementation\n",
    "      # of the logsumexp function\n",
    "      # The main purpose is to prevent numerical underflow\n",
    "      # in the denominator due to the need for summation in the formula\n",
    "      # Reference: https://stats.stackexchange.com/questions/105602/example-of-how-the-log-sum-exp-trick-works-in-naive-bayes\n",
    "      output_max = data_class.max()\n",
    "      logsumexp_result = output_max + np.log(np.sum(np.exp(data_class - output_max)))\n",
    "\n",
    "      results.append(logsumexp_result)\n",
    "    results = np.array(results).reshape(-1, 1)\n",
    "\n",
    "    return np.exp(outputs - results)\n",
    "\n",
    "  def predict(self, X):\n",
    "    outputs = self.predict_log_proba(X)\n",
    "\n",
    "    return np.argmax(outputs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nb = CustomBernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomBernoulliNB at 0x7f25f5ef5220>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_nb.fit(TFIDF_RESULT, SAMPLE_TRAIN_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(custom_nb.predict(SAMPLE_TEST_TRANSFORMED), sklearn_nb.predict(SAMPLE_TEST_TRANSFORMED))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
