{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TRAIN = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The sky is blue and the sun is shining.\",\n",
    "    \"Cats are wonderful pets to have around the house.\",\n",
    "    \"Soccer is a popular sport played all around the world.\",\n",
    "    \"Pizza topped with cheese and pepperoni is my favorite.\",\n",
    "    \"Learning a new language can be challenging but rewarding.\",\n",
    "    \"Reading books is a great way to expand your knowledge.\",\n",
    "    \"The internet has revolutionized the way we communicate.\",\n",
    "    \"Music has the power to evoke strong emotions in people.\",\n",
    "    \"Exercise is important for maintaining good health.\",\n",
    "    \"Science has made incredible advancements in recent years.\",\n",
    "    \"Coffee is the fuel that keeps many people going throughout the day.\",\n",
    "    \"Traveling allows you to experience different cultures and cuisines.\",\n",
    "    \"The importance of education cannot be overstated.\",\n",
    "    \"Technology continues to progress at a rapid pace.\",\n",
    "    \"Dogs are known for their loyalty and companionship.\",\n",
    "    \"Cooking homemade meals can be a fun and rewarding activity.\",\n",
    "    \"Nature has a way of calming the mind and soothing the soul.\",\n",
    "    \"Artistic expression comes in many forms, from painting to music.\",\n",
    "    \"Happiness is often found in the simplest of things.\"\n",
    "]\n",
    "\n",
    "SAMPLE_TEST = [\n",
    "    \"TF-IDF, short for term frequency-inverse document frequency, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents or a corpus.\",\n",
    "    \"The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word.\",\n",
    "    \"TF-IDF is commonly used in information retrieval and text mining as a weighting factor for scoring the relevance of a word to a document in a corpus.\",\n",
    "    \"The intuition behind TF-IDF is that words that occur frequently in a document but rarely in the corpus are more important in describing the content of the document.\",\n",
    "    \"In contrast, words that occur frequently in both the document and the corpus are less important as they are likely to be common words with less discriminative power.\",\n",
    "    \"To calculate the TF-IDF score for a word in a document, you multiply the term frequency (TF) by the inverse document frequency (IDF).\",\n",
    "    \"TF is the frequency of a term in a document, while IDF is the logarithmically scaled inverse fraction of the documents that contain the word.\",\n",
    "    \"By weighting the TF by IDF, the TF-IDF score penalizes words that are common across documents and emphasizes words that are unique to a specific document.\",\n",
    "    \"Once you have computed the TF-IDF scores for all words in a document, you can represent the document as a vector where each dimension corresponds to a word and the value corresponds to its TF-IDF score.\",\n",
    "    \"TF-IDF vectorization is a fundamental step in many natural language processing tasks, such as document classification, clustering, and information retrieval.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30116/136893857.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer()\n",
    "\n",
    "sklearn_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 111)\t2.0\n",
      "  (0, 93)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 43)\t1.0\n",
      "  (0, 62)\t1.0\n",
      "  (0, 81)\t1.0\n",
      "  (0, 67)\t1.0\n",
      "  (0, 30)\t1.0\n",
      "  (1, 111)\t2.0\n",
      "  (1, 102)\t1.0\n",
      "  (1, 61)\t2.0\n",
      "  (1, 10)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 108)\t1.0\n",
      "  (1, 100)\t1.0\n",
      "  (2, 111)\t1.0\n",
      "  (2, 17)\t1.0\n",
      "  (2, 5)\t1.0\n",
      "  (2, 121)\t1.0\n",
      "  (2, 87)\t1.0\n",
      "  (2, 115)\t1.0\n",
      "  (2, 52)\t1.0\n",
      "  (2, 6)\t1.0\n",
      "  (2, 55)\t1.0\n",
      "  (3, 111)\t1.0\n",
      "  :\t:\n",
      "  (17, 79)\t1.0\n",
      "  (17, 77)\t1.0\n",
      "  (17, 14)\t1.0\n",
      "  (17, 74)\t1.0\n",
      "  (17, 104)\t1.0\n",
      "  (17, 105)\t1.0\n",
      "  (18, 115)\t1.0\n",
      "  (18, 75)\t1.0\n",
      "  (18, 58)\t1.0\n",
      "  (18, 72)\t1.0\n",
      "  (18, 7)\t1.0\n",
      "  (18, 38)\t1.0\n",
      "  (18, 21)\t1.0\n",
      "  (18, 41)\t1.0\n",
      "  (18, 44)\t1.0\n",
      "  (18, 84)\t1.0\n",
      "  (19, 111)\t1.0\n",
      "  (19, 61)\t1.0\n",
      "  (19, 58)\t1.0\n",
      "  (19, 79)\t1.0\n",
      "  (19, 50)\t1.0\n",
      "  (19, 80)\t1.0\n",
      "  (19, 42)\t1.0\n",
      "  (19, 101)\t1.0\n",
      "  (19, 113)\t1.0\n"
     ]
    }
   ],
   "source": [
    "sklearn_result = sklearn_tfidf.fit_transform(SAMPLE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 30)\t0.35431563620454354\n",
      "  (0, 67)\t0.35431563620454354\n",
      "  (0, 81)\t0.35431563620454354\n",
      "  (0, 62)\t0.35431563620454354\n",
      "  (0, 43)\t0.35431563620454354\n",
      "  (0, 12)\t0.35431563620454354\n",
      "  (0, 93)\t0.35431563620454354\n",
      "  (0, 111)\t0.3481709487978384\n",
      "  (1, 100)\t0.38538683279827396\n",
      "  (1, 108)\t0.38538683279827396\n",
      "  (1, 4)\t0.2413270616212252\n",
      "  (1, 10)\t0.38538683279827396\n",
      "  (1, 61)\t0.45194360201823913\n",
      "  (1, 102)\t0.38538683279827396\n",
      "  (1, 111)\t0.3787032959282317\n",
      "  (2, 55)\t0.37322633729126187\n",
      "  (2, 6)\t0.3280716632883578\n",
      "  (2, 52)\t0.37322633729126187\n",
      "  (2, 115)\t0.23371222790400412\n",
      "  (2, 87)\t0.37322633729126187\n",
      "  (2, 121)\t0.37322633729126187\n",
      "  (2, 5)\t0.3280716632883578\n",
      "  (2, 17)\t0.37322633729126187\n",
      "  (2, 111)\t0.18337684636647475\n",
      "  (3, 122)\t0.36865801601525017\n",
      "  :\t:\n",
      "  (17, 77)\t0.35041531879418736\n",
      "  (17, 79)\t0.2779407757754563\n",
      "  (17, 51)\t0.254609184213604\n",
      "  (17, 118)\t0.2779407757754563\n",
      "  (17, 4)\t0.2194280967454082\n",
      "  (17, 111)\t0.34433827229526226\n",
      "  (18, 84)\t0.34369757070594353\n",
      "  (18, 44)\t0.34369757070594353\n",
      "  (18, 41)\t0.34369757070594353\n",
      "  (18, 21)\t0.34369757070594353\n",
      "  (18, 38)\t0.34369757070594353\n",
      "  (18, 7)\t0.34369757070594353\n",
      "  (18, 72)\t0.3021154254761827\n",
      "  (18, 58)\t0.24972811803651476\n",
      "  (18, 75)\t0.3021154254761827\n",
      "  (18, 115)\t0.2152214807718519\n",
      "  (19, 113)\t0.3851206199002829\n",
      "  (19, 101)\t0.3851206199002829\n",
      "  (19, 42)\t0.3851206199002829\n",
      "  (19, 80)\t0.3851206199002829\n",
      "  (19, 50)\t0.3851206199002829\n",
      "  (19, 79)\t0.30546816340834254\n",
      "  (19, 58)\t0.2798257998367937\n",
      "  (19, 61)\t0.2258157069164021\n",
      "  (19, 111)\t0.18922084990187302\n"
     ]
    }
   ],
   "source": [
    "print(sklearn_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sklearn_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'sky', 'is', 'blue', 'and', 'sun', 'shining', 'cats', 'are', 'wonderful', 'pets', 'to', 'have', 'around', 'house', 'soccer', 'popular', 'sport', 'played', 'all', 'world', 'pizza', 'topped', 'with', 'cheese', 'pepperoni', 'my', 'favorite', 'learning', 'new', 'language', 'can', 'be', 'challenging', 'but', 'rewarding', 'reading', 'books', 'great', 'way', 'expand', 'your', 'knowledge', 'internet', 'has', 'revolutionized', 'we', 'communicate', 'music', 'power', 'evoke', 'strong', 'emotions', 'in', 'people', 'exercise', 'important', 'for', 'maintaining', 'good', 'health', 'science', 'made', 'incredible', 'advancements', 'recent', 'years', 'coffee', 'fuel', 'that', 'keeps', 'many', 'going', 'throughout', 'day', 'traveling', 'allows', 'you', 'experience', 'different', 'cultures', 'cuisines', 'importance', 'of', 'education', 'cannot', 'overstated', 'technology', 'continues', 'progress', 'at', 'rapid', 'pace', 'dogs', 'known', 'their', 'loyalty', 'companionship', 'cooking', 'homemade', 'meals', 'fun', 'activity', 'nature', 'calming', 'mind', 'soothing', 'soul', 'artistic', 'expression', 'comes', 'forms', 'from', 'painting', 'happiness', 'often', 'found', 'simplest', 'things'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lazy</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jumps</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fox</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brown</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quick</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text  index\n",
       "0    dog     30\n",
       "1   lazy     67\n",
       "2   over     81\n",
       "3  jumps     62\n",
       "4    fox     43\n",
       "5  brown     12\n",
       "6  quick     93\n",
       "7    the    111"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = sklearn_tfidf.get_feature_names_out()\n",
    "sentence_indices = [30, 67, 81, 62, 43, 12, 93, 111]\n",
    "vocabulary[[sentence_indices]]\n",
    "pd.DataFrame({\n",
    "  'text': vocabulary[sentence_indices],\n",
    "  'index': sentence_indices,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the quick brown fox jumps over the lazy dog', 'the sky is blue and the sun is shining', 'cats are wonderful pets to have around the house', 'soccer is a popular sport played all around the world', 'pizza topped with cheese and pepperoni is my favorite', 'learning a new language can be challenging but rewarding', 'reading books is a great way to expand your knowledge', 'the internet has revolutionized the way we communicate', 'music has the power to evoke strong emotions in people', 'exercise is important for maintaining good health', 'science has made incredible advancements in recent years', 'coffee is the fuel that keeps many people going throughout the day', 'traveling allows you to experience different cultures and cuisines', 'the importance of education cannot be overstated', 'technology continues to progress at a rapid pace', 'dogs are known for their loyalty and companionship', 'cooking homemade meals can be a fun and rewarding activity', 'nature has a way of calming the mind and soothing the soul', 'artistic expression comes in many forms from painting to music', 'happiness is often found in the simplest of things']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Lowercase text\n",
    "preprocess_train = [\n",
    "  text.lower()\n",
    "  for text \n",
    "  in SAMPLE_TRAIN\n",
    "]\n",
    "\n",
    "# Remove punctuation\n",
    "preprocess_train = [\n",
    "  re.sub(r'[^\\w\\s]', '', text)\n",
    "  for text\n",
    "  in preprocess_train\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique word to set\n",
    "UNIQUE_WORDS = set()\n",
    "\n",
    "for sample in preprocess_train:\n",
    "  for word in sample.split(' '):\n",
    "    # Only add unique words of 2 or more characters\n",
    "    # SKLearn instead uses this regex pattern when\n",
    "    # processing text:\n",
    "    # r”(?u)\\b\\w\\w+\\b”\n",
    "    if len(word) < 2: continue\n",
    "    UNIQUE_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activity',\n",
       " 'advancements',\n",
       " 'all',\n",
       " 'allows',\n",
       " 'and',\n",
       " 'are',\n",
       " 'around',\n",
       " 'artistic',\n",
       " 'at',\n",
       " 'be',\n",
       " 'blue',\n",
       " 'books',\n",
       " 'brown',\n",
       " 'but',\n",
       " 'calming',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cats',\n",
       " 'challenging',\n",
       " 'cheese',\n",
       " 'coffee',\n",
       " 'comes',\n",
       " 'communicate',\n",
       " 'companionship',\n",
       " 'continues',\n",
       " 'cooking',\n",
       " 'cuisines',\n",
       " 'cultures',\n",
       " 'day',\n",
       " 'different',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'education',\n",
       " 'emotions',\n",
       " 'evoke',\n",
       " 'exercise',\n",
       " 'expand',\n",
       " 'experience',\n",
       " 'expression',\n",
       " 'favorite',\n",
       " 'for',\n",
       " 'forms',\n",
       " 'found',\n",
       " 'fox',\n",
       " 'from',\n",
       " 'fuel',\n",
       " 'fun',\n",
       " 'going',\n",
       " 'good',\n",
       " 'great',\n",
       " 'happiness',\n",
       " 'has',\n",
       " 'have',\n",
       " 'health',\n",
       " 'homemade',\n",
       " 'house',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'in',\n",
       " 'incredible',\n",
       " 'internet',\n",
       " 'is',\n",
       " 'jumps',\n",
       " 'keeps',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'language',\n",
       " 'lazy',\n",
       " 'learning',\n",
       " 'loyalty',\n",
       " 'made',\n",
       " 'maintaining',\n",
       " 'many',\n",
       " 'meals',\n",
       " 'mind',\n",
       " 'music',\n",
       " 'my',\n",
       " 'nature',\n",
       " 'new',\n",
       " 'of',\n",
       " 'often',\n",
       " 'over',\n",
       " 'overstated',\n",
       " 'pace',\n",
       " 'painting',\n",
       " 'people',\n",
       " 'pepperoni',\n",
       " 'pets',\n",
       " 'pizza',\n",
       " 'played',\n",
       " 'popular',\n",
       " 'power',\n",
       " 'progress',\n",
       " 'quick',\n",
       " 'rapid',\n",
       " 'reading',\n",
       " 'recent',\n",
       " 'revolutionized',\n",
       " 'rewarding',\n",
       " 'science',\n",
       " 'shining',\n",
       " 'simplest',\n",
       " 'sky',\n",
       " 'soccer',\n",
       " 'soothing',\n",
       " 'soul',\n",
       " 'sport',\n",
       " 'strong',\n",
       " 'sun',\n",
       " 'technology',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'things',\n",
       " 'throughout',\n",
       " 'to',\n",
       " 'topped',\n",
       " 'traveling',\n",
       " 'way',\n",
       " 'we',\n",
       " 'with',\n",
       " 'wonderful',\n",
       " 'world',\n",
       " 'years',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "for text in UNIQUE_WORDS:\n",
    "  if text not in sklearn_tfidf.vocabulary_.keys():\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(UNIQUE_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency(t) = (number of times term t appears in a document) / (total number of terms in a document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        for doc in raw_documents:\n",
    "            feature_counter = {}\n",
    "            for feature in analyze(doc):\n",
    "                try:\n",
    "                    feature_idx = vocabulary[feature]\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                except KeyError:\n",
    "                    # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                    continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\n",
    "                    \"empty vocabulary; perhaps the documents only contain stop words\"\n",
    "                )\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "            if _IS_32BIT:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"sparse CSR array has {} non-zero \"\n",
    "                        \"elements and requires 64 bit indexing, \"\n",
    "                        \"which is unsupported with 32 bit Python.\"\n",
    "                    ).format(indptr[-1])\n",
    "                )\n",
    "            indices_dtype = np.int64\n",
    "\n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix(\n",
    "            (values, j_indices, indptr),\n",
    "            shape=(len(indptr) - 1, len(vocabulary)),\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        for doc in raw_documents:\n",
    "            feature_counter = {}\n",
    "            for feature in analyze(doc):\n",
    "                try:\n",
    "                    feature_idx = vocabulary[feature]\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                except KeyError:\n",
    "                    # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                    continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\n",
    "                    \"empty vocabulary; perhaps the documents only contain stop words\"\n",
    "                )\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "            if _IS_32BIT:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"sparse CSR array has {} non-zero \"\n",
    "                        \"elements and requires 64 bit indexing, \"\n",
    "                        \"which is unsupported with 32 bit Python.\"\n",
    "                    ).format(indptr[-1])\n",
    "                )\n",
    "            indices_dtype = np.int64\n",
    "\n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix(\n",
    "            (values, j_indices, indptr),\n",
    "            shape=(len(indptr) - 1, len(vocabulary)),\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        for doc in raw_documents:\n",
    "            feature_counter = {}\n",
    "            for feature in analyze(doc):\n",
    "                try:\n",
    "                    feature_idx = vocabulary[feature]\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                except KeyError:\n",
    "                    # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                    continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\n",
    "                    \"empty vocabulary; perhaps the documents only contain stop words\"\n",
    "                )\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "            if _IS_32BIT:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"sparse CSR array has {} non-zero \"\n",
    "                        \"elements and requires 64 bit indexing, \"\n",
    "                        \"which is unsupported with 32 bit Python.\"\n",
    "                    ).format(indptr[-1])\n",
    "                )\n",
    "            indices_dtype = np.int64\n",
    "\n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix(\n",
    "            (values, j_indices, indptr),\n",
    "            shape=(len(indptr) - 1, len(vocabulary)),\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
