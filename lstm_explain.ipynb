{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CalamanCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calamancy vectors are trained from a skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37046/3484305165.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./misc/stopwords.txt') as f:\n",
    "  stopwords = f.read().replace('\\n', ' ').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./misc/training_text.txt') as f:\n",
    "  text = f.read().replace('\\n', '')\n",
    "  text = text.translate(\n",
    "    str.maketrans('', '', string.punctuation)\n",
    "  )\n",
    "  text = ''.join([t for t in text if t not in '0123456789'])\n",
    "  text = text.lower().split()\n",
    "\n",
    "text = [w for w in text if w not in stopwords][:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'learning',\n",
       " 'fundamentals',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'hot',\n",
       " 'growing',\n",
       " 'fields',\n",
       " 'alternative',\n",
       " 'names',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'big',\n",
       " 'data',\n",
       " 'etc',\n",
       " 'im',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'talk',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'long',\n",
       " 'passions',\n",
       " 'mine',\n",
       " 'didnt',\n",
       " 'used',\n",
       " 'good',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'studying',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'long',\n",
       " 'time',\n",
       " 'got',\n",
       " 'better',\n",
       " 'better',\n",
       " 'became',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'expert',\n",
       " 'im',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'talk',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'thanks',\n",
       " 'listening',\n",
       " 'talk',\n",
       " 'data',\n",
       " 'science',\n",
       " 'statistics']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce negative samples so model knows what are\n",
    "# context words as well as what are not context words\n",
    "\n",
    "WINDOW_SIZE = 3\n",
    "NUM_NEGATIVE_SAMPLES = 3\n",
    "\n",
    "data = []\n",
    "\n",
    "# Iterate over all words\n",
    "for index, center_word in enumerate(\n",
    "  text[WINDOW_SIZE - 1 : -WINDOW_SIZE]\n",
    "):\n",
    "  # Iterate over context words around center word\n",
    "  context_words = [\n",
    "    context_word \n",
    "    for context_word \n",
    "    in text[index : index + 2 * WINDOW_SIZE-1] \n",
    "    if context_word != center_word\n",
    "  ]\n",
    "\n",
    "  # Get words not in context as negative samples\n",
    "  for context_word in context_words:\n",
    "    data.append([center_word, context_word, 1])\n",
    "    negative_samples = np.random.choice([\n",
    "      w \n",
    "      for w\n",
    "      in text[WINDOW_SIZE - 1 : -WINDOW_SIZE]\n",
    "      if w != center_word\n",
    "      and w not in context_words\n",
    "    ], NUM_NEGATIVE_SAMPLES)\n",
    "\n",
    "    for negative_sample in negative_samples:\n",
    "      data.append([center_word, negative_sample, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fundamentals</td>\n",
       "      <td>today</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fundamentals</td>\n",
       "      <td>long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fundamentals</td>\n",
       "      <td>growing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fundamentals</td>\n",
       "      <td>etc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fundamentals</td>\n",
       "      <td>learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>talk</td>\n",
       "      <td>better</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>talk</td>\n",
       "      <td>science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>talk</td>\n",
       "      <td>got</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>talk</td>\n",
       "      <td>im</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>talk</td>\n",
       "      <td>mine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1  2\n",
       "0    fundamentals     today  1\n",
       "1    fundamentals      long  0\n",
       "2    fundamentals   growing  0\n",
       "3    fundamentals       etc  0\n",
       "4    fundamentals  learning  1\n",
       "..            ...       ... ..\n",
       "979          talk    better  0\n",
       "980          talk   science  1\n",
       "981          talk       got  0\n",
       "982          talk        im  0\n",
       "983          talk      mine  0\n",
       "\n",
       "[984 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](assets/image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f_t = sigmoid(W_f * [h_{t-1},x_t] + b_f) $\n",
    "\n",
    "Forget gate decides what information will be retained in the cell state.\n",
    "\n",
    "Multiply the weight of the forget gate $ W_t $ with previous state $ h_{t-1} $ and current input $ x_t $. Add the bias of the forget gate $ b_f $.\n",
    "\n",
    "The sigmoid function turns this into a value between 0 and 1 indicating how much percent of cell state to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](assets/image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ i_t = sigmoid(W_i * [h_{t-1},x_t] + b_i) $\n",
    "\n",
    "The input gate decides which values to update in the cell state and by how much.\n",
    "\n",
    "$ \\~{C_t} = tanh(W_C * [h_{t-1},x_t] + b_C) $\n",
    "\n",
    "$ \\~{C_t} $ determines the new values to be added to the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](assets/image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ C_t = f_t * C_{t-1} + i_t * \\tilde{C} $\n",
    "\n",
    "Cell state is the long term memory. It forgets some information from the previous step in the sequence, then adds new information calculated from the input gate, forming the updated cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./assets/image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ o_t = sigmoid(W_o [h_{t-1},x_t] + b_o) $\n",
    "\n",
    "The output gate determines which values in the cell state will be forwarded.\n",
    "\n",
    "$ h_t = o_t * tanh(C_t) $\n",
    "\n",
    "The hidden state calculates the output by multiplying the output gate and a $ tanh $-filtered cell state. It doubly functions as the short term memory for the next input in the sequence.\n",
    "\n",
    "The `hidden_size` parameter determines the size, or dimensions, of the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/blob/9347a79f1c35c76535310111d1c50bada4e975a8/aten/src/ATen/native/RNN.cpp#L716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get result of $ W_h(h_{t-1}) + W_i(x_t) + b_h + b_i $\n",
    "```cpp\n",
    "const auto gates = params.linear_hh(hx).add_(\n",
    "    pre_compute_input ? input : params.linear_ih(input));\n",
    "```\n",
    "\n",
    "Apply activation to each gate\n",
    "```cpp\n",
    "auto chunked_gates = gates.unsafe_chunk(4, 1);\n",
    "auto ingate = chunked_gates[0].sigmoid_();\n",
    "auto forgetgate = chunked_gates[1].sigmoid_();\n",
    "auto cellgate = chunked_gates[2].tanh_();\n",
    "auto outgate = chunked_gates[3].sigmoid_();\n",
    "```\n",
    "\n",
    "Update cell state (long term) and hidden state (short term)\n",
    "```cpp\n",
    "auto cy = (forgetgate * cx).add_(ingate * cellgate);\n",
    "auto hy = outgate * cy.tanh();\n",
    "return std::make_tuple(std::move(hy), std::move(cy));\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
