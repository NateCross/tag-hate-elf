{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "This Jupyter notebook is designed to demonstrate the loading of three distinct trained ensemble models, perform predictions on new data, and evaluate these predictions using several metrics: accuracy, recall, precision, and F1 score. The process encompasses obtaining predictions from individual learners within each ensemble and aggregating these predictions to form the final ensemble predictions. The evaluation metrics provide a comprehensive understanding of the ensemble models' performance on the given dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, we import all necessary libraries and modules. This includes standard data processing and machine learning libraries such as NumPy and joblib for model loading, and specific functions from scikit-learn for ensemble methods and metrics. We also suppress warnings to keep the notebook clean and more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ensemble Models\n",
    "\n",
    "Here, we load the pre-trained ensemble models from their respective files. These models have been saved previously using the joblib library, which allows for easy storage and loading of Python objects. Ensure that the paths to the model files ('ensemble1.pkl', 'ensemble2.pkl', 'ensemble3.pkl') are correct and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = joblib.load('ensemble-hard.pkl')\n",
    "soft_ensemble = joblib.load('ensemble-soft.pkl')\n",
    "stack_ensemble = joblib.load('ensemble-stacking.pkl')\n",
    "\n",
    "ensembles = [hard_ensemble, soft_ensemble, stack_ensemble]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input Data and True Labels\n",
    "\n",
    "In this cell, we define the input data on which we want to make predictions. This data consists of a list of text samples. Additionally, we specify the true labels for these samples, which are required to evaluate the performance of our models. Replace the placeholder true labels with the actual labels corresponding to your input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = [\n",
    "    \"Gago ka putang ina\", \n",
    "    'OIDAjodiajisjdai', \n",
    "    \"You are a fucking bitch\", \n",
    "    \"NAKO  NAHIYA  YUNG  KAPAL  NG  PERA  NI  BINAY \", \n",
    "    \"fuck you binay gago ka\",\n",
    "]\n",
    "\n",
    "true_labels = np.array([1, 0, 1, 1, 1])  # Example placeholder labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Metrics Evaluation\n",
    "\n",
    "For each of the loaded ensemble models, we perform the following steps:\n",
    "- Obtain final predictions for the input data. For hard voting classifiers, we directly use the 'predict' method. For soft voting classifiers or other ensemble types, we use the 'predict_proba' method and then derive predictions based on the highest probability.\n",
    "- Evaluate the models' predictions using four metrics: accuracy, recall, precision, and F1 score. These metrics provide a holistic view of the models' performance, indicating not only their overall correctness (accuracy) but also how well they manage positive class predictions (recall and precision) and the balance between recall and precision (F1 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, ensemble in enumerate(ensembles, start=1):\n",
    "    print(f\"=== Ensemble Model {idx} ===\")\n",
    "    \n",
    "    # Final ensemble predictions\n",
    "    if isinstance(ensemble, VotingClassifier) and ensemble.voting == 'hard':\n",
    "        predictions = ensemble.predict(quotes)\n",
    "    else:\n",
    "        # Assuming binary classification and taking the class with the higher probability\n",
    "        predictions_proba = ensemble.predict_proba(quotes)\n",
    "        predictions = np.argmax(predictions_proba, axis=1)\n",
    "    \n",
    "    # Metrics evaluation\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1 Score: {f1}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate-speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
