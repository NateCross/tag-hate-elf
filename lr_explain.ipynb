{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50369/1328158918.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_logistic.fit([\n",
    "  [0.1, 0.2],\n",
    "  [0.3, 0.4],\n",
    "  [0.5, 0.6],\n",
    "], [1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.94608035e-06,  7.34661849e-06]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_logistic.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314773])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_logistic.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x) = \\frac{1}{1+e^{-x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nate/miniconda3/lib/python3.9/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "trained_lr = joblib.load('./models/model_lr/best/lr.pkl')\n",
    "\n",
    "trained_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.70518487, 3.24559592, 3.23285169]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.65501405])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_output = [\n",
    "  [0.1, 0.2, 0.3],\n",
    "  [0.3, 0.4, 0.5],\n",
    "  [0.5, 0.6, 0.7],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94611545, 0.05388455],\n",
       "       [0.77360749, 0.22639251],\n",
       "       [0.39940806, 0.60059194]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lr_output = trained_lr.predict_proba(trained_output)\n",
    "\n",
    "trained_lr_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.86552087])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solving_linear_combination = trained_lr.coef_[0][0] * trained_output[0][0] \\\n",
    "  + trained_lr.coef_[0][1] * trained_output[0][1] \\\n",
    "  + trained_lr.coef_[0][2] * trained_output[0][2] \\\n",
    "  + trained_lr.intercept_\n",
    "\n",
    "solving_linear_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05388455])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_output = 1 / (1 + np.exp(-solving_linear_combination))\n",
    "\n",
    "positive_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94611545])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_output = 1 - positive_output\n",
    "\n",
    "negative_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94611545, 0.05388455])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([negative_output, positive_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94611545, 0.05388455])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lr_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_learner_preds = joblib.load('./models/predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.99999999e-01],\n",
       "        [3.16572526e-05],\n",
       "        [4.73412315e-01],\n",
       "        ...,\n",
       "        [9.86733107e-01],\n",
       "        [1.67257632e-05],\n",
       "        [9.99886411e-01]],\n",
       "\n",
       "       [[9.90881920e-01],\n",
       "        [2.38863844e-03],\n",
       "        [1.58231616e-01],\n",
       "        ...,\n",
       "        [8.29080284e-01],\n",
       "        [3.19831399e-03],\n",
       "        [9.69492972e-01]],\n",
       "\n",
       "       [[9.94101822e-01],\n",
       "        [2.08841115e-02],\n",
       "        [6.82193711e-02],\n",
       "        ...,\n",
       "        [7.42848635e-01],\n",
       "        [2.01400183e-02],\n",
       "        [9.63838995e-01]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_learner_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 22769, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_learner_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_learner_test_preds = joblib.load('./models/test-preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.98147245e-01, 1.85275489e-03],\n",
       "        [9.96582061e-01, 3.41793926e-03],\n",
       "        [9.99917052e-01, 8.29482744e-05],\n",
       "        ...,\n",
       "        [7.57227710e-01, 2.42772290e-01],\n",
       "        [4.02596080e-02, 9.59740392e-01],\n",
       "        [9.96524714e-01, 3.47528557e-03]],\n",
       "\n",
       "       [[5.05567789e-01, 4.94432271e-01],\n",
       "        [9.56122428e-02, 9.04387772e-01],\n",
       "        [9.84410524e-01, 1.55894412e-02],\n",
       "        ...,\n",
       "        [6.18802786e-01, 3.81197274e-01],\n",
       "        [1.44663244e-01, 8.55336726e-01],\n",
       "        [9.95531023e-01, 4.46896069e-03]],\n",
       "\n",
       "       [[2.75914729e-01, 7.24085271e-01],\n",
       "        [2.86573887e-01, 7.13426113e-01],\n",
       "        [9.70233858e-01, 2.97661163e-02],\n",
       "        ...,\n",
       "        [5.19174397e-01, 4.80825573e-01],\n",
       "        [1.09437846e-01, 8.90562117e-01],\n",
       "        [9.82982695e-01, 1.70173123e-02]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_learner_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5692, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_learner_test_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "random_number_generator = np.random.default_rng(seed=0)\n",
    "def shuffle_data_frame(data_frame):\n",
    "    text = list(data_frame['text'])\n",
    "    label = list(data_frame['label'])\n",
    "\n",
    "    assert(len(text) == len(label))\n",
    "\n",
    "    indices = list(range(len(label)))\n",
    "\n",
    "    # Make a random number generator that will shuffle list of indices\n",
    "    # It is seeded to be reproducible\n",
    "    random_number_generator.shuffle(indices)\n",
    "\n",
    "    shuffled_text = []\n",
    "    shuffled_labels = []\n",
    "\n",
    "    # Iterate through the list of indices and add the original data\n",
    "    # from those shuffled indices\n",
    "    for index in indices:\n",
    "        shuffled_text.append(text[index])\n",
    "        shuffled_labels.append(label[index])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'text': shuffled_text,\n",
    "        'label': shuffled_labels,\n",
    "    })\n",
    "\n",
    "\n",
    "def get_train_test_split(data_frame: pd.DataFrame, test_size: float):\n",
    "    \"\"\"\n",
    "    Makes a stratified train test split.\n",
    "    This aims to preserve the distribution between classes.\n",
    "    \"\"\"\n",
    "    if not (1 > test_size > 0):\n",
    "        print('ERROR: test_size must be between 0 and 1')\n",
    "        return\n",
    "\n",
    "    data_frame = shuffle_data_frame(data_frame)\n",
    "\n",
    "    data_frame_length = len(data_frame)\n",
    "    train_size = 1 - test_size\n",
    "\n",
    "    nonhate_rows = data_frame[data_frame['label'] == 0]\n",
    "    nonhate_row_length = len(nonhate_rows)\n",
    "\n",
    "    nonhate_row_train_size = math.ceil(nonhate_row_length * train_size)\n",
    "\n",
    "    nonhate_row_train = nonhate_rows[0:nonhate_row_train_size]\n",
    "    nonhate_row_test = nonhate_rows[nonhate_row_train_size:nonhate_row_length]\n",
    "\n",
    "    assert(len(nonhate_row_train) + len(nonhate_row_test) == nonhate_row_length)\n",
    "\n",
    "    hate_rows = data_frame[data_frame['label'] == 1]\n",
    "    hate_row_length = len(hate_rows)\n",
    "\n",
    "    hate_row_train_size = math.ceil(hate_row_length * train_size)\n",
    "\n",
    "    hate_row_train = hate_rows[0:hate_row_train_size]\n",
    "    hate_row_test = hate_rows[hate_row_train_size:hate_row_length]\n",
    "\n",
    "    assert(len(hate_row_train) + len(hate_row_test) == hate_row_length)\n",
    "\n",
    "    combined_train = pd.concat([nonhate_row_train, hate_row_train])\n",
    "    combined_test = pd.concat([nonhate_row_test, hate_row_test])\n",
    "\n",
    "    assert(len(combined_train) + len(combined_test) == data_frame_length)\n",
    "\n",
    "    shuffled_train = shuffle_data_frame(combined_train)\n",
    "    shuffled_test = shuffle_data_frame(combined_test)\n",
    "\n",
    "    assert(len(shuffled_train) + len(shuffled_test) == data_frame_length)\n",
    "\n",
    "    return (\n",
    "        shuffled_train['text'],\n",
    "        shuffled_test['text'],\n",
    "        shuffled_train['label'],\n",
    "        shuffled_test['label'],\n",
    "    )\n",
    "\n",
    "def read_csv_file(filename: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        data = pd.read_csv(filename, lineterminator='\\n', usecols=range(2))\n",
    "        print(\"CSV file read successfully!\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: File not found\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Binay: Patuloy ang kahirapan dahil sa maling p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SA GOBYERNONG TAPAT WELCOME SA BAGUO ANG LAHAT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wait so ur telling me Let Leni Lead mo pero NY...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[USERNAME]wish this is just a nightmare that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc willie ong and isko sabunutan po</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28456</th>\n",
       "      <td>Bisaya, Probinsyano/a, mostly Bisaya = katulong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28457</th>\n",
       "      <td>Amnesia. In my whole life wala pa ako nakasala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28458</th>\n",
       "      <td>Kontrabida na ilang beses na tinalo at obvious...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28459</th>\n",
       "      <td>Yung antagonist laging kailangang sobrang sama...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28460</th>\n",
       "      <td>May nabaril or nasaksak na pero 'di pa tatawag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Binay: Patuloy ang kahirapan dahil sa maling p...      0\n",
       "1      SA GOBYERNONG TAPAT WELCOME SA BAGUO ANG LAHAT...      0\n",
       "2      wait so ur telling me Let Leni Lead mo pero NY...      1\n",
       "3       [USERNAME]wish this is just a nightmare that ...      0\n",
       "4                   doc willie ong and isko sabunutan po      0\n",
       "...                                                  ...    ...\n",
       "28456    Bisaya, Probinsyano/a, mostly Bisaya = katulong      1\n",
       "28457  Amnesia. In my whole life wala pa ako nakasala...      1\n",
       "28458  Kontrabida na ilang beses na tinalo at obvious...      1\n",
       "28459  Yung antagonist laging kailangang sobrang sama...      1\n",
       "28460  May nabaril or nasaksak na pero 'di pa tatawag...      1\n",
       "\n",
       "[28461 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = read_csv_file('datasets/datasetall.csv')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_split(dataset, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Matthew Chang [USERNAME] Remind ko lang di ba ...\n",
       "1        Yay! The interview served its purpose wellJess...\n",
       "2                                             I say DASURV\n",
       "3                                TayNew said Let Leni Lead\n",
       "4        Gloc 9 is not endorsing Jejomar Binay as his p...\n",
       "                               ...                        \n",
       "22764    Nov. 11: on [USERNAME] saw tv ads of Jojo Bina...\n",
       "22765    Mar Roxas your call for unity describes one th...\n",
       "22766    Buti nalang nagdecide nakong hindi manood ng T...\n",
       "22767    sang boto para sa pagbabago. Let Leni Lead phi...\n",
       "22768               Nakakainit ng dugo yung tv ad ni Binay\n",
       "Name: text, Length: 22769, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "22764    1\n",
       "22765    1\n",
       "22766    0\n",
       "22767    0\n",
       "22768    1\n",
       "Name: label, Length: 22769, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize starting parameters\n",
    "# We want to optimize these to best fit the learner preds\n",
    "COEFFICIENTS = [0, 0, 0]\n",
    "INTERCEPT = 0\n",
    "\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_combination(X_transposed):\n",
    "  return INTERCEPT \\\n",
    "    + COEFFICIENTS[0] * X_transposed[:, 0] \\\n",
    "    + COEFFICIENTS[1] * X_transposed[:, 1] \\\n",
    "    + COEFFICIENTS[2] * X_transposed[:, 2]\n",
    "\n",
    "def calculate_gradients(X, y):\n",
    "  X_transposed = X.T[0]\n",
    "\n",
    "  numerator = np.exp(get_linear_combination(X_transposed))\n",
    "\n",
    "  p = numerator / (1 + numerator)\n",
    "\n",
    "  partial_derivative_intercept = np.sum(y - p)\n",
    "  partial_derivative_0 = np.sum((y - p) * X_transposed[:, 0])\n",
    "  partial_derivative_1 = np.sum((y - p) * X_transposed[:, 1])\n",
    "  partial_derivative_2 = np.sum((y - p) * X_transposed[:, 2])\n",
    "\n",
    "  return np.array([\n",
    "    partial_derivative_intercept,\n",
    "    partial_derivative_0,\n",
    "    partial_derivative_1,\n",
    "    partial_derivative_2,\n",
    "  ])\n",
    "\n",
    "def logistic_regression(X):\n",
    "  X_transposed = X.T[0]\n",
    "\n",
    "  linear_combination = get_linear_combination(X_transposed)\n",
    "\n",
    "  return 1 / (1 + np.exp(-linear_combination))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Accuracy: 0.14898102600140548\n",
      "Epoch: 20 | Accuracy: 0.1500351370344343\n",
      "Epoch: 30 | Accuracy: 0.151791988756149\n",
      "Epoch: 40 | Accuracy: 0.15126493323963458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Accuracy: 0.15126493323963458\n",
      "Epoch: 60 | Accuracy: 0.1510892480674631\n",
      "Epoch: 70 | Accuracy: 0.1505621925509487\n",
      "Epoch: 80 | Accuracy: 0.15126493323963458\n",
      "Epoch: 90 | Accuracy: 0.15073787772312017\n",
      "Epoch: 100 | Accuracy: 0.1500351370344343\n",
      "Epoch: 110 | Accuracy: 0.14968376669009137\n",
      "Epoch: 120 | Accuracy: 0.14968376669009137\n",
      "Epoch: 130 | Accuracy: 0.14898102600140548\n",
      "Epoch: 140 | Accuracy: 0.14898102600140548\n",
      "Epoch: 150 | Accuracy: 0.1493323963457484\n",
      "Epoch: 160 | Accuracy: 0.14915671117357696\n",
      "Epoch: 170 | Accuracy: 0.14862965565706254\n",
      "Epoch: 180 | Accuracy: 0.14845397048489106\n",
      "Epoch: 190 | Accuracy: 0.14862965565706254\n",
      "Epoch: 200 | Accuracy: 0.14845397048489106\n",
      "Epoch: 210 | Accuracy: 0.14792691496837668\n",
      "Epoch: 220 | Accuracy: 0.14792691496837668\n",
      "Epoch: 230 | Accuracy: 0.1477512297962052\n",
      "Epoch: 240 | Accuracy: 0.1477512297962052\n",
      "Epoch: 250 | Accuracy: 0.14757554462403374\n",
      "Epoch: 260 | Accuracy: 0.14757554462403374\n",
      "Epoch: 270 | Accuracy: 0.14757554462403374\n",
      "Epoch: 280 | Accuracy: 0.14757554462403374\n",
      "Epoch: 290 | Accuracy: 0.14722417427969078\n",
      "Epoch: 300 | Accuracy: 0.14739985945186226\n",
      "Epoch: 310 | Accuracy: 0.14739985945186226\n",
      "Epoch: 320 | Accuracy: 0.14757554462403374\n",
      "Epoch: 330 | Accuracy: 0.1477512297962052\n",
      "Epoch: 340 | Accuracy: 0.14757554462403374\n",
      "Epoch: 350 | Accuracy: 0.14757554462403374\n",
      "Epoch: 360 | Accuracy: 0.1477512297962052\n",
      "Epoch: 370 | Accuracy: 0.14810260014054813\n",
      "Epoch: 380 | Accuracy: 0.14792691496837668\n",
      "Epoch: 390 | Accuracy: 0.14792691496837668\n",
      "Epoch: 400 | Accuracy: 0.14792691496837668\n",
      "Epoch: 410 | Accuracy: 0.14792691496837668\n",
      "Epoch: 420 | Accuracy: 0.1477512297962052\n",
      "Epoch: 430 | Accuracy: 0.1477512297962052\n",
      "Epoch: 440 | Accuracy: 0.1477512297962052\n",
      "Epoch: 450 | Accuracy: 0.1477512297962052\n",
      "Epoch: 460 | Accuracy: 0.1477512297962052\n",
      "Epoch: 470 | Accuracy: 0.1477512297962052\n",
      "Epoch: 480 | Accuracy: 0.1477512297962052\n",
      "Epoch: 490 | Accuracy: 0.14792691496837668\n",
      "Epoch: 500 | Accuracy: 0.14792691496837668\n",
      "Epoch: 510 | Accuracy: 0.14792691496837668\n",
      "Epoch: 520 | Accuracy: 0.14810260014054813\n",
      "Epoch: 530 | Accuracy: 0.14810260014054813\n",
      "Epoch: 540 | Accuracy: 0.14810260014054813\n",
      "Epoch: 550 | Accuracy: 0.14810260014054813\n",
      "Epoch: 560 | Accuracy: 0.1482782853127196\n",
      "Epoch: 570 | Accuracy: 0.14845397048489106\n",
      "Epoch: 580 | Accuracy: 0.14845397048489106\n",
      "Epoch: 590 | Accuracy: 0.14845397048489106\n",
      "Epoch: 600 | Accuracy: 0.14845397048489106\n",
      "Epoch: 610 | Accuracy: 0.14845397048489106\n",
      "Epoch: 620 | Accuracy: 0.14845397048489106\n",
      "Epoch: 630 | Accuracy: 0.14845397048489106\n",
      "Epoch: 640 | Accuracy: 0.14845397048489106\n",
      "Epoch: 650 | Accuracy: 0.14862965565706254\n",
      "Epoch: 660 | Accuracy: 0.14862965565706254\n",
      "Epoch: 670 | Accuracy: 0.14862965565706254\n",
      "Epoch: 680 | Accuracy: 0.14862965565706254\n",
      "Epoch: 690 | Accuracy: 0.14862965565706254\n",
      "Epoch: 700 | Accuracy: 0.14862965565706254\n",
      "Epoch: 710 | Accuracy: 0.14862965565706254\n",
      "Epoch: 720 | Accuracy: 0.14862965565706254\n",
      "Epoch: 730 | Accuracy: 0.14862965565706254\n",
      "Epoch: 740 | Accuracy: 0.14862965565706254\n",
      "Epoch: 750 | Accuracy: 0.14862965565706254\n",
      "Epoch: 760 | Accuracy: 0.14862965565706254\n",
      "Epoch: 770 | Accuracy: 0.14862965565706254\n",
      "Epoch: 780 | Accuracy: 0.14862965565706254\n",
      "Epoch: 790 | Accuracy: 0.14862965565706254\n",
      "Epoch: 800 | Accuracy: 0.14862965565706254\n",
      "Epoch: 810 | Accuracy: 0.14862965565706254\n",
      "Epoch: 820 | Accuracy: 0.14862965565706254\n",
      "Epoch: 830 | Accuracy: 0.14862965565706254\n",
      "Epoch: 840 | Accuracy: 0.14862965565706254\n",
      "Epoch: 850 | Accuracy: 0.14862965565706254\n",
      "Epoch: 860 | Accuracy: 0.14862965565706254\n",
      "Epoch: 870 | Accuracy: 0.14862965565706254\n",
      "Epoch: 880 | Accuracy: 0.14862965565706254\n",
      "Epoch: 890 | Accuracy: 0.14862965565706254\n",
      "Epoch: 900 | Accuracy: 0.14862965565706254\n",
      "Epoch: 910 | Accuracy: 0.14862965565706254\n",
      "Epoch: 920 | Accuracy: 0.14862965565706254\n",
      "Epoch: 930 | Accuracy: 0.14862965565706254\n",
      "Epoch: 940 | Accuracy: 0.14862965565706254\n",
      "Epoch: 950 | Accuracy: 0.14862965565706254\n",
      "Epoch: 960 | Accuracy: 0.14862965565706254\n",
      "Epoch: 970 | Accuracy: 0.14862965565706254\n",
      "Epoch: 980 | Accuracy: 0.14862965565706254\n",
      "Epoch: 990 | Accuracy: 0.14862965565706254\n",
      "Epoch: 1000 | Accuracy: 0.14862965565706254\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "accuracy_values = []\n",
    "\n",
    "epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "epoch_threshold = 1000\n",
    "\n",
    "while epochs_without_improvement < epoch_threshold:\n",
    "  epoch += 1\n",
    "\n",
    "  grads = calculate_gradients(concatenated_learner_preds, y_train)\n",
    "  diff = abs(grads).sum()\n",
    "  INTERCEPT += LEARNING_RATE * grads[0]\n",
    "  COEFFICIENTS += LEARNING_RATE * grads[1:]\n",
    "\n",
    "  # Evaluate on test\n",
    "  test_output = logistic_regression(concatenated_learner_test_preds)\n",
    "\n",
    "  discrete_test_output = np.round(test_output)\n",
    "\n",
    "  test_accuracy = accuracy_score(y_test, discrete_test_output)\n",
    "  accuracy_values.append(test_accuracy)\n",
    "\n",
    "  if not (epoch % 10):\n",
    "    print(f\"Epoch: {epoch} | Accuracy: {test_accuracy}\")\n",
    "\n",
    "  if test_accuracy > best_accuracy:\n",
    "    best_accuracy = test_accuracy\n",
    "  else:\n",
    "    epochs_without_improvement += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69853832, 3.26696485, 3.24849701])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COEFFICIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.67271406863987"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INTERCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5040407589599438,\n",
       " 0.1758608573436402,\n",
       " 0.15987350667603653,\n",
       " 0.15548137737174983,\n",
       " 0.15126493323963458,\n",
       " 0.1526704146170063,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.14968376669009137,\n",
       " 0.14898102600140548,\n",
       " 0.14862965565706254,\n",
       " 0.14880534082923402,\n",
       " 0.14845397048489106,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1500351370344343,\n",
       " 0.1493323963457484,\n",
       " 0.1493323963457484,\n",
       " 0.14985945186226282,\n",
       " 0.1500351370344343,\n",
       " 0.15021082220660575,\n",
       " 0.15038650737877723,\n",
       " 0.15038650737877723,\n",
       " 0.15091356289529165,\n",
       " 0.1510892480674631,\n",
       " 0.15091356289529165,\n",
       " 0.1516163035839775,\n",
       " 0.1516163035839775,\n",
       " 0.15144061841180603,\n",
       " 0.151791988756149,\n",
       " 0.1516163035839775,\n",
       " 0.15196767392832045,\n",
       " 0.15214335910049193,\n",
       " 0.15214335910049193,\n",
       " 0.15196767392832045,\n",
       " 0.15214335910049193,\n",
       " 0.15196767392832045,\n",
       " 0.1516163035839775,\n",
       " 0.1516163035839775,\n",
       " 0.15126493323963458,\n",
       " 0.15126493323963458,\n",
       " 0.15126493323963458,\n",
       " 0.15144061841180603,\n",
       " 0.15126493323963458,\n",
       " 0.15144061841180603,\n",
       " 0.1516163035839775,\n",
       " 0.151791988756149,\n",
       " 0.151791988756149,\n",
       " 0.15144061841180603,\n",
       " 0.15126493323963458,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.1510892480674631,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.15073787772312017,\n",
       " 0.1510892480674631,\n",
       " 0.1510892480674631,\n",
       " 0.1510892480674631,\n",
       " 0.15126493323963458,\n",
       " 0.1510892480674631,\n",
       " 0.1510892480674631,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.15038650737877723,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.15038650737877723,\n",
       " 0.15038650737877723,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.15091356289529165,\n",
       " 0.1510892480674631,\n",
       " 0.15126493323963458,\n",
       " 0.15126493323963458,\n",
       " 0.1510892480674631,\n",
       " 0.15126493323963458,\n",
       " 0.15126493323963458,\n",
       " 0.15126493323963458,\n",
       " 0.1510892480674631,\n",
       " 0.1510892480674631,\n",
       " 0.15091356289529165,\n",
       " 0.15073787772312017,\n",
       " 0.15073787772312017,\n",
       " 0.15073787772312017,\n",
       " 0.15073787772312017,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.1505621925509487,\n",
       " 0.15038650737877723,\n",
       " 0.15038650737877723,\n",
       " 0.1500351370344343,\n",
       " 0.1500351370344343,\n",
       " 0.1500351370344343,\n",
       " 0.14985945186226282,\n",
       " 0.14985945186226282,\n",
       " 0.14968376669009137,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.14968376669009137,\n",
       " 0.1493323963457484,\n",
       " 0.14898102600140548,\n",
       " 0.14880534082923402,\n",
       " 0.14880534082923402,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.1493323963457484,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1495080815179199,\n",
       " 0.1493323963457484,\n",
       " 0.1493323963457484,\n",
       " 0.1493323963457484,\n",
       " 0.1495080815179199,\n",
       " 0.1493323963457484,\n",
       " 0.1493323963457484,\n",
       " 0.1493323963457484,\n",
       " 0.1493323963457484,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14915671117357696,\n",
       " 0.14898102600140548,\n",
       " 0.14898102600140548,\n",
       " 0.14880534082923402,\n",
       " 0.14862965565706254,\n",
       " 0.14880534082923402,\n",
       " 0.14898102600140548,\n",
       " 0.14880534082923402,\n",
       " 0.14898102600140548,\n",
       " 0.14880534082923402,\n",
       " 0.14862965565706254,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14862965565706254,\n",
       " 0.14845397048489106,\n",
       " 0.14862965565706254,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.14810260014054813,\n",
       " 0.14792691496837668,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14722417427969078,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14739985945186226,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.14757554462403374,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.1477512297962052,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14792691496837668,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.14810260014054813,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.1482782853127196,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14845397048489106,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " 0.14862965565706254,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
